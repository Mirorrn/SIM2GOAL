diff --git a/config.py b/config.py
index ce1fa7d..9f03d07 100644
--- a/config.py
+++ b/config.py
@@ -1,3 +1,4 @@
+import math
 import os
 from os.path import expanduser
 home = expanduser("~")
@@ -5,14 +6,15 @@ home = expanduser("~")
 class Config:
     def __init__(self):
         self.DIR = home +'/Sim2Goal/models/weights/'
-        self.experiment_name = 'SI-TrajNet_NEW_NoKL_5'
+        self.experiment_name = 'SIM2Goal-univ_fast_Transformer_debug3'
         path= self.DIR + self.experiment_name
         if not os.path.exists(path):
             os.makedirs(path)
         self.model_path = path
         # Dataset options
-        self.dataset_name = 'synth'   # Choose dataset split only for single split training
-        self.trajnet = True # Trajenet++ training datasetset will be used
+        self.dataset_name = 'univ'   # Choose dataset split
+        self.goal_sampling = False
+        self.trajnet = False # Trajenet++ training datasetset will be used
         self.nabs = False # absolute position but shift the origin to the latest observed time slot
         self.delim = 'tab'
         self.loader_num_workers = 4
@@ -38,25 +40,25 @@ class Config:
         self.l2_loss = 10.
         self.best_k = 1.
         self.num_samples = 20
+
+        # for Goal Flow Training
+        self.clamp_flow = False
+        self.clip_grad = False
+
         # Generator Options
         self.g_learning_rate = 0.001    # For Coloss-GAN and all other generators learning rate
         self.g_steps = 1                # generator step
-        # for Goal Flow Training
-        self.clamp_flow = True
-        self.clip_grad = True
         # Discriminator Options
         self.adam = 1                  # check in code (Preperation) what else will be used, div sampler sgd will be used
         self.augment = True
         self.all_rel_persons = False
         self.gpu_num = "0"
-        self.device='cuda'
+        self.device='cpu'
         # Optimization
         self.batch_size = 64
-        self.num_epochs = 30
-        # Loss Options for SI
-        self.g_w = 30.
-        self.si_w = 0.
-        self.nll_w = 50.
+        self.num_epochs = 300
+        # Loss Options
+        self.l2_loss_weight = 1
         # Output
         self.output_dir = self.model_path
         self.checkpoint_name = 'checkpoint'
diff --git a/eval_GoalFlow.py b/eval_GoalFlow.py
deleted file mode 100644
index ef89a2a..0000000
--- a/eval_GoalFlow.py
+++ /dev/null
@@ -1,143 +0,0 @@
-import argparse
-import os
-os.environ["CUDA_VISIBLE_DEVICES"] = "0"
-import torch
-from config import *
-import numpy as np
-config = Config()
-from data.loader import data_loader
-from models.GoalFLow import GoalGenerator
-from utils.utils import (
-    get_dset_path,
-    plot_goals,
-
-)
-from utils.losses import final_displacement_error
-from attrdict import AttrDict
-
-seed = config.seed
-torch.manual_seed(seed)
-np.random.seed(seed)
-
-def evaluate_helper(error, seq_start_end):
-    sum_ = 0
-    error = torch.stack(error, dim=1)
-    ids = []
-    for (start, end) in seq_start_end:
-        start = start.item()
-        end = end.item()
-        _error = error[start:end]
-        _error = torch.sum(_error, dim=0).unsqueeze(dim=1)
-        _error, id = torch.min(_error, 0)
-        ids.append(id.squeeze().item() )
-        sum_ += _error.squeeze()
-    return sum_, ids
-
-
-def get_generator(checkpoint):
-    model = GoalGenerator(config)
-    model.load_state_dict(checkpoint["best_state"])
-    model.cuda()
-    model.eval()
-    return model
-
-
-def cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask):
-    fde = final_displacement_error(pred_traj_fake, pred_traj_gt[-1], val_mask[-1], mode="raw")
-    return fde
-
-
-
-def evaluate(args, loader, generator, plot_traj=False, plot_sample=False):
-    fde_outer = []
-    total_traj = 0
-    szene_id = 0.
-    samples = None
-    eval_fde_batch_errors = np.array([])
-    with torch.no_grad():
-        for batch in loader:
-            batch = [tensor.cuda() for tensor in batch]
-            obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
-             loss_mask, seq_start_end, nei_num_index, nei_num = batch
-            all_l2_errors_dest = []
-            total_traj += nei_num.sum()
-            batch_pred_traj_fake = []
-            batch_samples = []
-            att_score_list_batch = []
-            model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-            val_mask = val_mask[-config.pred_len :]
-            for _ in range(args.num_samples):
-                if plot_sample:
-                    pred_traj_fake, samples = generator(model_input, obs_traj, pred_traj_gt,
-                                                   seq_start_end, nei_num_index, nei_num, plot_sample=plot_sample)
-                else:
-                    pred_traj_fake = generator(model_input, obs_traj, pred_traj_gt,
-                                                    seq_start_end, nei_num_index, nei_num)
-                batch_samples.append(samples)
-                batch_pred_traj_fake.append(pred_traj_fake)
-                fde_ = cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask)
-                all_l2_errors_dest.append(fde_.cpu().numpy())
-
-                # fde_sum, ids = evaluate_helper(fde, seq_start_end)
-
-            all_l2_errors_dest = np.array(all_l2_errors_dest)
-            # all_guesses = np.array(all_guesses)
-            # average error
-            # l2error_avg_dest = np.mean(all_l2_errors_dest)
-
-            # choosing the best guess
-            # indices = np.argmin(all_l2_errors_dest, axis=0)
-
-            # best_guess_dest = all_guesses[indices, np.arange(obs_traj.shape[0]), :]
-            l2error_dest = np.min(all_l2_errors_dest, axis=0)
-            eval_fde_batch_errors = np.hstack([eval_fde_batch_errors, l2error_dest])
-            if (plot_traj):
-                plot_goals(obs_traj, pred_traj_gt, batch_pred_traj_fake, seq_start_end, batch_samples)
-            szene_id += 1
-
-        fde = np.mean(eval_fde_batch_errors)  # / (total_traj)
-
-        return fde
-
-
-# check if trajnet is set to True in config if trajnet evaluation
-# EXPERIMENT_NAME = 'GFLOW-ETHandUCY' # eth
-EXPERIMENT_NAME = 'GFLOW-TrajNet' # eth
-_dir = os.path.dirname(__file__)
-_dir = _dir.split("/")[:-1]
-_dir = "/".join(_dir) + "/Sim2Goal/models/weights/"
-DIR = _dir + EXPERIMENT_NAME + '-' # -eth and so on
-# /checkpoint_with_model.pt
-parser = argparse.ArgumentParser()
-parser.add_argument('--model_path', default=DIR, type=str)
-parser.add_argument('--num_samples', default=20, type=int)
-parser.add_argument('--dset_type', default='test', type=str)
-
-
-def main(args):
-    paths = []
-    if config.trajnet:
-        datasets = ['hotel', 'lcas', 'students1', 'students3', 'wildtrack', 'zara1', 'zara3']
-    else:
-        datasets = ['eth', 'hotel', 'univ', 'zara1', 'zara2']
-    for i in datasets:
-        paths.append(args.model_path+i + '/checkpoint_with_model.pt')
-    # paths = [args.model_path] + datasets
-    for path in paths:
-        checkpoint = torch.load(path)
-        generator = get_generator(checkpoint)
-        _args = AttrDict(checkpoint['args'])
-        path = get_dset_path(_args.dataset_name, args.dset_type)
-        _, loader = data_loader(_args, path)
-        fde = evaluate(_args, loader, generator, plot_traj=False, plot_sample=False)
-        print(
-            "Dataset:  {} \n"
-            "Pred Len: {} FDE \n"
-            "{:.4f}".format(
-                _args.dataset_name, _args.pred_len, fde
-            )
-        )
-
-if __name__ == "__main__":
-    args = parser.parse_args()
-    main(args)
\ No newline at end of file
diff --git a/eval_GoalFlow_divSampling.py b/eval_GoalFlow_divSampling.py
deleted file mode 100644
index 559aae1..0000000
--- a/eval_GoalFlow_divSampling.py
+++ /dev/null
@@ -1,191 +0,0 @@
-import argparse
-import os
-
-os.environ["CUDA_VISIBLE_DEVICES"] = "0"
-import torch
-from config import *
-import numpy as np
-config = Config()
-from data.loader import data_loader
-from models.GoalFLow import GoalGenerator
-from models.Sampler import Sampler
-from utils.utils import (
-    get_dset_path,
-    plot_goals,
-
-)
-from utils.losses import final_displacement_error
-from attrdict import AttrDict
-
-# Change to your path here!
-# DIR = home +'/Documents/NFTraj/192_Goal_NuScene_FLOW3_TEST_sample10_div/checkpoint_with_model.pt'
-# DIR = home +'/Documents/NFTraj/GFLOW-ETHandUCY_corrected_sampler-eth/checkpoint_with_model.pt'
-#
-#
-# parser = argparse.ArgumentParser()
-# parser.add_argument('--model_path', default=DIR, type=str)
-# parser.add_argument('--num_samples', default=20, type=int)
-# parser.add_argument('--dset_type', default='test', type=str)
-#
-# seed = config.seed
-# torch.manual_seed(seed)
-# np.random.seed(seed)
-# import locale
-# locale.setlocale(locale.LC_ALL, 'de_DE.utf8')
-
-def evaluate_helper(error, seq_start_end):
-    sum_ = 0
-    error = torch.stack(error, dim=1)
-    ids = []
-    for (start, end) in seq_start_end:
-        start = start.item()
-        end = end.item()
-        _error = error[start:end]
-        _error = torch.sum(_error, dim=0).unsqueeze(dim=1)
-        _error, id = torch.min(_error, 0)
-        ids.append(id.squeeze().item() )
-        sum_ += _error.squeeze()
-    return sum_, ids
-
-
-def get_generator(checkpoint):
-    model = GoalGenerator(config)
-    model.load_state_dict(checkpoint["best_state"])
-    model.cuda()
-    model.eval()
-    sampler = Sampler(config)
-    sampler.load_state_dict(checkpoint["best_state_sampler"])
-    sampler.cuda()
-    sampler.eval()
-    return model, sampler
-
-
-def cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask):
-    fde = final_displacement_error(pred_traj_fake, pred_traj_gt[-1], val_mask[-1], mode="raw")
-    return fde
-
-
-
-def evaluate(args, loader, generator, sampler, plot_traj=False, plot_sample=False):
-    fde_outer = []
-    total_traj = 0
-    szene_id = 0.
-    samples = None
-    eval_fde_batch_errors = np.array([])
-    with torch.no_grad():
-        for batch in loader:
-            batch = [tensor.cuda() for tensor in batch]
-            obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
-             loss_mask, seq_start_end, nei_num_index, nei_num = batch
-            all_l2_errors_dest = []
-            total_traj += nei_num.sum()
-            batch_pred_traj_fake = []
-            batch_samples = []
-            att_score_list_batch = []
-            model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-            val_mask = val_mask[-config.pred_len :]
-            pred_traj_fake,_ = generator(model_input, obs_traj, pred_traj_gt,
-                                                seq_start_end, nei_num_index, nei_num,
-                                         plot_sample=plot_sample, mode='sampling', sampling_module=sampler)
-            for i_top20 in pred_traj_fake:
-                batch_samples.append(samples)
-                batch_pred_traj_fake.append(i_top20)
-                fde_ = cal_ade_fde(pred_traj_gt, i_top20, val_mask)
-                all_l2_errors_dest.append(fde_.cpu().numpy())
-
-                # fde_sum, ids = evaluate_helper(fde, seq_start_end)
-
-            all_l2_errors_dest = np.array(all_l2_errors_dest)
-            # all_guesses = np.array(all_guesses)
-            # average error
-            # l2error_avg_dest = np.mean(all_l2_errors_dest)
-
-            # choosing the best guess
-            # indices = np.argmin(all_l2_errors_dest, axis=0)
-
-            # best_guess_dest = all_guesses[indices, np.arange(obs_traj.shape[0]), :]
-            l2error_dest = np.min(all_l2_errors_dest, axis=0)
-            eval_fde_batch_errors = np.hstack([eval_fde_batch_errors, l2error_dest])
-            if (plot_traj):
-                plot_goals(obs_traj, pred_traj_gt, batch_pred_traj_fake, seq_start_end, batch_samples)
-            szene_id += 1
-
-        fde = np.mean(eval_fde_batch_errors)  # / (total_traj)
-
-        return fde
-
-
-# def main(args):
-#     if os.path.isdir(args.model_path):
-#         filenames = os.listdir(args.model_path)
-#         filenames.sort()
-#         paths = [
-#             os.path.join(args.model_path, file_) for file_ in filenames
-#         ]
-#     else:
-#         paths = [args.model_path]
-#     for path in paths:
-#         checkpoint = torch.load(path)
-#         generator, sampler = get_generator(checkpoint)
-#         _args = AttrDict(checkpoint['args'])
-#         path = get_dset_path(_args.dataset_name, args.dset_type)
-#
-#         _, loader = data_loader(_args, path)
-#         fde = evaluate(_args, loader, generator, sampler, plot_traj=False, plot_sample=False)
-#         print(
-#             "Dataset:  {} \n"
-#             "Pred Len: {} FDE \n"
-#             "{:.4f}".format(
-#                 _args.dataset_name, _args.pred_len, fde
-#             )
-#         )
-#
-#
-# if __name__ == "__main__":
-#     args = parser.parse_args()
-#     torch.backends.cudnn.deterministic = True
-#     torch.backends.cudnn.benchmark = False
-#     main(args)
-
-
-# check if trajnet is set to True in config if trajnet evaluation
-EXPERIMENT_NAME = 'GFLOW-ETHandUCY_sampler'
-_dir = os.path.dirname(__file__)
-_dir = _dir.split("/")[:-1]
-_dir = "/".join(_dir) + "/Sim2Goal/models/weights/"
-DIR = _dir + EXPERIMENT_NAME + '-' # -eth and so on
-# /checkpoint_with_model.pt
-parser = argparse.ArgumentParser()
-parser.add_argument('--model_path', default=DIR, type=str)
-parser.add_argument('--num_samples', default=20, type=int)
-parser.add_argument('--dset_type', default='test', type=str)
-
-
-def main(args):
-    paths = []
-    if config.trajnet:
-        datasets = ['hotel', 'lcas', 'students1', 'students3', 'wildtrack', 'zara1', 'zara3']
-    else:
-        datasets = ['eth', 'hotel', 'univ', 'zara1', 'zara2']
-    for i in datasets:
-        paths.append(args.model_path+i + '/checkpoint_with_model.pt')
-    # paths = [args.model_path] + datasets
-    for path in paths:
-        checkpoint = torch.load(path)
-        generator, sampler = get_generator(checkpoint)
-        _args = AttrDict(checkpoint['args'])
-        path = get_dset_path(_args.dataset_name, args.dset_type)
-
-        _, loader = data_loader(_args, path)
-        fde = evaluate(_args, loader, generator, sampler, plot_traj=False, plot_sample=False)
-        print(
-            "Dataset:  {} \n"
-            "Pred Len: {} FDE \n"
-            "{:.4f}".format(
-                _args.dataset_name, _args.pred_len, fde
-            )
-        )
-
-if __name__ == "__main__":
-    args = parser.parse_args()
-    main(args)
\ No newline at end of file
diff --git a/eval_SI.py b/eval_SI.py
deleted file mode 100644
index dbccc92..0000000
--- a/eval_SI.py
+++ /dev/null
@@ -1,314 +0,0 @@
-import argparse
-import os
-
-from tqdm import tqdm
-
-os.environ["CUDA_VISIBLE_DEVICES"] = "0"
-import torch
-from config import *
-import numpy as np
-config = Config()
-from data.loader import data_loader
-from models.Sim2Goal import TrajectoryGenerator
-from models.Robot import Robot
-from utils.utils import (
-    relative_to_abs,
-    get_dset_path,
-    fast_coll_counter
-)
-from utils.losses import KL_gaussians
-from attrdict import AttrDict
-
-
-def get_generator(checkpoint, exp_name):
-    _args = AttrDict(checkpoint['args'])
-    model = TrajectoryGenerator(config)
-    model.load_state_dict(checkpoint["best_state"])
-    model.cuda()
-    model.eval()
-    checkpoint_robot_path = config.DIR + exp_name + '/checkpoint_with_model.pt' # from paper no SI
-    checkpoint_sampler = torch.load(checkpoint_robot_path)
-    robot = Robot()
-    robot.load_state_dict(checkpoint_sampler["best_state"])
-    robot.cuda().eval()
-
-    return model,robot
-
-def compute_ade(predicted_trajs, gt_traj, axis = 0):
-    error = np.linalg.norm(predicted_trajs - gt_traj, axis=-1)
-    ade = np.mean(error, axis=axis)
-    if axis == 0:
-        return ade.flatten()
-    else:
-        return ade
-
-def compute_fde(predicted_trajs, gt_traj):
-    final_error = torch.sqrt(((predicted_trajs-gt_traj)**2).sum(dim=-1))
-    return final_error.mean(dim=0)
-
-def evaluate_social(args, loader, Student, robot_net, plot_traj=False, sample_z=2, same_goal = False):
-    mutual_info, ADE_min, ADE_max, MPE_vel_min,\
-    MPE_vel_max, sum_timestep_KL_list, FDE,\
-    ADE_avg_scene, MPE_avg_scene, MRE  = [], [], [], [], [], [], [], [], [], []
-    coll_all, count_fake_all = 0., 0.
-    sample = 20
-    sample_z = sample_z
-    same_goal = same_goal
-    id_influencer = 0 # on Trajnet++ always zero
-    szene_id = 0
-    with torch.no_grad():
-        for j,batch in enumerate(tqdm(loader)):
-            if j>=100:
-                break
-            batch = [tensor.cuda() for tensor in batch]
-            obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
-             loss_mask, seq_start_end, nei_num_index, nei_num = batch
-            model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-            batch_size = seq_start_end.shape[0]
-            robots_ids = seq_start_end[:,0]
-            sgoals = pred_traj_gt[-1]
-            current_pos = obs_traj[-1]
-            dist_mat_pred = torch.cdist(sgoals, current_pos, p=2.0, compute_mode='donot_use_mm_for_euclid_dist')
-            if same_goal == True:
-                for _, (start, end) in enumerate(seq_start_end):
-                   start = start.item()
-                   end = end.item()
-                   dist_mat_pred_scene = dist_mat_pred[start:end,start]
-                   dist_mat_pred_scene[0] = np.inf
-                   min_index = torch.argmin(dist_mat_pred_scene)
-                   sgoals[start] = sgoals[min_index + start]
-
-            #sgoals[robots_ids] = sgoals[seq_start_end[:,1]-1]
-            sum_timestep_KL_batch, ADE_avg_scene_tmp_batch, MPE_avg_scene_tmp_batch,\
-            MRE_batch, ADE_max_scene_batch, ADE_min_scene_batch,\
-            MPE_vel_min_scene_batch, MPE_vel_max_scene_batch= [], [], [], [], [], [], [], []
-            for l in range(sample_z):
-                if l == 0:
-                    z = torch.zeros_like(pred_traj_gt) # get the most likely behaviour of the humans
-                else:
-                    z = torch.randn_like(pred_traj_gt)
-                conditioned_pos, conditioned_vel = [], []
-
-                mu_c_samples = []
-                logscale_c_samples = []
-
-                for i in range(sample):
-                    z_robot = torch.randn_like(pred_traj_gt)[:,0:batch_size]
-                    z[:, robots_ids] = z_robot
-                    conditioned, mu_c, logscale_c = Student(model_input, obs_traj, pred_traj_gt,
-                                                              seq_start_end, nei_num_index, nei_num,
-                                                              mode='test', sample_goal=sgoals, robot_net=robot_net,
-                                                              robotID=robots_ids, noise=z)
-                    pos_cond = relative_to_abs(conditioned, obs_traj[-1])
-                    conditioned_pos.append(pos_cond)
-                    conditioned_vel.append(conditioned)
-                    mu_c_offset = torch.cat([obs_traj[-1].unsqueeze(dim=0), pos_cond], dim=0)
-                    mu_c = mu_c + mu_c_offset[0:-1]
-                    mu_c_samples.append(mu_c)
-                    logscale_c_samples.append(logscale_c)
-
-                coll_pro_szene_fake, count_fake, _, _ = fast_coll_counter(
-                    conditioned_pos, seq_start_end, None, nei_num_index, sample=sample)
-                conditioned_pos = torch.stack(conditioned_pos)
-                coll_all += coll_pro_szene_fake
-                count_fake_all += count_fake
-                fde = compute_fde(conditioned_pos[:,-1], sgoals)[robots_ids]
-                FDE = np.hstack([FDE, fde.cpu().numpy()])
-                conditioned_vel = torch.stack(conditioned_vel) / 0.4
-                # conditioned_acc = torch.zeros(conditioned_vel[:, :, robots_ids].shape) / 0.4
-                # conditioned_acc[:, 1:] = conditioned_vel[:, 1:, robots_ids] - conditioned_vel[:, :-1, robots_ids]
-                conditioned_acc = torch.zeros(conditioned_vel[:, :].shape)
-                conditioned_acc[:, 1:] = (conditioned_vel[:, 1:] - conditioned_vel[:, :-1]) / 0.4
-                conditioned_acc_robot = torch.sqrt((conditioned_acc[:, :, robots_ids]**2).sum(dim=-1)).mean(dim=1).cpu().numpy()
-                mu_c_samples = torch.stack(mu_c_samples)
-                logscale_c_samples = torch.stack(logscale_c_samples)
-                ids_no_robot= torch.ones([seq_start_end[-1,1]], dtype=torch.bool).cuda()
-                ids_no_robot[robots_ids] = False
-                nei_num_index_tmp = nei_num_index[:, ids_no_robot]
-                nei_num_index_tmp = nei_num_index_tmp[:, :, ids_no_robot]
-                model_input_tmp = model_input[:, ids_no_robot]
-                obs_traj_tmp = obs_traj[:, ids_no_robot]
-                pred_traj_gt_tmp = pred_traj_gt[:, ids_no_robot]
-                sgoals_tmp = sgoals[ids_no_robot]
-                unconditioned_vel, mu_unc, logscale_unc = Student(model_input_tmp, obs_traj_tmp, pred_traj_gt_tmp,
-                                                                  seq_start_end, nei_num_index_tmp, nei_num,
-                                                                  mode='test', sample_goal=sgoals_tmp,
-                                                                  noise=z[:, ids_no_robot])
-
-                unconditioned_pos = relative_to_abs(unconditioned_vel, obs_traj_tmp[-1])
-                unconditioned_vel = unconditioned_vel / 0.4
-                unconditioned_acc = torch.zeros(unconditioned_vel.shape)
-                unconditioned_acc[1:] = (unconditioned_vel[1:] - unconditioned_vel[:-1]) / 0.4
-                mu_offset = torch.cat([obs_traj_tmp[-1].unsqueeze(dim=0),unconditioned_pos],dim = 0)
-                mu_unc = mu_unc + mu_offset[0:-1]
-                sum_timestep_KL = KL_gaussians(mu_c_samples[:,:, ids_no_robot], logscale_c_samples[:,:, ids_no_robot],
-                                           mu_unc.unsqueeze(dim=0), logscale_unc.unsqueeze(dim=0), sum_dim=1)
-                sum_timestep_KL_batch.append(sum_timestep_KL)
-
-                seq_start_end_no_rob = seq_start_end.clone()
-                f = torch.arange(0, batch_size).cuda()
-                seq_start_end_no_rob[:, 0] = seq_start_end_no_rob[:, 0] - f
-                seq_start_end_no_rob[:, 1] = seq_start_end_no_rob[:, 1] - f - 1
-                ADE_max_scene, ADE_min_scene, MPE_vel_min_scene, MPE_vel_max_scene = [], [], [], []
-                for (start, end),(start_r,end_r) in zip(seq_start_end,seq_start_end_no_rob):
-                    if start_r == end_r:
-                        continue
-                    conditioned_pos_scene = conditioned_pos[:,:,start+1:end]
-                    conditioned_vel_scene = conditioned_acc[:,:,start+1:end]
-                    unconditioned_pos_scene = unconditioned_pos[:,start_r:end_r]
-                    unconditioned_vel_scene = unconditioned_acc[:,start_r:end_r]
-                    sum_batch = sum_timestep_KL[:,start_r:end_r]
-                    sum_over_persons = sum_batch.sum(dim=-1)
-                    max_KL = torch.max(sum_timestep_KL[:, start_r:end_r])
-                    max_id = torch.argmax(sum_over_persons).cpu().numpy()
-                    min_id = torch.argmin(sum_over_persons).cpu().numpy()
-                    conditioned_pos_scene_min = conditioned_pos_scene[min_id]
-                    conditioned_pos_scene_max = conditioned_pos_scene[max_id]
-                    conditioned_vel_scene_min = conditioned_vel_scene[min_id]
-                    conditioned_vel_scene_max = conditioned_vel_scene[max_id]
-
-                    ADE_min_scene.append(compute_ade(conditioned_pos_scene_min.cpu().numpy(),unconditioned_pos_scene.cpu().numpy()))
-                    ADE_max_scene.append(compute_ade(conditioned_pos_scene_max.cpu().numpy(),unconditioned_pos_scene.cpu().numpy()))
-                    MPE_vel_min_scene.append(compute_ade(conditioned_vel_scene_min.cpu().numpy(),unconditioned_vel_scene.cpu().numpy()))
-                    MPE_vel_max_scene.append(compute_ade(conditioned_vel_scene_max.cpu().numpy(),unconditioned_vel_scene.cpu().numpy()))
-                   # if plot_traj and max_KL > 200:
-                   #  plot_influencer_for_paper(obs_traj[:,start:end].cpu().numpy(), pred_traj_gt[:,start:end].cpu().numpy(),
-                   #                   conditioned_pos[:,:,start:end].cpu().numpy(),
-                   #                   id_influencer, ids_no_robot[start:end], unconditioned_pos[:,start_r:end_r].cpu().numpy(),
-                   #                   sgoals[start:end].cpu().numpy(), min_id, szene_id)
-                    szene_id += 1
-                ADE_min_scene_batch.append(np.hstack(ADE_min_scene))
-                ADE_max_scene_batch.append(np.hstack(ADE_max_scene))
-                MPE_vel_min_scene_batch.append(np.hstack(MPE_vel_min_scene))
-                MPE_vel_max_scene_batch.append(np.hstack(MPE_vel_max_scene))
-
-                sum_timestep_KL_list = np.hstack([sum_timestep_KL_list, sum_timestep_KL.flatten().cpu().numpy()])
-
-
-                ADE_avg_scene_tmp = compute_ade(conditioned_pos[:, :, ids_no_robot].cpu().numpy(),
-                                                unconditioned_pos.cpu().numpy(), axis=1) # .mean(axis=0)
-                ADE_avg_scene_tmp_batch.append(ADE_avg_scene_tmp)
-
-                # con_test_acc = torch.zeros(conditioned_vel[:, :, ids_no_robot].shape)
-                # test_vel = conditioned_vel
-                # con_test_acc[:, 1:] = (test_vel[:, 1:, ids_no_robot] - test_vel[:, :-1, ids_no_robot])
-                #
-                # ucon_test_acc = torch.zeros(unconditioned_vel.shape)
-                # utest_vel = unconditioned_vel
-                # ucon_test_acc[:, 1:] = (utest_vel[:, 1:,] - utest_vel[:, :-1,])
-
-                MPE_avg_scene_tmp = compute_ade(conditioned_acc[:, :, ids_no_robot].cpu().numpy(),
-                                                unconditioned_acc.cpu().numpy(), axis=1) # .mean(axis=0)
-                # MPE_avg_scene_tmp_test = compute_ade(con_test_acc.cpu().numpy(),ucon_test_acc.cpu().numpy(), axis=1) # .mean(axis=0)
-                MPE_avg_scene_tmp_batch.append(MPE_avg_scene_tmp)
-                MRE_batch.append(conditioned_acc_robot)
-
-            ADE_min_scene_batch = np.stack(ADE_min_scene_batch)
-            ADE_min = np.hstack([ADE_min, ADE_min_scene_batch.sum(axis=0) / (sample_z)])
-            ADE_max_scene_batch = np.stack(ADE_max_scene_batch)
-            ADE_max = np.hstack([ADE_max, ADE_max_scene_batch.sum(axis=0) / (sample_z)])
-            ADE_avg_scene_tmp_batch = np.stack(ADE_avg_scene_tmp_batch)
-            ADE_avg_scene = np.hstack([ADE_avg_scene, ADE_avg_scene_tmp_batch.sum(axis=0).sum(axis=0) / (sample*sample_z)])
-
-            MPE_vel_min_scene_batch = np.stack(MPE_vel_min_scene_batch)
-            MPE_vel_min = np.hstack([MPE_vel_min, MPE_vel_min_scene_batch.sum(axis=0) / (sample_z)])
-            MPE_vel_max_scene_batch = np.stack(MPE_vel_max_scene_batch)
-            MPE_vel_max = np.hstack([MPE_vel_max, MPE_vel_max_scene_batch.sum(axis=0) / (sample_z)])
-
-            MPE_avg_scene_tmp_batch = np.stack(MPE_avg_scene_tmp_batch)
-            MPE_avg_scene = np.hstack([MPE_avg_scene, MPE_avg_scene_tmp_batch.sum(axis=0).sum(axis=0) / (sample*sample_z)])
-            MRE_batch = np.stack(MRE_batch)
-            MRE = np.hstack([MRE, MRE_batch.sum(axis=0).sum(axis=0) / (sample*sample_z)])
-
-            sum_timestep_KL_batch = torch.stack(sum_timestep_KL_batch)
-            estimated_per_person_MutualInfo = sum_timestep_KL_batch.sum(dim=0).sum(dim=0) / (sample*sample_z)
-            mutual_info = np.hstack([mutual_info, estimated_per_person_MutualInfo.cpu().numpy()])
-
-        # test = np.vstack([mutual_info, ADE_min, ADE_max,ADE_avg_scene, MPE_vel_min, MPE_vel_max, MPE_avg_scene]).T
-        # import seaborn as sns
-
-        # df = pd.DataFrame(test, columns=['Mutual_info','ADE_min', 'ADE_max','ADE_avg_scene','MPE_vel_min','MPE_vel_max','MPE_avg_scene' ])
-        # df.to_csv(config.DIR + 'SI_Policy_test_SI_50nlllcas_1'+ '/out.csv')
-        # boxplot = df.boxplot(column=['Mutual_info'])
-        # plot.hist(bins=12, alpha=0.5)
-        # ax = df.plot.hist(bins=120, alpha=0.5)
-        # ax = sns.boxplot(data=test, showfliers = False)
-        # ax = sns.swarmplot(x=test)
-        # plt.show()
-        ADE_min_mean = np.mean(ADE_min)
-        ADE_max_mean = np.mean(ADE_max)
-        MPE_vel_min_mean = np.mean(MPE_vel_min)
-        MPE_vel_max_mean = np.mean(MPE_vel_max)
-        MPE_avg_scene = np.mean(MPE_avg_scene)
-        ADE_avg_scene = np.mean(ADE_avg_scene)
-        MRE = np.mean(MRE)
-        FDE_mean = np.mean(FDE)
-        mutual_info = np.array(mutual_info)
-        act = coll_all / count_fake_all
-        sum_timestep_KL_list = np.array(sum_timestep_KL_list)
-        # plt.hist(mutual_info,bins='auto', density=False, range=(0,4))
-        # plt.title("histogram mutual_info")
-        # plt.show()
-        mean_mutual_info = np.mean(mutual_info)
-        # plt.hist(sum_timestep_KL_list, bins='auto', density=False, range=(0, 4))
-        # plt.title("histogram timestep KL")
-        # plt.show()
-        return ADE_min_mean, ADE_max_mean, ADE_avg_scene, MPE_vel_min_mean,\
-               MPE_vel_max_mean, MPE_avg_scene, mean_mutual_info, FDE_mean, act, MRE
-
-def main(args, exp_name):
-    if os.path.isdir(args.model_path):
-        filenames = os.listdir(args.model_path)
-        filenames.sort()
-        paths = [
-            os.path.join(args.model_path, file_) for file_ in filenames
-        ]
-    else:
-        paths = [args.model_path]
-    for path in paths:
-        checkpoint = torch.load(path)
-        student, robot = get_generator(checkpoint, exp_name)
-        # robot = None
-        _args = AttrDict(checkpoint['args'])
-        # path = get_dset_path(_args.dataset_name, args.dset_type)
-        path = get_dset_path('synth', args.dset_type)
-        _, loader = data_loader(_args, path)
-        ADE_min_mean, ADE_max_mean, ADE_avg_scene, MPE_vel_min_mean,\
-        MPE_vel_max_mean, MPE_avg_scene, Mean_mutual_info,\
-        FDE_mean, act, MRE = evaluate_social(_args, loader, student, robot, plot_traj=False, sample_z=5, same_goal=True)
-        print(
-            "Dataset:  {} \n"
-            "Pred Len: {}, ADE_min_mean, ADE_max_mean, ADE_avg_scene, MPE_vel_min_mean,"
-            " MPE_vel_max_mean, MPE_vel_avg_scene, Mean_mutual_info, FDE_mean, ACT, MRE \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f} \n"
-            .format(
-                _args.dataset_name, _args.pred_len, ADE_min_mean, ADE_max_mean, ADE_avg_scene,
-                MPE_vel_min_mean, MPE_vel_max_mean, MPE_avg_scene, Mean_mutual_info, FDE_mean, act, MRE
-            )
-        )
-# Here we define the prediction model for the experiments!
-exp_name = 'SI-TrajNet_NoKL'
-DIR = config.student_checkpoint_start_from \
-                                 + 'lcas' + '/checkpoint_with_model.pt'
-parser = argparse.ArgumentParser()
-parser.add_argument('--model_path', default=DIR, type=str)
-parser.add_argument('--num_samples', default=20, type=int)
-parser.add_argument('--dset_type', default='test', type=str)
-seed = config.seed
-torch.manual_seed(seed)
-np.random.seed(seed)
-
-if __name__ == "__main__":
-    args = parser.parse_args()
-    # torch.backends.cudnn.deterministic = True
-    # torch.backends.cudnn.benchmark = False
-    main(args, exp_name)
diff --git a/eval_Sim2Goal.py b/eval_Sim2Goal.py
index 6d465f8..17637e9 100644
--- a/eval_Sim2Goal.py
+++ b/eval_Sim2Goal.py
@@ -12,12 +12,14 @@ from utils.utils import (
     relative_to_abs,
     get_dset_path,
     plot_best_goal,
+    plot_best,
     fast_coll_counter
 )
-from models.Sampler import Sampler
+
 from utils.losses import displacement_error
 from attrdict import AttrDict
-from models.GoalFLow import GoalGenerator
+
+
 seed = config.seed
 torch.manual_seed(seed)
 np.random.seed(seed)
@@ -43,19 +45,8 @@ def get_generator(checkpoint):
     model.load_state_dict(checkpoint["best_state"])
     model.cuda()
     model.eval()
-    checkpoint_sampler_path = config.sampler_checkpoint_start_from  + _args.dataset_name \
-                             + '/checkpoint_with_model.pt'
-    checkpoint_sampler = torch.load(checkpoint_sampler_path)
-    sample_generator = GoalGenerator(_args)
-    sample_generator.load_state_dict(checkpoint_sampler["best_state"])
-    sample_generator.cuda().eval()
-
-    sampler = Sampler(_args)
-    sampler.load_state_dict(checkpoint_sampler["best_state_sampler"])
-    sampler.cuda().eval()
 
-
-    return model,sample_generator,sampler
+    return model
 
 def compute_ade(predicted_trajs, gt_traj, val_mask):
     error = np.linalg.norm(predicted_trajs - gt_traj, axis=-1)
@@ -88,8 +79,6 @@ def evaluate(args, loader, generator, sample_generator, sampler , gt_coll=False,
     samples = None
     with torch.no_grad():
         for j, batch in enumerate(loader):
-            if j>=20:
-                break
             batch = [tensor.cuda() for tensor in batch]
             obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
              loss_mask, seq_start_end, nei_num_index, nei_num = batch
@@ -101,30 +90,28 @@ def evaluate(args, loader, generator, sample_generator, sampler , gt_coll=False,
             att_score_list_batch = []
             model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
             val_mask = val_mask[-config.pred_len :]
-            pred_traj_fake_goal, _ = sample_generator(model_input, obs_traj, pred_traj_gt,
-                                          seq_start_end, nei_num_index, nei_num,
-                                          plot_sample=plot_sample, mode='sampling', sampling_module=sampler)
-            for sgoals in pred_traj_fake_goal:
-                # sgoals = pred_traj_gt[-1]
-
-                pred_traj_fake_rel, _, _ = generator(model_input, obs_traj, pred_traj_gt,
-                                               seq_start_end, nei_num_index, nei_num, plot_sample=plot_sample,
-                                                        mode='test', sample_goal=sgoals,robot_net=robot,
-                                                        robotID=robots_ids)
-
-                pred_traj_fake_rel = pred_traj_fake_rel[-args.pred_len :]
-                pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-                # pred_traj_fake[-1] = sgoals
-                batch_samples.append(samples)
-                batch_pred_traj_fake.append(pred_traj_fake)
-                ade_, fde_, ade_col_ = cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask)
-                if robot != None:
-                    ade_ = ade_[robots_ids.cpu().numpy()]
-                    fde_ = fde_[robots_ids.cpu().numpy()]
-                    ade_col_ = ade_col_[robots_ids.cpu().numpy()]
-                ade.append(ade_)
-                fde.append(fde_)
-                ade_col.append(ade_col_)
+
+  
+            sgoals = pred_traj_gt[-1] # ToDo: Comment this out
+
+            pred_traj_fake_rel, _, _ = generator(model_input, obs_traj, pred_traj_gt,
+                                            seq_start_end, nei_num_index, nei_num, plot_sample=plot_sample,
+                                                    mode='test', sample_goal=sgoals,robot_net=robot,
+                                                    robotID=robots_ids)
+
+            pred_traj_fake_rel = pred_traj_fake_rel[-args.pred_len :]
+            pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
+            # pred_traj_fake[-1] = sgoals
+            batch_samples.append(samples)
+            batch_pred_traj_fake.append(pred_traj_fake)
+            ade_, fde_, ade_col_ = cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask)
+            if robot != None:
+                ade_ = ade_[robots_ids.cpu().numpy()]
+                fde_ = fde_[robots_ids.cpu().numpy()]
+                ade_col_ = ade_col_[robots_ids.cpu().numpy()]
+            ade.append(ade_)
+            fde.append(fde_)
+            ade_col.append(ade_col_)
 
             _, ids = evaluate_helper(ade_col, seq_start_end)
             ade_sum = np.array(ade)
@@ -139,12 +126,12 @@ def evaluate(args, loader, generator, sample_generator, sampler , gt_coll=False,
             coll_pro_szenes_fake += coll_pro_szene_fake
             count_szenes_fake += count_fake
             if (plot_traj):
-                # plot_best(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
-                #           stack_of_coll_indeces, seq_start_end,
-                #           ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
-                plot_best_goal(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
+                plot_best(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
                           stack_of_coll_indeces, seq_start_end,
-                          ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples, pred_traj_fake_goal)
+                          ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
+                # plot_best_goal(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
+                #           stack_of_coll_indeces, seq_start_end,
+                #           ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples, pred_traj_fake_goal)
                 #
                 # plot_multimodal(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
                 #                stack_of_coll_indeces, seq_start_end,
@@ -154,14 +141,14 @@ def evaluate(args, loader, generator, sample_generator, sampler , gt_coll=False,
         ade = np.mean(ade_outer) #/ (total_traj * args.pred_len)
         fde = np.mean(fde_outer) #/ (total_traj)
         act = coll_pro_szenes_fake / count_szenes_fake
-        if gt_coll:
-            act_gt = coll_pro_szenes / count_szenes
-            return ade, fde, act, act_gt
+        # if gt_coll:
+        #     act_gt = coll_pro_szenes / count_szenes
+        #     return ade, fde, act, act_gt
         return ade, fde, act, 0
 
 # check if trajnet is set to True in config if trajnet evaluation
 # EXPERIMENT_NAME = 'SIM2Goal-ETHandUCY' # please change in config Trajnet =False
-EXPERIMENT_NAME ='SIM2Goal-TrajNet'  # please change in config Trajnet =True
+EXPERIMENT_NAME ='SIM2Goal-ETHandUCY'  # please change in config Trajnet =True
 _dir = os.path.dirname(__file__)
 _dir = _dir.split("/")[:-1]
 _dir = "/".join(_dir) + "/Sim2Goal/models/weights/"
diff --git a/experiments/.DS_Store b/experiments/.DS_Store
deleted file mode 100644
index d8edd03..0000000
Binary files a/experiments/.DS_Store and /dev/null differ
diff --git a/experiments/__init__.py b/experiments/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/experiments/eval_SimNoGoal.py b/experiments/eval_SimNoGoal.py
deleted file mode 100644
index ed4887b..0000000
--- a/experiments/eval_SimNoGoal.py
+++ /dev/null
@@ -1,211 +0,0 @@
-import argparse
-import os
-
-os.environ["CUDA_VISIBLE_DEVICES"] = "0"
-import torch
-
-# torch.backends.cudnn.benchmark = False
-# torch.backends.cudnn.enabled = True
-
-from config import *
-import numpy as np
-config = Config()
-from data.loader import data_loader
-from models.SIMnoGoal import TrajectoryGenerator
-from utils.utils import (
-    relative_to_abs,
-    get_dset_path,
-    plot_best
-)
-from utils.losses import displacement_error
-from attrdict import AttrDict
-
-# Change to your path here!
-DIR = home +'/Documents/NFTraj/SIMnoGoal_TrajNet-lcas/checkpoint_with_model.pt'
-
-
-parser = argparse.ArgumentParser()
-parser.add_argument('--model_path', default=DIR, type=str)
-parser.add_argument('--num_samples', default=20, type=int)
-parser.add_argument('--dset_type', default='test', type=str)
-
-seed = config.seed
-torch.manual_seed(seed)
-np.random.seed(seed)
-# import locale
-# locale.setlocale(locale.LC_ALL, 'de_DE.utf8')
-
-def evaluate_helper(error, seq_start_end):
-    sum_ = 0
-    error = torch.stack(error, dim=1)
-    ids = []
-    for (start, end) in seq_start_end:
-        start = start.item()
-        end = end.item()
-        _error = error[start:end]
-        _error = torch.sum(_error, dim=0).unsqueeze(dim=1)
-        _error, id = torch.min(_error, 0)
-        ids.append(id.squeeze().item())
-        sum_ += _error.squeeze()
-    return sum_, ids
-
-
-def get_generator(checkpoint):
-    model = TrajectoryGenerator(config)
-    model.load_state_dict(checkpoint["best_state"])
-    model.cuda()
-    model.eval()
-    return model
-
-def compute_ade(predicted_trajs, gt_traj, val_mask):
-    error = np.linalg.norm(predicted_trajs - gt_traj, axis=-1)
-    ade = np.mean(error, axis=0)
-    return ade.flatten()
-
-
-def compute_fde(predicted_trajs, gt_traj, val_mask):
-    final_error = np.linalg.norm(predicted_trajs[-1] - gt_traj[-1], axis=-1)
-    return final_error.flatten()
-
-
-def cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask):
-
-    ade_col = displacement_error(pred_traj_fake, pred_traj_gt, val_mask, mode="raw")
-    fde = compute_fde(pred_traj_fake.cpu().numpy(), pred_traj_gt.cpu().numpy(), val_mask[-1])
-    ade = compute_ade(pred_traj_fake.cpu().numpy(), pred_traj_gt.cpu().numpy(), val_mask)
-    return ade, fde, ade_col
-
-def fast_coll_counter(pred_batch,seq_start_end, ids, mask):
-    ids_of_col_szenes = np.zeros([seq_start_end.shape[0]])
-    coll_pro_szene = 0
-    stack_of_coll_indeces = []
-    pred_batch = torch.stack(pred_batch)
-    if not ids:
-        mask = mask.unsqueeze(dim=0).repeat(config.num_samples, 1, 1, 1)
-        mask = mask.view(config.num_samples * 12, pred_batch.shape[2], pred_batch.shape[2])
-        pred = pred_batch.view(config.num_samples * 12, pred_batch.shape[2], 2)
-    for i, (start, end) in enumerate(seq_start_end):
-        start = start.item()
-        end = end.item()
-        if ids:
-            pred = pred_batch[ids[i]]
-        currSzene = pred[:, start:end]
-        dist_mat = torch.cdist(currSzene, currSzene, p=2.0, compute_mode='donot_use_mm_for_euclid_dist')
-        dist_mat_one_triu = torch.triu(dist_mat)
-        dist_mat_one_triu = dist_mat_one_triu #* mask[:,start:end, start:end]
-        filter_zeros = torch.logical_and(0. != dist_mat_one_triu,  dist_mat_one_triu <= config.collision_distance)
-        filter_col_pos = torch.logical_and(0. != dist_mat,  dist_mat <= config.collision_distance)
-        filter_zeros_sum=filter_zeros.sum().unsqueeze(dim=0).item()
-        coll_pro_szene += filter_zeros_sum
-        if filter_zeros_sum > 0.:
-            ids_of_col_szenes[i] = 1
-        stack_of_coll_indeces.append(filter_col_pos)
-    count = len(seq_start_end) #* count_empty
-    if not ids:
-        count = count * config.num_samples
-    return coll_pro_szene, count, ids_of_col_szenes, stack_of_coll_indeces
-
-def evaluate(args, loader, generator, gt_coll=False, plot_traj=False, plot_sample=False):
-    ade_outer, fde_outer = [], []
-    total_traj = 0
-    coll_pro_szenes_fake = 0.
-    count_szenes_fake = 0.
-    coll_pro_szenes = 0.
-    count_szenes = 0.
-    szene_id = 0.
-    samples = None
-    with torch.no_grad():
-        for batch in loader:
-            batch = [tensor.cuda() for tensor in batch]
-            obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
-             loss_mask, seq_start_end, nei_num_index, nei_num = batch
-            ade, fde, ade_col = [], [], []
-            total_traj += nei_num.sum()
-            batch_pred_traj_fake = []
-            batch_samples = []
-            att_score_list_batch = []
-            model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-            val_mask = val_mask[-config.pred_len :]
-            for _ in range(args.num_samples):
-                if plot_sample:
-                    pred_traj_fake_rel, samples = generator(model_input, obs_traj, pred_traj_gt,
-                                                   seq_start_end, nei_num_index, nei_num, plot_sample=plot_sample)
-                else:
-                    pred_traj_fake_rel = generator(model_input, obs_traj, pred_traj_gt,
-                                                    seq_start_end, nei_num_index, nei_num)
-                pred_traj_fake_rel = pred_traj_fake_rel[-args.pred_len :]
-                pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-                # pred_traj_fake = pred_traj_fake_rel
-                batch_samples.append(samples)
-                batch_pred_traj_fake.append(pred_traj_fake)
-                ade_, fde_, ade_col_ = cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask)
-                ade.append(ade_)
-                fde.append(fde_)
-                ade_col.append(ade_col_)
-
-            _, ids = evaluate_helper(ade_col, seq_start_end)
-            ade_sum = np.array(ade)
-            fde_sum = np.array(fde)
-            fde_sum = np.min(fde_sum, axis=0)
-            fde_outer = np.hstack([fde_outer, fde_sum])
-            ade_sum = np.min(ade_sum, axis=0)
-            ade_outer = np.hstack([ade_outer, ade_sum])
-
-            coll_pro_szene_fake, count_fake, ids_of_col_szenes_fake, stack_of_coll_indeces = fast_coll_counter(batch_pred_traj_fake,
-                                                                                        seq_start_end, None, nei_num_index)
-            coll_pro_szenes_fake += coll_pro_szene_fake
-            count_szenes_fake += count_fake
-            if(plot_traj):
-                plot_best(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
-                               stack_of_coll_indeces, seq_start_end,
-                               ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
-
-                #
-                # plot_multimodal(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
-                #                stack_of_coll_indeces, seq_start_end,
-                #                ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
-            szene_id += 1
-
-        ade = np.mean(ade_outer) #/ (total_traj * args.pred_len)
-        fde = np.mean(fde_outer) #/ (total_traj)
-        act = coll_pro_szenes_fake / count_szenes_fake
-        if gt_coll:
-            act_gt = coll_pro_szenes / count_szenes
-            return ade, fde, act, act_gt
-        return ade, fde, act, 0
-
-
-def main(args):
-    if os.path.isdir(args.model_path):
-        filenames = os.listdir(args.model_path)
-        filenames.sort()
-        paths = [
-            os.path.join(args.model_path, file_) for file_ in filenames
-        ]
-    else:
-        paths = [args.model_path]
-    for path in paths:
-        checkpoint = torch.load(path)
-        generator = get_generator(checkpoint)
-        _args = AttrDict(checkpoint['args'])
-        path = get_dset_path(_args.dataset_name, args.dset_type)
-
-        _, loader = data_loader(_args, path)
-        ade, fde, act, act_gt = evaluate(_args, loader, generator, gt_coll=False, plot_traj=False, plot_sample=False)
-        print(
-            "Dataset:  {} \n"
-            "Pred Len: {}, ADE, FDE, ACT,ACT_gt \n"
-            "{:.4f} \n" 
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f}".format(
-                _args.dataset_name, _args.pred_len, ade, fde, act, act_gt
-            )
-        )
-
-
-if __name__ == "__main__":
-    args = parser.parse_args()
-    # torch.backends.cudnn.deterministic = True
-    # torch.backends.cudnn.benchmark = False
-    main(args)
diff --git a/experiments/evaluate_GAN.py b/experiments/evaluate_GAN.py
deleted file mode 100644
index 132d3b6..0000000
--- a/experiments/evaluate_GAN.py
+++ /dev/null
@@ -1,209 +0,0 @@
-import argparse
-import os
-os.environ["CUDA_VISIBLE_DEVICES"] = "0"
-import torch
-from config import *
-import numpy as np
-config = Config()
-from data.loader import data_loader
-from experiments.models.CoLoss_GAN import TrajectoryGenerator
-from utils.utils import (
-    relative_to_abs,
-    get_dset_path,
-    plot_multimodal,
-    plot_best
-)
-from utils.losses import displacement_error
-from attrdict import AttrDict
-
-seed = config.seed
-torch.manual_seed(seed)
-np.random.seed(seed)
-
-
-def evaluate_helper(error, seq_start_end):
-    sum_ = 0
-    error = torch.stack(error, dim=1)
-    ids = []
-    for (start, end) in seq_start_end:
-        start = start.item()
-        end = end.item()
-        _error = error[start:end]
-        _error = torch.sum(_error, dim=0).unsqueeze(dim=1)
-        _error, id = torch.min(_error, 0)
-        ids.append(id.squeeze().item())
-        sum_ += _error.squeeze()
-    return sum_, ids
-
-
-def get_generator(checkpoint):
-    model = TrajectoryGenerator(config)
-    model.load_state_dict(checkpoint["g_best_state"]) # g_best_state for Coloss-GAN
-    model.cuda()
-    model.eval()
-    return model
-
-def compute_ade(predicted_trajs, gt_traj, val_mask):
-    error = np.linalg.norm(predicted_trajs - gt_traj, axis=-1)
-    ade = np.mean(error, axis=0)
-    return ade.flatten()
-
-
-def compute_fde(predicted_trajs, gt_traj, val_mask):
-    final_error = np.linalg.norm(predicted_trajs[-1] - gt_traj[-1], axis=-1)
-    return final_error.flatten()
-
-
-def cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask):
-
-    ade_col = displacement_error(pred_traj_fake, pred_traj_gt, val_mask, mode="raw")
-    fde = compute_fde(pred_traj_fake.cpu().numpy(), pred_traj_gt.cpu().numpy(), val_mask[-1])
-    ade = compute_ade(pred_traj_fake.cpu().numpy(), pred_traj_gt.cpu().numpy(), val_mask)
-    return ade, fde, ade_col
-
-def fast_coll_counter(pred_batch,seq_start_end, ids, mask):
-    ids_of_col_szenes = np.zeros([seq_start_end.shape[0]])
-    coll_pro_szene = 0
-    stack_of_coll_indeces = []
-    pred_batch = torch.stack(pred_batch)
-    if not ids:
-        mask = mask.unsqueeze(dim=0).repeat(config.num_samples, 1, 1, 1)
-        mask = mask.view(config.num_samples * 12, pred_batch.shape[2], pred_batch.shape[2])
-        pred = pred_batch.view(config.num_samples * 12, pred_batch.shape[2], 2)
-    for i, (start, end) in enumerate(seq_start_end):
-        start = start.item()
-        end = end.item()
-        if ids:
-            pred = pred_batch[ids[i]]
-        currSzene = pred[:, start:end]
-        dist_mat = torch.cdist(currSzene, currSzene, p=2.0, compute_mode='donot_use_mm_for_euclid_dist')
-        dist_mat_one_triu = torch.triu(dist_mat)
-        dist_mat_one_triu = dist_mat_one_triu #* mask[:,start:end, start:end]
-        filter_zeros = torch.logical_and(0. != dist_mat_one_triu,  dist_mat_one_triu <= config.collision_distance)
-        filter_col_pos = torch.logical_and(0. != dist_mat,  dist_mat <= config.collision_distance)
-        filter_zeros_sum=filter_zeros.sum().unsqueeze(dim=0).item()
-        coll_pro_szene += filter_zeros_sum
-        if filter_zeros_sum > 0.:
-            ids_of_col_szenes[i] = 1
-        stack_of_coll_indeces.append(filter_col_pos)
-    count = len(seq_start_end) #* count_empty
-    if not ids:
-        count = count * config.num_samples
-    return coll_pro_szene, count, ids_of_col_szenes, stack_of_coll_indeces
-
-def evaluate(args, loader, generator, gt_coll=False, plot_traj=False, plot_sample=False):
-    ade_outer, fde_outer = [], []
-    total_traj = 0
-    coll_pro_szenes_fake = 0.
-    count_szenes_fake = 0.
-    coll_pro_szenes = 0.
-    count_szenes = 0.
-    szene_id = 0.
-    samples = None
-    with torch.no_grad():
-        for batch in loader:
-            batch = [tensor.cuda() for tensor in batch]
-            obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
-             loss_mask, seq_start_end, nei_num_index, nei_num = batch
-            ade, fde, ade_col = [], [], []
-            total_traj += nei_num.sum()
-            batch_pred_traj_fake = []
-            batch_samples = []
-            att_score_list_batch = []
-            model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-            val_mask = val_mask[-config.pred_len :]
-            loss_mask = loss_mask[-config.pred_len:]
-            for _ in range(args.num_samples):
-                if plot_sample:
-                    pred_traj_fake_rel, samples = generator(model_input, obs_traj, pred_traj_gt,
-                                                   seq_start_end, nei_num_index, nei_num, 0)
-                else:
-                    pred_traj_fake_rel = generator(model_input, obs_traj, pred_traj_gt,
-                                                    seq_start_end, nei_num_index, nei_num, 0)
-                pred_traj_fake_rel = pred_traj_fake_rel[-args.pred_len :]
-                pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-                # pred_traj_fake = pred_traj_fake_rel
-                batch_samples.append(samples)
-                batch_pred_traj_fake.append(pred_traj_fake)
-                ade_, fde_, ade_col_ = cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask)
-                ade.append(ade_)
-                fde.append(fde_)
-                ade_col.append(ade_col_)
-
-            _, ids = evaluate_helper(ade_col, seq_start_end)
-            ade_sum = np.array(ade)
-            fde_sum = np.array(fde)
-            fde_sum = np.min(fde_sum, axis=0)
-            fde_outer = np.hstack([fde_outer, fde_sum])
-            ade_sum = np.min(ade_sum, axis=0)
-            ade_outer = np.hstack([ade_outer, ade_sum])
-
-            coll_pro_szene_fake, count_fake, ids_of_col_szenes_fake, stack_of_coll_indeces = fast_coll_counter(batch_pred_traj_fake,
-                                                                                        seq_start_end, None, nei_num_index)
-            coll_pro_szenes_fake += coll_pro_szene_fake
-            count_szenes_fake += count_fake
-            if(plot_traj):
-                plot_best(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
-                               stack_of_coll_indeces, seq_start_end,
-                               ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
-
-                #
-                # plot_multimodal(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
-                #                stack_of_coll_indeces, seq_start_end,
-                #                ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
-            szene_id += 1
-
-        ade = np.mean(ade_outer) #/ (total_traj * args.pred_len)
-        fde = np.mean(fde_outer) #/ (total_traj)
-        act = coll_pro_szenes_fake / count_szenes_fake
-        if gt_coll:
-            act_gt = coll_pro_szenes / count_szenes
-            return ade, fde, act, act_gt
-        return ade, fde, act, 0
-
-
-# EXPERIMENT_NAME = 'SGAN_ETHandUTC' # please change in config Trajnet = False, AND IMPORT SGAN
-# EXPERIMENT_NAME = 'CoLoss-GAN_ETHandUTC' # please change in config Trajnet = False AND IMPORT Coloss gan
-EXPERIMENT_NAME ='CoLoss-GAN_TrajNet'  # please change in config Trajnet =True
-_dir = os.path.dirname(__file__)
-_dir = _dir.split("/")[:-2]
-_dir = "/".join(_dir) + "/Sim2Goal/models/weights/"
-DIR = _dir + EXPERIMENT_NAME + '-' # -eth and so on
-# /checkpoint_with_model.pt
-parser = argparse.ArgumentParser()
-parser.add_argument('--model_path', default=DIR, type=str)
-parser.add_argument('--num_samples', default=20, type=int)
-parser.add_argument('--dset_type', default='test', type=str)
-
-
-def main(args):
-    paths = []
-    if config.trajnet:
-        datasets = ['hotel', 'lcas', 'students1', 'students3', 'wildtrack', 'zara1', 'zara3']
-    else:
-        datasets = ['eth', 'hotel', 'univ', 'zara1', 'zara2']
-    for i in datasets:
-        paths.append(args.model_path+i + '/checkpoint_with_model.pt')
-    # paths = [args.model_path] + datasets
-    for path in paths:
-        checkpoint = torch.load(path)
-        generator = get_generator(checkpoint)
-        _args = AttrDict(checkpoint['args'])
-        path = get_dset_path(_args.dataset_name, args.dset_type)
-
-        _, loader = data_loader(_args, path)
-        ade, fde, act, act_gt = evaluate(_args, loader, generator, gt_coll=False, plot_traj=False, plot_sample=False)
-        print(
-            "Dataset:  {} \n"
-            "Pred Len: {}, ADE, FDE, ACT,ACT_gt \n"
-            "{:.4f} \n" 
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f}".format(
-                _args.dataset_name, _args.pred_len, ade, fde, act, act_gt
-            )
-        )
-
-if __name__ == "__main__":
-    args = parser.parse_args()
-    main(args)
diff --git a/experiments/evaluate_VAE.py b/experiments/evaluate_VAE.py
deleted file mode 100644
index 9dc34fd..0000000
--- a/experiments/evaluate_VAE.py
+++ /dev/null
@@ -1,224 +0,0 @@
-import argparse
-import os
-
-os.environ["CUDA_VISIBLE_DEVICES"] = "0"
-import torch
-
-# torch.backends.cudnn.benchmark = False
-# torch.backends.cudnn.enabled = True
-
-from config import *
-import numpy as np
-config = Config()
-from data.loader import data_loader
-from experiments.models.VAE import TrajectoryGenerator
-from utils.utils import (
-    relative_to_abs,
-    get_dset_path,
-    plot_multimodal,
-    plot_best
-)
-from utils.losses import displacement_error
-from attrdict import AttrDict
-
-seed = config.seed
-torch.manual_seed(seed)
-np.random.seed(seed)
-
-
-def evaluate_helper(error, seq_start_end):
-    sum_ = 0
-    error = torch.stack(error, dim=1)
-    ids = []
-    for (start, end) in seq_start_end:
-        start = start.item()
-        end = end.item()
-        _error = error[start:end]
-        _error = torch.sum(_error, dim=0).unsqueeze(dim=1)
-        _error, id = torch.min(_error, 0)
-        ids.append(id.squeeze().item())
-        sum_ += _error.squeeze()
-    return sum_, ids
-
-
-def get_generator(checkpoint):
-    model = TrajectoryGenerator(config)
-    model.load_state_dict(checkpoint["best_state"]) # g_best_state for Coloss-GAN
-    model.cuda()
-    model.eval()
-    return model
-
-def compute_ade(predicted_trajs, gt_traj, val_mask):
-    error = np.linalg.norm(predicted_trajs - gt_traj, axis=-1)
-    ade = np.mean(error, axis=0)
-    return ade.flatten()
-
-
-def compute_fde(predicted_trajs, gt_traj, val_mask):
-    final_error = np.linalg.norm(predicted_trajs[-1] - gt_traj[-1], axis=-1)
-    return final_error.flatten()
-
-
-def cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask):
-
-    ade_col = displacement_error(pred_traj_fake, pred_traj_gt, val_mask, mode="raw")
-    fde = compute_fde(pred_traj_fake.cpu().numpy(), pred_traj_gt.cpu().numpy(), val_mask[-1])
-    ade = compute_ade(pred_traj_fake.cpu().numpy(), pred_traj_gt.cpu().numpy(), val_mask)
-    return ade, fde, ade_col
-
-def fast_coll_counter(pred_batch,seq_start_end, ids, mask):
-    ids_of_col_szenes = np.zeros([seq_start_end.shape[0]])
-    coll_pro_szene = 0
-    stack_of_coll_indeces = []
-    pred_batch = torch.stack(pred_batch)
-    if not ids:
-        mask = mask.unsqueeze(dim=0).repeat(config.num_samples, 1, 1, 1)
-        mask = mask.view(config.num_samples * 12, pred_batch.shape[2], pred_batch.shape[2])
-        pred = pred_batch.view(config.num_samples * 12, pred_batch.shape[2], 2)
-    for i, (start, end) in enumerate(seq_start_end):
-        start = start.item()
-        end = end.item()
-        if ids:
-            pred = pred_batch[ids[i]]
-        currSzene = pred[:, start:end]
-        dist_mat = torch.cdist(currSzene, currSzene, p=2.0, compute_mode='donot_use_mm_for_euclid_dist')
-        dist_mat_one_triu = torch.triu(dist_mat)
-        dist_mat_one_triu = dist_mat_one_triu #* mask[:,start:end, start:end]
-        filter_zeros = torch.logical_and(0. != dist_mat_one_triu,  dist_mat_one_triu <= config.collision_distance)
-        filter_col_pos = torch.logical_and(0. != dist_mat,  dist_mat <= config.collision_distance)
-        filter_zeros_sum=filter_zeros.sum().unsqueeze(dim=0).item()
-        coll_pro_szene += filter_zeros_sum
-        if filter_zeros_sum > 0.:
-            ids_of_col_szenes[i] = 1
-        stack_of_coll_indeces.append(filter_col_pos)
-    count = len(seq_start_end) #* count_empty
-    if not ids:
-        count = count * config.num_samples
-    return coll_pro_szene, count, ids_of_col_szenes, stack_of_coll_indeces
-
-def evaluate(args, loader, generator, gt_coll=False, plot_traj=False, plot_sample=False):
-    ade_outer, fde_outer = [], []
-    total_traj = 0
-    coll_pro_szenes_fake = 0.
-    count_szenes_fake = 0.
-    coll_pro_szenes = 0.
-    count_szenes = 0.
-    szene_id = 0.
-    samples = None
-    with torch.no_grad():
-        for batch in loader:
-            batch = [tensor.cuda() for tensor in batch]
-            obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
-             loss_mask, seq_start_end, nei_num_index, nei_num = batch
-            ade, fde, ade_col = [], [], []
-            total_traj += nei_num.sum()
-            batch_pred_traj_fake = []
-            batch_samples = []
-            att_score_list_batch = []
-            model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-            val_mask = val_mask[-config.pred_len :]
-            loss_mask = loss_mask[-config.pred_len:]
-            for _ in range(args.num_samples):
-                if plot_sample:
-                    pred_traj_fake_rel, samples = generator(model_input, obs_traj, pred_traj_gt,
-                                                   seq_start_end, nei_num_index, nei_num)
-                else:
-                    pred_traj_fake_rel = generator(model_input, obs_traj, pred_traj_gt,
-                                                    seq_start_end, nei_num_index, nei_num,mode='test')
-                pred_traj_fake_rel = pred_traj_fake_rel[-args.pred_len :]
-                pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-                # pred_traj_fake = pred_traj_fake_rel
-                batch_samples.append(samples)
-                batch_pred_traj_fake.append(pred_traj_fake)
-                ade_, fde_, ade_col_ = cal_ade_fde(pred_traj_gt, pred_traj_fake, val_mask)
-                ade.append(ade_)
-                fde.append(fde_)
-                ade_col.append(ade_col_)
-
-            _, ids = evaluate_helper(ade_col, seq_start_end)
-            ade_sum = np.array(ade)
-            fde_sum = np.array(fde)
-            fde_sum = np.min(fde_sum, axis=0)
-            fde_outer = np.hstack([fde_outer, fde_sum])
-            ade_sum = np.min(ade_sum, axis=0)
-            ade_outer = np.hstack([ade_outer, ade_sum])
-
-            coll_pro_szene_fake, count_fake, ids_of_col_szenes_fake, stack_of_coll_indeces = fast_coll_counter(batch_pred_traj_fake,
-                                                                                        seq_start_end, None, nei_num_index)
-            coll_pro_szenes_fake += coll_pro_szene_fake
-            count_szenes_fake += count_fake
-            if(plot_traj):
-                plot_best(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
-                               stack_of_coll_indeces, seq_start_end,
-                               ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
-
-                #
-                # plot_multimodal(obs_traj, pred_traj_gt, batch_pred_traj_fake, att_score_list_batch, ids,
-                #                stack_of_coll_indeces, seq_start_end,
-                #                ids_of_col_szenes_fake, loss_mask, config, szene_id, batch_samples)
-            szene_id += 1
-
-        ade = np.mean(ade_outer) #/ (total_traj * args.pred_len)
-        fde = np.mean(fde_outer) #/ (total_traj)
-        act = coll_pro_szenes_fake / count_szenes_fake
-        if gt_coll:
-            act_gt = coll_pro_szenes / count_szenes
-            return ade, fde, act, act_gt
-        return ade, fde, act, 0
-
-# # Change to your path here!
-# # DIR = home +'/Documents/NFTraj/Test-eth-AF_with_col/checkpoint_with_model.pt'
-# DIR = home +'/Documents/NFTraj/CoLoss-GAN_TrajNet-hotel/checkpoint_with_model.pt'
-# # DIR = home +'/Documents/NFTraj/AR_191_noCV-eth/checkpoint_with_model.pt'
-#
-# parser = argparse.ArgumentParser()
-# parser.add_argument('--model_path', default=DIR, type=str)
-# parser.add_argument('--num_samples', default=20, type=int)
-# parser.add_argument('--dset_type', default='test', type=str)
-
-# check if trajnet is set to True in config if trajnet evaluation
-EXPERIMENT_NAME = 'VAE_Trajnet'  # please copy vae config into config
-
-_dir = os.path.dirname(__file__)
-_dir = _dir.split("/")[:-2]
-_dir = "/".join(_dir) + "/Sim2Goal/models/weights/"
-DIR = _dir + EXPERIMENT_NAME + '-' # -eth and so on
-# /checkpoint_with_model.pt
-parser = argparse.ArgumentParser()
-parser.add_argument('--model_path', default=DIR, type=str)
-parser.add_argument('--num_samples', default=20, type=int)
-parser.add_argument('--dset_type', default='test', type=str)
-
-
-def main(args):
-    paths = []
-    if config.trajnet:
-        datasets = ['hotel', 'lcas', 'students1', 'students3', 'wildtrack', 'zara1', 'zara3']
-    else:
-        datasets = ['eth', 'hotel', 'univ', 'zara1', 'zara2']
-    for i in datasets:
-        paths.append(args.model_path+i + '/checkpoint_with_model.pt')
-    # paths = [args.model_path] + datasets
-    for path in paths:
-        checkpoint = torch.load(path)
-        generator = get_generator(checkpoint)
-        _args = AttrDict(checkpoint['args'])
-        path = get_dset_path(_args.dataset_name, args.dset_type)
-
-        _, loader = data_loader(_args, path)
-        ade, fde, act, act_gt = evaluate(_args, loader, generator, gt_coll=False, plot_traj=False, plot_sample=False)
-        print(
-            "Dataset:  {} \n"
-            "Pred Len: {}, ADE, FDE, ACT,ACT_gt \n"
-            "{:.4f} \n" 
-            "{:.4f} \n"
-            "{:.4f} \n"
-            "{:.4f}".format(
-                _args.dataset_name, _args.pred_len, ade, fde, act, act_gt
-            )
-        )
-
-
-if __name__ == "__main__":
-    args = parser.parse_args()
-    main(args)
diff --git a/experiments/models/CoLoss_GAN.py b/experiments/models/CoLoss_GAN.py
deleted file mode 100644
index 6ddbafc..0000000
--- a/experiments/models/CoLoss_GAN.py
+++ /dev/null
@@ -1,209 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from config import *
-config = Config()
-from models.utils import Pooling_net
-
-
-
-def get_noise(shape, noise_type):
-    if noise_type == 'gaussian':
-        return torch.randn(*shape).to('cuda')
-    elif noise_type == 'uniform':
-        return torch.rand(*shape).sub_(0.5).mul_(2.0).to('cuda')
-    raise ValueError('Unrecognized noise type "%s"' % noise_type)
-
-
-class TrajectoryGenerator(nn.Module):
-    def __init__(self,config):
-        super(TrajectoryGenerator, self).__init__()
-        self.config = config
-        obs_len= config.obs_len
-        pred_len= config.pred_len
-        traj_lstm_input_size= 2
-        traj_lstm_hidden_size=16
-        rela_embed_size = 16
-        self.inputLayer_encoder = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.inputLayer_decoder = nn.Linear(traj_lstm_input_size + 16, rela_embed_size)
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-        self.pl_net = Pooling_net(h_dim=traj_lstm_hidden_size)
-
-        self.traj_lstm_hidden_size = traj_lstm_hidden_size
-        self.traj_lstm_input_size = traj_lstm_input_size
-        self.pred_lstm_hidden_size = self.traj_lstm_hidden_size
-        self.traj_lstm_model = nn.LSTMCell(rela_embed_size, 8)
-        self.pred_lstm_model =  nn.LSTMCell(rela_embed_size, 16)
-        self.pred_hidden2pos = nn.Linear(traj_lstm_hidden_size, 2)
-
-        self.init_parameters()
-        self.noise_dim = (8,)
-        self.noise_type = 'gaussian'
-        self.noise_mix_type = 'global'
-
-    def init_parameters(self):
-        nn.init.constant_(self.inputLayer_encoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder.weight, std=0.2)
-
-        nn.init.constant_(self.inputLayer_decoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_decoder.weight, std=0.2)
-
-        nn.init.xavier_uniform_(self.traj_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.traj_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.traj_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.traj_lstm_model.bias_hh, 0.0)
-        n = self.traj_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.traj_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-        nn.init.xavier_uniform_(self.pred_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.pred_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.pred_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.pred_lstm_model.bias_hh, 0.0)
-        n = self.pred_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.pred_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-        nn.init.constant_(self.pred_hidden2pos.bias, 0.0)
-        nn.init.normal_(self.pred_hidden2pos.weight, std=0.1)
-
-    def init_hidden_traj_lstm(self, batch):
-        return (
-            torch.randn(batch, 8).cuda(),
-            torch.randn(batch, 8).cuda(),
-        )
-
-    def add_noise(self, _input, seq_start_end, user_noise=None):
-        if not self.noise_dim:
-            return _input
-
-        if self.noise_mix_type == 'global':
-            noise_shape = (seq_start_end.size(0),) + self.noise_dim
-        else:
-            noise_shape = (_input.size(0),) + self.noise_dim
-
-        if user_noise is not None:
-            z_decoder = user_noise
-        else:
-            z_decoder = get_noise(noise_shape, self.noise_type)
-
-        if self.noise_mix_type == 'global':
-            _list = []
-            for idx, (start, end) in enumerate(seq_start_end):
-                start = start.item()
-                end = end.item()
-                _vec = z_decoder[idx].view(1, -1)
-                _to_cat = _vec.repeat(end - start, 1)  # TODO: maybe not repeat!
-                _list.append(torch.cat([_input[start:end], _to_cat], dim=1))
-            decoder_h = torch.cat(_list, dim=0)
-            return decoder_h
-
-        decoder_h = torch.cat([_input, z_decoder], dim=1)
-
-        return decoder_h
-
-    def forward(self, traj_rel, obs_traj_pos, pred_traj_gt_pos, seq_start_end,
-                nei_index, nei_num_index, plot_att=False):
-        batch = traj_rel.shape[1]
-        pred_traj_rel = []
-        traj_lstm_h_t, traj_lstm_c_t = self.init_hidden_traj_lstm(batch)
-        for i, input_t in enumerate(traj_rel[: self.obs_len].chunk(
-                traj_rel[: self.obs_len].size(0), dim=0)):
-            input_embedded = F.relu(self.inputLayer_encoder(input_t.squeeze(0)))
-            lstm_state = self.traj_lstm_model(
-                input_embedded, (traj_lstm_h_t, traj_lstm_c_t)
-            )
-            traj_lstm_h_t, traj_lstm_c_t = lstm_state
-
-        pred_lstm_hidden = self.add_noise(traj_lstm_h_t, seq_start_end)
-        pred_lstm_c_t = torch.zeros_like(pred_lstm_hidden).cuda()
-        output  = torch.zeros_like(traj_rel[self.obs_len - 1]).cuda()
-        lstm_state_context = torch.zeros_like(pred_lstm_hidden).cuda()
-        curr_pos_abs =  obs_traj_pos[-1]
-        for i in range(self.pred_len):
-            input_cat = torch.cat([lstm_state_context.detach(),output.detach()], dim=-1)
-            input_embedded = F.relu(self.inputLayer_decoder(input_cat)) # detach from history as input
-            lstm_state = self.pred_lstm_model(
-                input_embedded, (pred_lstm_hidden, pred_lstm_c_t)
-            )
-            pred_lstm_hidden = lstm_state[0]
-            pred_lstm_c_t = lstm_state[1]
-            corr = curr_pos_abs.repeat(batch, 1, 1)
-            corr_index = (corr.transpose(0,1)-corr)
-            lstm_state_hidden = lstm_state[0]
-            lstm_state_context, _, _ = self.pl_net(corr_index, nei_index[i], nei_num_index, lstm_state_hidden, curr_pos_abs)
-            concat_output = lstm_state_context + lstm_state_hidden
-            output =  torch.tanh(self.pred_hidden2pos(concat_output))* 4.4
-            curr_pos_abs = (curr_pos_abs + output).detach() # detach from history as input
-            pred_traj_rel += [output]
-        if plot_att:
-
-            return torch.stack(pred_traj_rel), None
-        else:
-            return torch.stack(pred_traj_rel)
-
-class Discriminator(nn.Module):
-    def __init__(self,config):
-        super(Discriminator, self).__init__()
-        self.config = config
-        obs_len= config.obs_len
-        pred_len= config.pred_len
-        traj_lstm_input_size= 2
-        traj_lstm_hidden_size=16
-        rela_embed_size = 16
-        self.inputLayer_encoder = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.inputLayer_decoder = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-        self.dropout = nn.Dropout(0.00)
-        self.gcn = Pooling_net(h_dim=traj_lstm_hidden_size)
-        self.traj_lstm_hidden_size = traj_lstm_hidden_size
-        self.traj_lstm_input_size = traj_lstm_input_size
-
-        self.pred_lstm_hidden_size = self.traj_lstm_hidden_size
-
-        self.pred_hidden2class = nn.Linear(64, 1)
-
-        self.init_parameters()
-
-        self.layer1 = nn.Linear(2*(config.pred_len + config.obs_len)+16 * 12, 32)
-        self.layer2 = nn.Linear(32, 64)
-
-
-    def init_parameters(self):
-        nn.init.constant_(self.inputLayer_encoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder.weight, std=0.2)
-
-        nn.init.constant_(self.pred_hidden2class.bias, 0.0)
-        nn.init.normal_(self.pred_hidden2class.weight, std=0.2)
-
-    def init_hidden_traj_lstm(self, batch, hidden_size):
-        return (
-            torch.randn(batch, hidden_size).cuda(),
-            torch.randn(batch, hidden_size).cuda(),
-        )
-
-    def forward(self, obs_rel, traj_rel_pred, obs_traj_pos,
-                nei_index, nei_num_index, loss_mask,plot_att=False):
-        batch = obs_rel.shape[1]
-        traj_rel_pred = traj_rel_pred * loss_mask
-        curr_pos_abs = obs_traj_pos[-1].detach()
-        inter_list = []
-        for j, input_t_j in enumerate(traj_rel_pred):
-            input_embedded = F.leaky_relu(self.inputLayer_decoder(input_t_j)) # detach from history as input
-            corr = curr_pos_abs.repeat(batch, 1, 1)
-            corr_index = (corr.transpose(0,1)-corr)
-            s_context, _, _ = self.gcn(corr_index, nei_index[j], nei_num_index, input_embedded, curr_pos_abs)
-            curr_pos_abs = (curr_pos_abs + input_t_j)
-            inter_list.append(s_context)
-        inter_list = torch.stack(inter_list, dim=1).view(-1, 16 * 12)
-        rel_traj = torch.cat([obs_rel, traj_rel_pred])
-        rel_traj = rel_traj.permute(1, 0, 2).reshape(batch, -1)
-
-        x = torch.cat([rel_traj, inter_list], dim=-1)
-        x = F.leaky_relu(self.layer1(x))
-        x = F.leaky_relu(self.layer2(x))
-        cls = self.pred_hidden2class(x)
-
-        return cls
diff --git a/experiments/models/SGAN.py b/experiments/models/SGAN.py
deleted file mode 100644
index 265f5e3..0000000
--- a/experiments/models/SGAN.py
+++ /dev/null
@@ -1,650 +0,0 @@
-import torch
-import torch.nn as nn
-
-#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-
-device = 'cuda'
-def make_mlp(dim_list, activation='relu', batch_norm=True, dropout=0):
-    layers = []
-    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):
-        layers.append(nn.Linear(dim_in, dim_out))
-        if batch_norm:
-            layers.append(nn.BatchNorm1d(dim_out))
-        if activation == 'relu':
-            layers.append(nn.ReLU())
-        elif activation == 'leakyrelu':
-            layers.append(nn.LeakyReLU())
-        if dropout > 0:
-            layers.append(nn.Dropout(p=dropout))
-    return nn.Sequential(*layers)
-
-
-def get_noise(shape, noise_type):
-    if noise_type == 'gaussian':
-        return torch.randn(*shape).to(device)
-    elif noise_type == 'uniform':
-        return torch.rand(*shape).sub_(0.5).mul_(2.0).to(device)
-    raise ValueError('Unrecognized noise type "%s"' % noise_type)
-
-
-class Encoder(nn.Module):
-    """Encoder is part of both TrajectoryGenerator and
-    TrajectoryDiscriminator"""
-    def __init__(
-        self, embedding_dim=64, h_dim=64, mlp_dim=1024, num_layers=1,
-        dropout=0.0
-    ):
-        super(Encoder, self).__init__()
-
-        self.mlp_dim = 1024
-        self.h_dim = h_dim
-        self.embedding_dim = embedding_dim
-        self.num_layers = num_layers
-
-        self.encoder = nn.LSTM(
-            embedding_dim, h_dim, num_layers, dropout=dropout
-        )
-
-        self.spatial_embedding = nn.Linear(2, embedding_dim)
-
-       # self.spatial_embedding = nn.Sequential(
-       #     nn.Linear(2, embedding_dim),
-       #     nn.Tanh())
-
-    def init_hidden(self, batch):
-        return (
-            torch.zeros(self.num_layers, batch, self.h_dim).to(device),
-            torch.zeros(self.num_layers, batch, self.h_dim).to(device)
-        )
-
-    def forward(self, obs_traj):
-        """
-        Inputs:
-        - obs_traj: Tensor of shape (obs_len, batch, 2) # e.g. 8/12 or 20 for discriminator
-        Output:
-        - final_h: Tensor of shape (self.num_layers, batch, self.h_dim)
-        """
-        # Encode observed Trajectory
-        batch = obs_traj.size(1)
-        # obs_traj_embedding = self.spatial_embedding(obs_traj.view(-1, 2)) # flatten obs_traj [x,2]
-        obs_traj_embedding = self.spatial_embedding(obs_traj.contiguous().view(-1, 2))
-        obs_traj_embedding = obs_traj_embedding.view(
-            -1, batch, self.embedding_dim
-        )
-        state_tuple = self.init_hidden(batch)
-        output, state = self.encoder(obs_traj_embedding, state_tuple)
-        final_h = state[0]
-        return final_h
-
-
-class Decoder(nn.Module):
-    """Decoder is part of TrajectoryGenerator"""
-    def __init__(
-        self, seq_len, embedding_dim=64, h_dim=128, mlp_dim=1024, num_layers=1,
-        pool_every_timestep=True, dropout=0.0, bottleneck_dim=1024,
-        activation='relu', batch_norm=True, pooling_type='pool_net',
-        neighborhood_size=2.0, grid_size=8
-    ):
-        super(Decoder, self).__init__()
-
-        self.seq_len = seq_len
-        self.mlp_dim = mlp_dim
-        self.h_dim = h_dim
-        self.embedding_dim = embedding_dim
-        self.pool_every_timestep = pool_every_timestep
-
-        self.decoder = nn.LSTM(
-            embedding_dim, h_dim, num_layers, dropout=dropout
-        )
-
-        if pool_every_timestep:
-            if pooling_type == 'pool_net':
-                self.pool_net = PoolHiddenNet(
-                    embedding_dim=self.embedding_dim,
-                    h_dim=self.h_dim,
-                    mlp_dim=mlp_dim,
-                    bottleneck_dim=bottleneck_dim,
-                    activation=activation,
-                    batch_norm=batch_norm,
-                    dropout=dropout
-                )
-            elif pooling_type == 'spool':
-                self.pool_net = SocialPooling(
-                    h_dim=self.h_dim,
-                    activation=activation,
-                    batch_norm=batch_norm,
-                    dropout=dropout,
-                    neighborhood_size=neighborhood_size,
-                    grid_size=grid_size
-                )
-
-            mlp_dims = [h_dim + bottleneck_dim, mlp_dim, h_dim]
-            self.mlp = make_mlp(
-                mlp_dims,
-                activation=activation,
-                batch_norm=batch_norm,
-                dropout=dropout
-            )
-
-        self.spatial_embedding = nn.Linear(2, embedding_dim)
-
-      #  self.spatial_embedding = nn.Sequential(
-      #      nn.Linear(2, embedding_dim),
-      #      nn.Tanh())
-
-
-        self.hidden2pos = nn.Linear(h_dim, 2)
-
-    def forward(self, last_pos, last_pos_rel, state_tuple, seq_start_end):
-        """
-        Inputs:
-        - last_pos: Tensor of shape (batch, 2)
-        - last_pos_rel: Tensor of shape (batch, 2)
-        - state_tuple: (hh, ch) each tensor of shape (num_layers, batch, h_dim)
-        - seq_start_end: A list of tuples which delimit sequences within batch
-        Output:
-        - pred_traj: tensor of shape (self.seq_len, batch, 2)
-        """
-        batch = last_pos.size(0) # num of elements in batch
-        pred_traj_fake_rel = []
-        decoder_input = self.spatial_embedding(last_pos_rel)
-        decoder_input = decoder_input.view(1, batch, self.embedding_dim)
-
-        for _ in range(self.seq_len):
-            output, state_tuple = self.decoder(decoder_input, state_tuple)
-            rel_pos = self.hidden2pos(output.view(-1, self.h_dim))
-            curr_pos = rel_pos + last_pos
-
-            if self.pool_every_timestep:
-                decoder_h = state_tuple[0]
-                pool_h = self.pool_net(decoder_h, seq_start_end, curr_pos)
-                decoder_h = torch.cat(
-                    [decoder_h.view(-1, self.h_dim), pool_h], dim=1)
-                decoder_h = self.mlp(decoder_h)
-                decoder_h = torch.unsqueeze(decoder_h, 0)
-                state_tuple = (decoder_h, state_tuple[1])
-
-            embedding_input = rel_pos
-
-            decoder_input = self.spatial_embedding(embedding_input)
-            decoder_input = decoder_input.view(1, batch, self.embedding_dim)
-            pred_traj_fake_rel.append(rel_pos.view(batch, -1))
-            last_pos = curr_pos
-
-        pred_traj_fake_rel = torch.stack(pred_traj_fake_rel, dim=0)
-        return pred_traj_fake_rel, state_tuple[0]
-
-
-class PoolHiddenNet(nn.Module):
-    """Pooling module as proposed in our paper"""
-    def __init__(
-        self, embedding_dim=64, h_dim=64, mlp_dim=1024, bottleneck_dim=1024,
-        activation='relu', batch_norm=True, dropout=0.0
-    ):
-        super(PoolHiddenNet, self).__init__()
-
-        self.mlp_dim = 1024
-        self.h_dim = h_dim
-        self.bottleneck_dim = bottleneck_dim
-        self.embedding_dim = embedding_dim
-
-        mlp_pre_dim = embedding_dim + h_dim
-        mlp_pre_pool_dims = [mlp_pre_dim, 512, bottleneck_dim]
-
-        self.spatial_embedding = nn.Linear(2, embedding_dim)
-        self.mlp_pre_pool = make_mlp(
-            mlp_pre_pool_dims,
-            activation=activation,
-            batch_norm=batch_norm,
-            dropout=dropout)
-
-    def repeat(self, tensor, num_reps):
-        """
-        Inputs:
-        -tensor: 2D tensor of any shape
-        -num_reps: Number of times to repeat each row
-        Outpus:
-        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2
-        """
-        col_len = tensor.size(1)
-        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)
-        tensor = tensor.view(-1, col_len)
-        return tensor
-
-    def forward(self, h_states, seq_start_end, end_pos):
-        """
-        Inputs:
-        - h_states: Tensor of shape (num_layers, batch, h_dim)
-        - seq_start_end: A list of tuples which delimit sequences within batch
-        - end_pos: Tensor of shape (batch, 2)
-        Output:
-        - pool_h: Tensor of shape (batch, bottleneck_dim)
-        """
-        pool_h = []
-        for _, (start, end) in enumerate(seq_start_end):
-            start = start.item()
-            end = end.item()
-            num_ped = end - start
-         #   curr_hidden = h_states.view(-1, self.h_dim)[start:end]
-            curr_hidden = h_states.contiguous().view(-1, self.h_dim)[start:end]
-            curr_end_pos = end_pos[start:end]
-            # Repeat -> H1, H2, H1, H2
-            curr_hidden_1 = curr_hidden.repeat(num_ped, 1)
-            # Repeat position -> P1, P2, P1, P2
-            curr_end_pos_1 = curr_end_pos.repeat(num_ped, 1)
-            # Repeat position -> P1, P1, P2, P2
-            curr_end_pos_2 = self.repeat(curr_end_pos, num_ped)
-            curr_rel_pos = curr_end_pos_1 - curr_end_pos_2
-            curr_rel_embedding = self.spatial_embedding(curr_rel_pos) # for each person calc relativ position, e.g. 6 people -> [6,6] mit 0 diagonal
-            mlp_h_input = torch.cat([curr_rel_embedding, curr_hidden_1], dim=1)
-            curr_pool_h = self.mlp_pre_pool(mlp_h_input)
-       #     testi = curr_pool_h.view(num_ped, num_ped, -1) # -1 is inferred from other dimensions to fit the new size, Note only one dimension can be logicly inferred!
-            curr_pool_h = curr_pool_h.view(num_ped, num_ped, -1).max(1)[0] # secodnd dimension to look elementwise for max values. Second max output tensor are the indeces!
-
-            pool_h.append(curr_pool_h)
-        pool_h = torch.cat(pool_h, dim=0)
-        return pool_h
-
-
-class SocialPooling(nn.Module):
-    """Current state of the art pooling mechanism:
-    http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf"""
-    def __init__(
-        self, h_dim=64, activation='relu', batch_norm=True, dropout=0.0,
-        neighborhood_size=2.0, grid_size=8, pool_dim=None
-    ):
-        super(SocialPooling, self).__init__()
-        self.h_dim = h_dim
-        self.grid_size = grid_size
-        self.neighborhood_size = neighborhood_size
-        if pool_dim:
-            mlp_pool_dims = [grid_size * grid_size * h_dim, pool_dim]
-        else:
-            mlp_pool_dims = [grid_size * grid_size * h_dim, h_dim]
-
-        self.mlp_pool = make_mlp(
-            mlp_pool_dims,
-            activation=activation,
-            batch_norm=batch_norm,
-            dropout=dropout
-        )
-
-    def get_bounds(self, ped_pos):
-        top_left_x = ped_pos[:, 0] - self.neighborhood_size / 2
-        top_left_y = ped_pos[:, 1] + self.neighborhood_size / 2
-        bottom_right_x = ped_pos[:, 0] + self.neighborhood_size / 2
-        bottom_right_y = ped_pos[:, 1] - self.neighborhood_size / 2
-        top_left = torch.stack([top_left_x, top_left_y], dim=1)
-        bottom_right = torch.stack([bottom_right_x, bottom_right_y], dim=1)
-        return top_left, bottom_right
-
-    def get_grid_locations(self, top_left, other_pos):
-        cell_x = torch.floor(
-            ((other_pos[:, 0] - top_left[:, 0]) / self.neighborhood_size) *
-            self.grid_size)
-        cell_y = torch.floor(
-            ((top_left[:, 1] - other_pos[:, 1]) / self.neighborhood_size) *
-            self.grid_size)
-        grid_pos = cell_x + cell_y * self.grid_size
-        return grid_pos
-
-    def repeat(self, tensor, num_reps):
-        """
-        Inputs:
-        -tensor: 2D tensor of any shape
-        -num_reps: Number of times to repeat each row
-        Outpus:
-        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2
-        """
-        col_len = tensor.size(1)
-        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)
-        tensor = tensor.view(-1, col_len)
-        return tensor
-
-    def forward(self, h_states, seq_start_end, end_pos):
-        """
-        Inputs:
-        - h_states: Tesnsor of shape (num_layers, batch, h_dim)
-        - seq_start_end: A list of tuples which delimit sequences within batch.
-        - end_pos: Absolute end position of obs_traj (batch, 2)
-        Output:
-        - pool_h: Tensor of shape (batch, h_dim)
-        """
-        pool_h = []
-        for _, (start, end) in enumerate(seq_start_end):
-            start = start.item()
-            end = end.item()
-            num_ped = end - start
-            grid_size = self.grid_size * self.grid_size
-            curr_hidden = h_states.view(-1, self.h_dim)[start:end]
-            curr_hidden_repeat = curr_hidden.repeat(num_ped, 1)
-            curr_end_pos = end_pos[start:end]
-            curr_pool_h_size = (num_ped * grid_size) + 1
-            curr_pool_h = curr_hidden.new_zeros((curr_pool_h_size, self.h_dim))
-            # curr_end_pos = curr_end_pos.data
-            top_left, bottom_right = self.get_bounds(curr_end_pos)
-
-            # Repeat position -> P1, P2, P1, P2
-            curr_end_pos = curr_end_pos.repeat(num_ped, 1)
-            # Repeat bounds -> B1, B1, B2, B2
-            top_left = self.repeat(top_left, num_ped)
-            bottom_right = self.repeat(bottom_right, num_ped)
-
-            grid_pos = self.get_grid_locations(
-                    top_left, curr_end_pos).type_as(seq_start_end)
-            # Make all positions to exclude as non-zero
-            # Find which peds to exclude
-            x_bound = ((curr_end_pos[:, 0] >= bottom_right[:, 0]) +
-                       (curr_end_pos[:, 0] <= top_left[:, 0]))
-            y_bound = ((curr_end_pos[:, 1] >= top_left[:, 1]) +
-                       (curr_end_pos[:, 1] <= bottom_right[:, 1]))
-
-            within_bound = x_bound + y_bound
-            within_bound[0::num_ped + 1] = 1  # Don't include the ped itself
-            within_bound = within_bound.view(-1)
-
-            # This is a tricky way to get scatter add to work. Helps me avoid a
-            # for loop. Offset everything by 1. Use the initial 0 position to
-            # dump all uncessary adds.
-            grid_pos += 1
-            total_grid_size = self.grid_size * self.grid_size
-            offset = torch.arange(
-                0, total_grid_size * num_ped, total_grid_size
-            ).type_as(seq_start_end)
-
-            offset = self.repeat(offset.view(-1, 1), num_ped).view(-1)
-            grid_pos += offset
-            grid_pos[within_bound != 0] = 0
-            grid_pos = grid_pos.view(-1, 1).expand_as(curr_hidden_repeat)
-
-            curr_pool_h = curr_pool_h.scatter_add(0, grid_pos,
-                                                  curr_hidden_repeat)
-            curr_pool_h = curr_pool_h[1:]
-            pool_h.append(curr_pool_h.view(num_ped, -1))
-
-        pool_h = torch.cat(pool_h, dim=0)
-        pool_h = self.mlp_pool(pool_h)
-        return pool_h
-
-
-class TrajectoryGenerator(nn.Module):
-    def __init__(self, cfg):
-        obs_len = 8
-        pred_len = 12
-        embedding_dim = 16
-        encoder_h_dim = 32
-        decoder_h_dim = 32
-        mlp_dim = 64
-        num_layers = 1
-        noise_dim = (8,)
-        noise_type = 'gaussian'
-        noise_mix_type = 'global'
-        pooling_type = 'pool_net'
-        pool_every_timestep = False
-        dropout = 0.0
-        bottleneck_dim = 8
-        activation = 'relu'
-        batch_norm = 0
-        neighborhood_size = 2.0
-        grid_size = 8
-
-        super(TrajectoryGenerator, self).__init__()
-
-        if pooling_type and pooling_type.lower() == 'none':
-            pooling_type = None
-
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-        self.mlp_dim = mlp_dim
-        self.encoder_h_dim = encoder_h_dim
-        self.decoder_h_dim = decoder_h_dim
-        self.embedding_dim = embedding_dim
-        self.noise_dim = noise_dim
-        self.num_layers = num_layers
-        self.noise_type = noise_type
-        self.noise_mix_type = noise_mix_type
-        self.pooling_type = pooling_type
-        self.noise_first_dim = 0
-        self.pool_every_timestep = pool_every_timestep
-        self.bottleneck_dim = 1024
-
-        self.encoder = Encoder(
-            embedding_dim=embedding_dim,
-            h_dim=encoder_h_dim,
-            mlp_dim=mlp_dim,
-            num_layers=num_layers,
-            dropout=dropout
-        )
-
-        self.decoder = Decoder(
-            pred_len,
-            embedding_dim=embedding_dim,
-            h_dim=decoder_h_dim,
-            mlp_dim=mlp_dim,
-            num_layers=num_layers,
-            pool_every_timestep=pool_every_timestep,
-            dropout=dropout,
-            bottleneck_dim=bottleneck_dim,
-            activation=activation,
-            batch_norm=batch_norm,
-            pooling_type=pooling_type,
-            grid_size=grid_size,
-            neighborhood_size=neighborhood_size
-        )
-
-        if pooling_type == 'pool_net':
-            self.pool_net = PoolHiddenNet(
-                embedding_dim=self.embedding_dim,
-                h_dim=encoder_h_dim,
-                mlp_dim=mlp_dim,
-                bottleneck_dim=bottleneck_dim,
-                activation=activation,
-                batch_norm=batch_norm
-            )
-        elif pooling_type == 'spool':
-            self.pool_net = SocialPooling(
-                h_dim=encoder_h_dim,
-                activation=activation,
-                batch_norm=batch_norm,
-                dropout=dropout,
-                neighborhood_size=neighborhood_size,
-                grid_size=grid_size
-            )
-
-        if self.noise_dim[0] == 0:
-            self.noise_dim = None
-        else:
-            self.noise_first_dim = noise_dim[0]
-
-        # Decoder Hidden
-        if pooling_type:
-            input_dim = encoder_h_dim + bottleneck_dim
-        else:
-            input_dim = encoder_h_dim
-
-        if self.mlp_decoder_needed():
-            mlp_decoder_context_dims = [
-                input_dim, mlp_dim, decoder_h_dim - self.noise_first_dim
-            ]
-
-            self.mlp_decoder_context = make_mlp(
-                mlp_decoder_context_dims,
-                activation=activation,
-                batch_norm=batch_norm,
-                dropout=dropout
-            )
-
-    def add_noise(self, _input, seq_start_end, user_noise=None):
-        """
-        Inputs:
-        - _input: Tensor of shape (_, decoder_h_dim - noise_first_dim)
-        - seq_start_end: A list of tuples which delimit sequences within batch.
-        - user_noise: Generally used for inference when you want to see
-        relation between different types of noise and outputs.
-        Outputs:
-        - decoder_h: Tensor of shape (_, decoder_h_dim)
-        """
-        if not self.noise_dim:
-            return _input
-
-        if self.noise_mix_type == 'global':
-            noise_shape = (seq_start_end.size(0), ) + self.noise_dim
-        else:
-            noise_shape = (_input.size(0), ) + self.noise_dim
-
-        if user_noise is not None:
-            z_decoder = user_noise
-        else:
-            z_decoder = get_noise(noise_shape, self.noise_type)
-
-        if self.noise_mix_type == 'global':
-            _list = []
-            for idx, (start, end) in enumerate(seq_start_end):
-                start = start.item()
-                end = end.item()
-                _vec = z_decoder[idx].view(1, -1)
-                _to_cat = _vec.repeat(end - start, 1)
-                _list.append(torch.cat([_input[start:end], _to_cat], dim=1))
-            decoder_h = torch.cat(_list, dim=0)
-            return decoder_h
-
-        decoder_h = torch.cat([_input, z_decoder], dim=1)
-
-        return decoder_h
-
-    def mlp_decoder_needed(self):
-        if (
-            self.noise_dim or self.pooling_type or
-            self.encoder_h_dim != self.decoder_h_dim
-        ):
-            return True
-        else:
-            return False
-
-    def forward(self, model_input, obs_traj, pred_traj_gt, seq_start_end,
-                nei_num_index, nei_num,teacher_forcing_ratio=0.5, user_noise = None, plot_att = False):
-        """
-        Inputs:
-        - obs_traj: Tensor of shape (obs_len, batch, 2)
-        - obs_traj_rel: Tensor of shape (obs_len, batch, 2)
-        - seq_start_end: A list of tuples which delimit sequences within batch.
-        - user_noise: Generally used for inference when you want to see
-        relation between different types of noise and outputs.
-        Output:
-        - pred_traj_rel: Tensor of shape (self.pred_len, batch, 2)
-        """
-        obs_traj_rel = model_input[0:self.obs_len]
-        batch = obs_traj_rel.size(1)
-        # Encode seq
-        final_encoder_h = self.encoder(obs_traj_rel)
-        # Pool States
-        if self.pooling_type:
-            end_pos = obs_traj[-1, :, :]
-            pool_h = self.pool_net(final_encoder_h, seq_start_end, end_pos)
-            # Construct input hidden states for decoder
-            mlp_decoder_context_input = torch.cat(
-                [final_encoder_h.view(-1, self.encoder_h_dim), pool_h], dim=1)
-        else:
-            mlp_decoder_context_input = final_encoder_h.view(
-                -1, self.encoder_h_dim)
-
-        # Add Noise
-        if self.mlp_decoder_needed():
-            noise_input = self.mlp_decoder_context(mlp_decoder_context_input)
-        else:
-            noise_input = mlp_decoder_context_input
-        decoder_h = self.add_noise(
-            noise_input, seq_start_end, user_noise=user_noise)
-        decoder_h = torch.unsqueeze(decoder_h, 0)
-
-        decoder_c = torch.zeros(
-            self.num_layers, batch, self.decoder_h_dim
-        ).to(device)
-
-        state_tuple = (decoder_h, decoder_c)
-        last_pos = obs_traj[-1]
-        last_pos_rel = obs_traj_rel[-1]
-        # Predict Trajectory
-
-        decoder_out = self.decoder(
-            last_pos,
-            last_pos_rel,
-            state_tuple,
-            seq_start_end,
-        )
-        pred_traj_fake_rel, final_decoder_h = decoder_out
-
-        return pred_traj_fake_rel
-
-
-class Discriminator(nn.Module):
-    def __init__(self, cfg):
-        super(Discriminator, self).__init__()
-        obs_len=8
-        pred_len=12
-        embedding_dim=16
-        h_dim=48
-        mlp_dim=64
-        num_layers=1
-        activation='relu'
-        batch_norm=False
-        dropout=0.0
-        d_type='global'
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-        self.seq_len = obs_len + pred_len
-        self.mlp_dim = mlp_dim
-        self.h_dim = h_dim
-        self.d_type = d_type
-
-        self.encoder = Encoder(
-            embedding_dim=embedding_dim,
-            h_dim=h_dim,
-            mlp_dim=mlp_dim,
-            num_layers=num_layers,
-            dropout=dropout
-        )
-
-        real_classifier_dims = [h_dim, mlp_dim, 1]
-        self.real_classifier = make_mlp(
-            real_classifier_dims,
-            activation=activation,
-            batch_norm=batch_norm,
-            dropout=dropout
-        )
-        if d_type == 'global':
-            mlp_pool_dims = [h_dim + embedding_dim, mlp_dim, h_dim]
-            self.pool_net = PoolHiddenNet(
-                embedding_dim=embedding_dim,
-                h_dim=h_dim,
-                mlp_dim=mlp_pool_dims,
-                bottleneck_dim=h_dim,
-                activation=activation,
-                batch_norm=batch_norm
-            )
-
-    def forward(self, obs_rel, traj_rel_pred, obs_traj_pos,
-                nei_index, nei_num_index, loss_mask, seq_start_end, plot_att = False):
-        """
-        Inputs:
-        - traj: Tensor of shape (obs_len + pred_len, batch, 2)
-        - traj_rel: Tensor of shape (obs_len + pred_len, batch, 2)
-        - seq_start_end: A list of tuples which delimit sequences within batch
-        Output:
-        - scores: Tensor of shape (batch,) with real/fake scores
-        """
-        traj_rel_pred = traj_rel_pred * loss_mask
-        traj_rel = torch.cat([obs_rel, traj_rel_pred])
-        final_h = self.encoder(traj_rel)
-        # Note: In case of 'global' option we are using start_pos as opposed to
-        # end_pos. The intution being that hidden state has the whole
-        # trajectory and relative postion at the start when combined with
-        # trajectory information should help in discriminative behavior.
-        if self.d_type == 'local':
-            classifier_input = final_h.squeeze()
-        else:
-            classifier_input = self.pool_net(
-                final_h.squeeze(), seq_start_end, obs_traj_pos[0]
-            )
-        scores = self.real_classifier(classifier_input)
-        return scores
diff --git a/experiments/models/SIMnoGoal.py b/experiments/models/SIMnoGoal.py
deleted file mode 100644
index 82bc80b..0000000
--- a/experiments/models/SIMnoGoal.py
+++ /dev/null
@@ -1,180 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import numpy as np
-import random
-from config import *
-config = Config()
-seed = config.seed
-torch.manual_seed(seed)
-np.random.seed(seed)
-random.seed(0)
-from utils.losses import GaußNLL
-from models.utils import Pooling_net
-# copy of Sim2Goal but without goal and different sampling during inference
-class TrajectoryGenerator(nn.Module):
-    def __init__(self,config):
-        super(TrajectoryGenerator, self).__init__()
-        self.config = config
-        obs_len= config.obs_len
-        pred_len= config.pred_len
-        traj_lstm_input_size= 2
-        traj_lstm_hidden_size=16
-        rela_embed_size = 16
-        self.inputLayer_encoder = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.goal_encoder = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.goal_encoder_dist = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.inputLayer_decoder = nn.Linear(traj_lstm_input_size + 16*2, rela_embed_size)
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-        self.pl_net = Pooling_net(h_dim=traj_lstm_hidden_size)
-        self.pl_net_correct = Pooling_net(h_dim=traj_lstm_hidden_size)
-        self.mlp_corrector = nn.Linear(traj_lstm_hidden_size, 2)
-        self.traj_lstm_hidden_size = traj_lstm_hidden_size
-        self.traj_lstm_input_size = traj_lstm_input_size
-        self.pred_lstm_hidden_size = self.traj_lstm_hidden_size
-        self.traj_lstm_model = nn.LSTMCell(rela_embed_size, 16)
-        self.pred_lstm_model =  nn.LSTMCell(rela_embed_size, 16)
-        self.pred_hidden2pos = nn.Linear(traj_lstm_hidden_size, 2*2)
-        self.dropout = nn.Dropout(p=0.0)
-        self.init_parameters()
-
-    def init_parameters(self):
-        nn.init.constant_(self.inputLayer_encoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder.weight, std=0.2)
-
-        nn.init.constant_(self.inputLayer_decoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_decoder.weight, std=0.2)
-
-        nn.init.xavier_uniform_(self.traj_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.traj_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.traj_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.traj_lstm_model.bias_hh, 0.0)
-        n = self.traj_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.traj_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-        nn.init.xavier_uniform_(self.pred_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.pred_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.pred_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.pred_lstm_model.bias_hh, 0.0)
-        n = self.pred_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.pred_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-
-    def init_hidden_traj_lstm(self, batch):
-        return (
-            torch.randn(batch, 16).cuda(),
-            torch.randn(batch, 16).cuda(),
-        )
-
-    def get_corrected(self, nei_index, lstm_state_hidden, nei_num_index, curr_pos_abs, output_pred_sampled, batch):
-        corr = curr_pos_abs.repeat(batch, 1, 1)
-        corr_index = (corr.transpose(0, 1) - corr)
-        lstm_state_context, _, _ = self.pl_net_correct(corr_index,nei_index, nei_num_index, lstm_state_hidden, curr_pos_abs)
-        corrected_output = self.mlp_corrector(lstm_state_context) + output_pred_sampled
-        return corrected_output
-
-
-    def forward(self, traj_rel, obs_traj_pos, pred_traj_gt_pos, seq_start_end,
-                nei_index, nei_num_index, mode="test", plot_sample=False,
-                sample_goal=None, noise=None, robot_net= None, robotID= None, detached=True):
-        batch = traj_rel.shape[1]
-        pred_traj_rel = []
-        pred_mu = []
-        pred_scale = []
-        samples = []
-        pred_traj_rel_sampled = []
-
-        nll = 0.
-        nll_robot = 0.
-
-        traj_lstm_h_t, traj_lstm_c_t = self.init_hidden_traj_lstm(batch)
-        for i, input_t in enumerate(traj_rel[: self.obs_len].chunk(
-                traj_rel[: self.obs_len].size(0), dim=0)):
-            input_embedded =self.dropout(F.relu(self.inputLayer_encoder(input_t.squeeze(0))))
-            lstm_state = self.traj_lstm_model(
-                input_embedded, (traj_lstm_h_t, traj_lstm_c_t)
-            )
-            traj_lstm_h_t, traj_lstm_c_t = lstm_state
-
-        pred_lstm_hidden = traj_lstm_h_t
-        pred_lstm_c_t = torch.zeros_like(pred_lstm_hidden).cuda()
-        output = traj_rel[self.obs_len-1]
-        lstm_state_context = torch.zeros_like(pred_lstm_hidden).cuda()
-        curr_pos_abs = obs_traj_pos[-1]
-
-        for i in range(self.pred_len):
-            if robot_net != None:
-                ids_no_robot = torch.zeros([self.config.pred_len, seq_start_end[-1, 1], 2], dtype=torch.bool).cuda()
-                ids_no_robot[i, robotID, :] = True
-            emb_distance_goal = self.goal_encoder_dist(curr_pos_abs)
-            if not detached:
-                input_cat = torch.cat([lstm_state_context, output,emb_distance_goal], dim=-1)
-            else:
-                input_cat = torch.cat([lstm_state_context.detach(), output.detach(), emb_distance_goal], dim=-1)
-            input_embedded = self.dropout(F.relu(self.inputLayer_decoder(input_cat))) # detach from history as input
-            lstm_state = self.pred_lstm_model(
-                input_embedded, (pred_lstm_hidden, pred_lstm_c_t)
-            )
-            pred_lstm_hidden = lstm_state[0]
-            pred_lstm_c_t = lstm_state[1]
-            corr = curr_pos_abs.repeat(batch, 1, 1)
-            corr_index = (corr.transpose(0,1)-corr)
-            lstm_state_hidden = lstm_state[0]
-            lstm_state_context, _, _ = self.pl_net(corr_index, nei_index[i], nei_num_index, lstm_state_hidden, curr_pos_abs)
-            concat_output = lstm_state_context + lstm_state_hidden + emb_distance_goal.detach()
-            mu, scale = self.pred_hidden2pos(concat_output).chunk(2, 1)
-            scale = torch.clamp(scale, min=-9, max=4)
-            if robot_net != None and noise != None:
-                z_robot = noise[i, robotID]
-                # z_robot = robot_net(torch.cat([concat_output[ids_no_robot[i, :, 0]], z_robot], dim=-1))  # .chunk(2, 1)
-                z_robot = robot_net(torch.cat([concat_output[ids_no_robot[i, :, 0]], mu[ids_no_robot[i, :, 0]], scale[ids_no_robot[i, :, 0]],z_robot], dim=-1))
-                # mu, scale
-                noise = noise.masked_scatter(ids_no_robot, z_robot)
-            if mode == 'test':
-                if noise == None:
-                    output_pred_sampled = mu + torch.exp(scale) * torch.randn_like(scale)
-                else:
-                    output_pred_sampled = mu + torch.exp(scale)*noise[i]  # for eval with a specific noise
-            elif mode == 'robot_train':
-                output_pred_sampled = mu + torch.exp(scale) * noise[i]
-                nll_robot = nll_robot + GaußNLL(mu, scale, output_pred_sampled)
-            else:
-                output_pred_sampled = mu + torch.exp(scale) * torch.randn_like(scale)
-                nll = nll + GaußNLL(mu, scale, traj_rel[self.obs_len + i])
-
-            if not detached:
-                curr_pos_abs_sampled = (curr_pos_abs + output_pred_sampled).detach()
-            else:
-                curr_pos_abs_sampled = (curr_pos_abs + output_pred_sampled)
-            # curr_pos_abs_sampled = (curr_pos_abs + output_pred_sampled).detach()
-            output_pred = self.get_corrected(nei_index[i], lstm_state_hidden, nei_num_index, curr_pos_abs_sampled,
-                                             output_pred_sampled, batch)
-            # output_pred = output_pred_sampled  # for no Sampling module
-            if not detached:
-                curr_pos_abs = (curr_pos_abs + output_pred)
-            else:
-                curr_pos_abs = (curr_pos_abs + output_pred).detach()
-            if plot_sample:
-                plot_samples = curr_pos_abs.unsqueeze(dim=1) + mu.unsqueeze(dim=1) + scale.unsqueeze(dim=1) * torch.randn(
-                    [scale.shape[0], 20, scale.shape[1]]).to('cuda')
-                samples.append(plot_samples)
-            pred_traj_rel += [output_pred]
-            output = output_pred
-            pred_mu += [mu]
-            pred_scale += [scale]
-            pred_traj_rel_sampled += [output_pred_sampled]
-        mu = torch.stack(pred_mu)
-        logscale = torch.stack(pred_scale)
-
-        if mode == 'test':
-            if plot_sample:
-                return torch.stack(pred_traj_rel), torch.stack(samples)
-            else:
-                return torch.stack(pred_traj_rel)
-        elif mode =='robot_train':
-            return torch.stack(pred_traj_rel), mu, logscale, nll_robot, torch.stack(pred_traj_rel_sampled) # For Robot training
-        else:
-            return torch.stack(pred_traj_rel), nll, torch.stack(pred_traj_rel_sampled) # Student learning
diff --git a/experiments/models/VAE.py b/experiments/models/VAE.py
deleted file mode 100644
index c42a6a7..0000000
--- a/experiments/models/VAE.py
+++ /dev/null
@@ -1,245 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import numpy as np
-from utils.losses import GaußNLL
-from models.utils import Pooling_net
-
-class TrajectoryGenerator(nn.Module):
-    def __init__(self,config):
-        super(TrajectoryGenerator, self).__init__()
-        self.config = config
-        obs_len= config.obs_len
-        pred_len= config.pred_len
-        traj_lstm_input_size= 2
-        traj_lstm_hidden_size=16
-        rela_embed_size = 16
-        latent_dim = self.latent_dim = self.config.latent_dim
-        self.inputLayer_encoder = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.inputLayer_encoder_fut = nn.Linear(traj_lstm_hidden_size, rela_embed_size)
-        self.inputLayer_decoder = nn.Linear(traj_lstm_input_size + 18, rela_embed_size)
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-
-        self.pl_net = Pooling_net(h_dim=traj_lstm_hidden_size)
-
-        self.traj_lstm_hidden_size = traj_lstm_hidden_size
-        self.traj_lstm_input_size = traj_lstm_input_size
-        self.pred_lstm_hidden_size = self.traj_lstm_hidden_size
-        self.traj_lstm_model = nn.LSTMCell(rela_embed_size, 16)
-        self.traj_lstm_model_future = nn.LSTMCell(rela_embed_size, traj_lstm_hidden_size)
-        self.pred_lstm_model =  nn.LSTMCell(rela_embed_size, 16)
-
-        self.pred_hidden2pos = nn.Linear(traj_lstm_hidden_size, 2*2)
-
-        # latent encoder during training
-        self.inputLayer_encoder1 = nn.Linear(traj_lstm_hidden_size * 2, latent_dim)
-        self.inputLayer_encoder2 = nn.Linear(traj_lstm_hidden_size * 2, latent_dim)
-        # latent encoder during testing
-        self.inputLayer_encoder_test1 = nn.Linear(traj_lstm_hidden_size, latent_dim)
-        self.inputLayer_encoder_test2 = nn.Linear(traj_lstm_hidden_size, latent_dim)
-        # latent decoder layer
-        self.latent_dec = nn.Linear(latent_dim, traj_lstm_hidden_size)
-
-        self.init_parameters()
-        self.noise_dim = (8,)
-        self.noise_type = 'gaussian'
-        self.noise_mix_type = 'global'
-
-    def init_parameters(self):
-        nn.init.constant_(self.inputLayer_encoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder.weight, std=0.02)
-
-        nn.init.constant_(self.inputLayer_decoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_decoder.weight, std=0.02)
-
-        nn.init.constant_(self.inputLayer_encoder_fut.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder_fut.weight, std=0.02)
-
-        # LSTM past
-        nn.init.xavier_uniform_(self.traj_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.traj_lstm_model.weight_hh, gain=0.001)
-        nn.init.constant_(self.traj_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.traj_lstm_model.bias_hh, 0.0)
-        n = self.traj_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.traj_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-        #LSTM future
-        nn.init.xavier_uniform_(self.traj_lstm_model_future.weight_ih)
-        nn.init.orthogonal_(self.traj_lstm_model_future.weight_hh, gain=0.001)
-        nn.init.constant_(self.traj_lstm_model_future.bias_ih, 0.0)
-        nn.init.constant_(self.traj_lstm_model_future.bias_hh, 0.0)
-        n = self.traj_lstm_model_future.bias_ih.size(0)
-        nn.init.constant_(self.traj_lstm_model_future.bias_ih[n // 4:n // 2], 1.0)
-        #LSTM decoder
-        nn.init.xavier_uniform_(self.pred_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.pred_lstm_model.weight_hh, gain=0.001)
-        nn.init.constant_(self.pred_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.pred_lstm_model.bias_hh, 0.0)
-        n = self.pred_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.pred_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-        nn.init.constant_(self.pred_hidden2pos.bias, 0.0)
-        nn.init.normal_(self.pred_hidden2pos.weight, std=0.01)
-
-        #latent layers
-        #encoder
-        nn.init.constant_(self.inputLayer_encoder1.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder1.weight, std=0.02)
-        #nn.init.xavier_uniform_(self.inputLayer_encoder1.weight)
-
-        nn.init.constant_(self.inputLayer_encoder2.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder2.weight, std=0.02)
-        #nn.init.xavier_uniform_(self.inputLayer_encoder2.weight)
-
-        nn.init.constant_(self.inputLayer_encoder_test1.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder_test1.weight, std=0.02)
-        #nn.init.xavier_uniform_(self.inputLayer_encoder_test1.weight)
-
-        nn.init.constant_(self.inputLayer_encoder_test2.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder_test2.weight, std=0.02)
-        #nn.init.xavier_uniform_(self.inputLayer_encoder_test2.weight)
-
-        #decoder
-        nn.init.constant_(self.latent_dec.bias, 0.0)
-        nn.init.normal_(self.latent_dec.weight, std=0.02)
-
-    def init_hidden_traj_lstm(self, batch):
-        return (
-            torch.randn(batch, self.traj_lstm_hidden_size).cuda(),
-            torch.randn(batch, self.traj_lstm_hidden_size).cuda(),
-        )
-    def latent_encoder(self, input_t, inference):
-        # Apply relu activation and MLP layer to the inputs
-        if not inference:  # if training, input_t is concatenation of past and future
-            z_mu = 4 * torch.tanh(self.inputLayer_encoder1(input_t))  # mean [-1 1], zero mean
-            z_log_var = 0.5*torch.sigmoid(self.inputLayer_encoder2(input_t))  # Var [1.01 ...]
-
-        else:  # if testing, input_t is only past, different nn.Linear layer for testing
-            z_mu = 4 * torch.tanh(self.inputLayer_encoder_test1(input_t))  # mean [-1 1], zero mean
-            z_log_var = 0.5*torch.sigmoid(self.inputLayer_encoder_test2(input_t))  # Var [1.01 ...]
-
-        mu = torch.reshape(z_mu, (input_t.shape[0], self.latent_dim))
-        logVar = torch.reshape(z_log_var, (input_t.shape[0], self.latent_dim))
-
-        return mu, logVar
-
-    def latent_decoder(self, z):
-        # decode latent variables
-        x = F.relu(self.latent_dec(z))
-        if self.config.softmax_dec:
-            x = F.softmax(x, dim=-1)
-        return x
-
-    def reparameterize(self, mu, logVar, inference):
-        std = torch.exp(logVar / 2)
-        eps = torch.randn_like(std)  # noise
-        z = mu + std * eps
-        if inference:
-            z = mu + std * eps
-
-        return z
-
-    def forward(self,traj_rel, obs_traj_pos, pred_traj_gt_pos, seq_start_end,
-                nei_index, nei_num_index, mode="test"):
-        batch = traj_rel.shape[1]
-        pred_traj_rel = []
-        pred_mu = []
-        pred_scale = []
-        samples = []
-        if mode == "test":
-            inference = True
-        else:
-            inference = False
-
-        traj_lstm_h_t, traj_lstm_c_t = self.init_hidden_traj_lstm(batch)
-        for i, input_t in enumerate(traj_rel[: self.obs_len].chunk(
-                traj_rel[: self.obs_len].size(0), dim=0)):
-            input_embedded = F.dropout(F.relu(self.inputLayer_encoder(input_t.squeeze(0))),
-                                       p=self.config.cond_dropout)
-            lstm_state = self.traj_lstm_model(
-                input_embedded, (traj_lstm_h_t, traj_lstm_c_t)
-            )
-            traj_lstm_h_t, traj_lstm_c_t = lstm_state
-
-        past_h_t = traj_lstm_h_t
-        z_mu_obs = torch.zeros(batch, self.latent_dim)
-        z_var_log_obs = torch.ones(batch, self.latent_dim)
-        z_mu, z_var_log = None, None
-        lstm_state_context = torch.zeros_like(past_h_t)
-
-        if not inference:  # if training
-            # embed future into LSTM
-            for j, input_t_fut in enumerate(traj_rel[self.obs_len:].chunk(
-                    traj_rel[self.obs_len:].size(0), dim=0)):
-                input_embedded_fut = F.relu(self.inputLayer_encoder(input_t_fut.squeeze(0)))
-                lstm_state_fut = self.traj_lstm_model_future(input_embedded_fut+lstm_state_context, (traj_lstm_h_t, traj_lstm_c_t))
-                traj_lstm_h_t, traj_lstm_c_t = lstm_state_fut
-                curr_pos_abs = traj_rel[-j]
-                corr = curr_pos_abs.repeat(batch, 1, 1)
-                corr_index = (corr.transpose(0, 1) - corr)
-                state_context, _, _ = self.pl_net(corr_index, nei_index[-j], nei_num_index,
-                                                       traj_lstm_h_t, curr_pos_abs)
-                lstm_state_context = F.relu(self.inputLayer_encoder_fut(state_context))
-
-            future_context = lstm_state_context + traj_lstm_h_t
-
-            # during training: concatenate past and future
-            input_encoder = torch.cat((past_h_t, future_context), -1)
-            z_mu, z_var_log = self.latent_encoder(input_encoder, inference)
-            z = self.reparameterize(z_mu, z_var_log, inference)
-
-        input_encoder = past_h_t  # only use past for z_mu_obs and z_var_log_obs
-        z_mu_obs, z_var_log_obs = self.latent_encoder(input_encoder, True)
-
-        if inference:  # if testing
-            z = self.reparameterize(z_mu_obs, z_var_log_obs, inference)
-
-
-        dec_z = self.latent_decoder(z)
-
-        pred_lstm_hidden = past_h_t + dec_z
-
-        pred_lstm_c_t = torch.zeros_like(pred_lstm_hidden).cuda()
-        output = traj_rel[self.obs_len]
-        mu = output
-        scale = torch.zeros_like(mu)
-        lstm_state_context = torch.zeros_like(pred_lstm_hidden).cuda()
-        curr_pos_abs = obs_traj_pos[-1]
-        nll = 0.
-
-        for i in range(self.pred_len):
-
-            input_cat = torch.cat([lstm_state_context.detach(),mu.detach(), scale.detach()], dim=-1)
-            input_embedded = F.relu(self.inputLayer_decoder(input_cat)) # detach from history as input
-            lstm_state = self.pred_lstm_model(
-                input_embedded, (pred_lstm_hidden, pred_lstm_c_t)
-            )
-            pred_lstm_hidden = lstm_state[0]
-            pred_lstm_c_t = lstm_state[1]
-            corr = curr_pos_abs.repeat(batch, 1, 1)
-            corr_index = (corr.transpose(0,1)-corr)
-            lstm_state_hidden = lstm_state[0]
-            lstm_state_context, _, _ = self.pl_net(corr_index, nei_index[i], nei_num_index, lstm_state_hidden, curr_pos_abs)
-            concat_output = lstm_state_context + lstm_state_hidden
-
-            mu, scale = self.pred_hidden2pos(concat_output).chunk(2, 1)
-            scale = torch.clamp(scale, min=-9, max=4)
-
-            if mode == 'test':
-
-                output_pred = mu + torch.exp(scale) * torch.randn_like(scale)
-
-            else:
-                output_pred = mu + torch.exp(scale) * torch.randn_like(scale)  #.to('cuda')
-            nll = nll + GaußNLL(mu, scale, traj_rel[self.obs_len + i])
-
-            curr_pos_abs = (curr_pos_abs + output_pred).detach() # detach from history as input
-            pred_traj_rel += [output_pred]
-            output = output_pred
-            # pred_mu += [mu]
-            # pred_scale += [scale]
-
-        if mode=='test':
-            return torch.stack(pred_traj_rel)
-        else:
-            return torch.stack(pred_traj_rel), nll, z_mu, z_var_log, z_mu_obs, z_var_log_obs
diff --git a/experiments/train_GAN.py b/experiments/train_GAN.py
deleted file mode 100644
index f1125d9..0000000
--- a/experiments/train_GAN.py
+++ /dev/null
@@ -1,320 +0,0 @@
-
-import logging
-import sys
-import numpy as np
-from collections import defaultdict
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import copy
-import wandb
-from data.loader import data_loader
-from utils.losses import l2_loss, coll_smoothed_loss, compute_gradient_penalty1D
-from utils.utils import get_dset_path
-from config import *
-from models.CoLoss_GAN import TrajectoryGenerator, Discriminator
-
-from evaluate_GAN import evaluate
-from utils.utils import relative_to_abs
-from torch.autograd import Variable
-torch.backends.cudnn.benchmark = False
-torch.backends.cudnn.enabled = True
-
-FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
-logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)
-logger = logging.getLogger(__name__)
-
-
-
-def get_dtypes(args):
-    long_dtype = torch.LongTensor
-    float_dtype = torch.FloatTensor
-    if args.use_gpu == 1:
-        long_dtype = torch.cuda.LongTensor
-        float_dtype = torch.cuda.FloatTensor
-    return long_dtype, float_dtype
-
-
-class Preparation:
-    def __init__(self, data=None):
-        self.config = Config()
-        if data:
-            self.config.dataset_name = data
-            group_name = self.config.experiment_name
-            self.config.experiment_name =  self.config.experiment_name + '-' + data
-            path = self.config.DIR + self.config.experiment_name
-            if not os.path.exists(path):
-                os.makedirs(path)
-            self.config.model_path = path
-            self.config.output_dir = self.config.model_path
-            self.config.checkpoint_start_from = self.config.model_path + '/checkpoint_with_model.pt'
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True, group=group_name)
-        else:
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True)
-            # print('no_wandb')
-        seed = self.config.seed
-        torch.manual_seed(seed)
-        np.random.seed(seed)
-
-        os.environ['CUDA_VISIBLE_DEVICES'] = self.config.gpu_num
-        train_path = get_dset_path(self.config.dataset_name, 'train')
-        val_path = get_dset_path(self.config.dataset_name, 'test')
-        long_dtype, float_dtype = get_dtypes(self.config)
-        logger.info("Initializing train dataset")
-        self.train_dset, self.train_loader = data_loader(self.config, train_path, augment=self.config.augment)
-        logger.info("Initializing val dataset")
-        _, self.val_loader = data_loader(self.config, val_path)
-        self.iterations_per_epoch = len(self.train_loader)
-        logger.info(
-            'There are {} iterations per epoch'.format(self.iterations_per_epoch)
-        )
-
-        self.generator = TrajectoryGenerator(self.config)
-        self.generator.type(float_dtype).train()
-        logger.info('Here is the generator:')
-        logger.info(self.generator)
-        self.discriminator = Discriminator(self.config)
-        self.discriminator.type(float_dtype).train()
-        logger.info('Here is the discriminator:')
-        logger.info(self.discriminator)
-        # Log model
-        # wandb.watch(self.generator, log='all')
-
-
-        if self.config.adam:
-            print('Learning with ADAM!')
-            betas_d = (0.5, 0.9999)
-            self.optimizer_g = optim.Adam(self.generator.parameters(), lr=self.config.g_learning_rate, betas=betas_d)
-            self.optimizer_d = optim.Adam(self.discriminator.parameters(), lr=self.config.d_learning_rate, betas=betas_d)
-        else:
-            print('Learning with RMSprop!')
-            self.optimizer_g = optim.SGD(self.generator.parameters(), lr=0.01, momentum=0.9)
-            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer_g, 'min')
-        restore_path = None
-        if self.config.checkpoint_start_from is not None:
-            restore_path = self.config.checkpoint_start_from
-        elif self.config.restore_from_checkpoint == True:
-            restore_path = os.path.join(self.config.output_dir,
-                                        '%s_with_model.pt' % self.config.checkpoint_name)
-
-        if os.path.isfile(restore_path):
-            logger.info('Restoring from checkpoint {}'.format(restore_path))
-            self.checkpoint = torch.load(restore_path)
-            self.generator.load_state_dict(self.checkpoint['g_state'])
-            self.discriminator.load_state_dict(self.checkpoint['d_state'])
-            self.optimizer_g.load_state_dict(self.checkpoint['g_optim_state'])
-            self.optimizer_d.load_state_dict(self.checkpoint['d_optim_state'])
-            self.t = self.checkpoint['counters']['t']
-            self.epoch = self.checkpoint['counters']['epoch']
-            self.checkpoint['restore_ts'].append(self.t)
-        else:
-            # Starting from scratch, so initialize checkpoint data structure
-            self.t, self.epoch = 0, 0
-            self.checkpoint = {
-                'args': self.config.__dict__,
-                'G_losses': defaultdict(list),
-                'D_losses': defaultdict(list),
-                'losses_ts': [],
-                'metrics_val': defaultdict(list),
-                'metrics_train': defaultdict(list),
-                'sample_ts': [],
-                'restore_ts': [],
-                'counters': {
-                    't': None,
-                    'epoch': None,
-                },
-                'g_state': None,
-                'g_optim_state': None,
-                'g_best_state': None,
-                'd_state': None,
-                'd_optim_state': None,
-                'd_best_state': None,
-            }
-
-    def check_accuracy(self, loader, generator):  # TODO Change this!
-        metrics = {}
-        generator.eval()  # will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
-
-        ade, fde, act, _ = evaluate(self.config, loader, generator)
-        metrics['act'] = act
-        metrics['ade'] = ade
-        metrics['fde'] = fde
-        mean = (act+ ade)/2
-        metrics['mean'] = mean
-        generator.train()
-        wandb.log({"ade":  metrics['ade'], "fde": metrics['fde'],
-                   "act_best_ade": metrics['act'], "Mean_ade_act": metrics['mean']})
-        return metrics
-
-    def print_stats(self):
-        dictlist = 'Epoch = {}, t = {} '.format(self.epoch, self.t) + '[D] '
-        dictlist += ' [G] '
-
-        for k, v in sorted(self.losses_g.items()):
-            self.checkpoint['G_losses'][k].append(v)
-            dictlist += ' ' + '{}: {:.6f}'.format(k, v)
-
-        logger.info(dictlist)
-
-        self.checkpoint['losses_ts'].append(self.t)
-
-    def save_model(self):
-        self.checkpoint['counters']['t'] = self.t
-        self.checkpoint['counters']['epoch'] = self.epoch
-        self.checkpoint['sample_ts'].append(self.t)
-
-        # Check stats on the validation set
-        logger.info('Checking stats on val ...')
-        metrics_val = self.check_accuracy(self.val_loader, self.generator)
-    #    self.scheduler.step(metrics_val['ade'])
-        for k, v in sorted(metrics_val.items()):
-            logger.info('  [val] {}: {:.3f}'.format(k, v))
-            self.checkpoint['metrics_val'][k].append(v)
-
-        min_mean = min(self.checkpoint['metrics_val']['mean'])
-        if metrics_val['mean'] == min_mean:
-            logger.info('New low for mean error')
-            self.checkpoint['best_t'] = self.t
-            self.checkpoint['g_best_state'] = copy.deepcopy(self.generator.state_dict())
-            self.checkpoint['d_best_state'] = self.discriminator.state_dict()
-
-        # Save another checkpoint with model weights and
-        # optimizer state
-        self.checkpoint['g_state'] = self.generator.state_dict()
-        self.checkpoint['g_optim_state'] = self.optimizer_g.state_dict()
-        self.checkpoint['d_state'] = self.discriminator.state_dict()
-        self.checkpoint['d_optim_state'] = self.optimizer_g.state_dict()
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_with_model.pt' % self.config.checkpoint_name
-        )
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        torch.save(self.checkpoint, checkpoint_path)
-        torch.save(self.checkpoint, os.path.join(wandb.run.dir, 'model.pt'))
-        logger.info('Done.')
-
-        # Save a checkpoint with no model weights by making a shallow
-        # copy of the checkpoint excluding some items
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_no_model.pt' % self.config.checkpoint_name)
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        key_blacklist = [
-            'g_state', 'g_best_state', 'g_optim_state',
-            'd_state', 'd_best_state', 'd_optim_state'
-        ]
-        small_checkpoint = {}
-        for k, v in self.checkpoint.items():
-            if k not in key_blacklist:
-                small_checkpoint[k] = v
-        torch.save(small_checkpoint, checkpoint_path)
-        logger.info('Done.')
-
-    def discriminator_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask, \
-        loss_mask, seq_start_end, nei_num_index, nei_num = batch
-        losses = {}
-        # self.discriminator.zero_grad()
-        loss_mask = loss_mask[-self.config.pred_len:]
-        for param in self.discriminator.parameters(): param.grad = None # more eff than self.discriminator.zero_grad()
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-        pred_traj_fake_rel = self.generator(model_input, obs_traj, pred_traj_gt,
-                                            seq_start_end, nei_num_index, nei_num, 0).detach()
-        scores_real = self.discriminator(obs_traj_rel, pred_traj_gt_rel,
-                                         obs_traj,nei_num_index, nei_num, loss_mask)
-        scores_fake = self.discriminator(obs_traj_rel, pred_traj_fake_rel,
-                                         obs_traj,nei_num_index, nei_num, loss_mask)
-        if self.config.loss == 'hinge':
-            disc_loss = nn.ReLU()(1.0 - scores_real).mean() + nn.ReLU()(1.0 + scores_fake).mean()
-        elif self.config.loss == 'wasserstein':
-            disc_loss = -scores_real.mean() + scores_fake.mean()
-        else:
-            disc_loss = nn.BCEWithLogitsLoss()(scores_real, Variable(torch.ones(scores_real.size()[0], 1).cuda())) + \
-                        nn.BCEWithLogitsLoss()(scores_fake, Variable(torch.zeros(scores_real.size()[0], 1).cuda()))
-        loss = disc_loss
-        if self.config.cpg_loss:
-            cgp_loss = self.config.cpg_loss * compute_gradient_penalty1D(self.discriminator, obs_traj_rel, pred_traj_gt_rel, pred_traj_fake_rel,
-                                                                         obs_traj, nei_num_index, nei_num, loss_mask)
-            loss = loss + cgp_loss
-            # cgp_loss.backward()
-        loss.backward()
-        losses['D_loss'] = disc_loss.item()
-        self.optimizer_d.step()
-        wandb.log({"D_loss": disc_loss.item()})
-        return losses
-
-    def gen_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask,\
-        loss_mask, seq_start_end, nei_num_index, nei_num, = batch
-        l2_loss_rel = []
-        losses = {}
-        for param in self.generator.parameters(): param.grad = None
-
-        loss_mask = loss_mask[-self.config.pred_len :]
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-        for _ in range(self.config.best_k):
-            pred_traj_fake_rel = self.generator(model_input, obs_traj, pred_traj_gt,
-                                                seq_start_end, nei_num_index, nei_num, 0)
-            l2_loss_rel.append(
-                l2_loss(
-                    pred_traj_fake_rel,
-                    model_input[-self.config.pred_len :],
-                    loss_mask,
-                    mode="raw",
-                )
-            )
-        l2_loss_sum_rel = torch.zeros(1).to(pred_traj_gt)
-        l2_loss_rel = torch.stack(l2_loss_rel, dim=1)
-        for start, end in seq_start_end.data:
-            _l2_loss_rel = torch.narrow(l2_loss_rel, 0, start, end - start)
-            _l2_loss_rel = torch.sum(_l2_loss_rel, dim=0)  # [20]
-            _l2_loss_rel = torch.min(_l2_loss_rel) / (
-                    (pred_traj_fake_rel.shape[0]) * (end - start)
-            )
-            l2_loss_sum_rel += _l2_loss_rel
-
-        if self.config.loss == 'hinge' or self.config.loss == 'wasserstein':
-            scores_fake = -self.discriminator(obs_traj_rel, pred_traj_fake_rel,
-                                              obs_traj, nei_num_index, nei_num, loss_mask).mean() #+ loss_l2*100
-        else:
-            scores_fake = nn.BCEWithLogitsLoss()(self.discriminator(obs_traj_rel, pred_traj_fake_rel,
-                                                                    obs_traj,nei_num_index, nei_num, loss_mask),
-                                                 Variable(torch.ones(obs_traj.size()[1], 1).cuda()))
-
-        pred_traj_fake_abs = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-        loss_count = coll_smoothed_loss(pred_traj_fake_abs, seq_start_end, nei_num_index)
-        loss = l2_loss_sum_rel + scores_fake + 10 * loss_count
-        # loss = scores_fake
-        loss.backward()
-        losses['L2_loss'] = l2_loss_sum_rel.item()
-        self.optimizer_g.step()
-        wandb.log({"Train L2": l2_loss_sum_rel.item()})
-        return losses
-
-    def train(self):
-        self.t_step = 0
-        while self.epoch < self.config.num_epochs:
-            d_steps_left = self.config.d_steps
-            g_steps_left = self.config.g_steps
-            self.t_step = 0
-            logger.info('Starting epoch {}'.format(self.epoch))
-            for batch in self.train_loader:
-                batch = [tensor.cuda() for tensor in batch]
-                if g_steps_left > 0:
-                    self.losses_g = self.gen_step(batch)
-                    g_steps_left -= 1
-                elif d_steps_left > 0:
-                    self.losses_d = self.discriminator_step(batch)
-                    d_steps_left -= 1
-                self.t += 1
-                if d_steps_left > 0 or g_steps_left > 0:
-                    continue
-                d_steps_left = self.config.d_steps
-                g_steps_left = self.config.g_steps
-                self.t_step += 1
-            if self.epoch % self.config.check_after_num_epochs == 0:
-                self.save_model()
-            self.epoch += 1
-
-
-if __name__ == '__main__':
-    prep = Preparation()
-    prep.train()
\ No newline at end of file
diff --git a/experiments/train_SimNoGoal.py b/experiments/train_SimNoGoal.py
deleted file mode 100644
index 5f5dbc4..0000000
--- a/experiments/train_SimNoGoal.py
+++ /dev/null
@@ -1,238 +0,0 @@
-
-import logging
-import sys
-import numpy as np
-from collections import defaultdict
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import copy
-import wandb
-from data.loader import data_loader
-from utils.losses import coll_smoothed_loss
-from utils.utils import get_dset_path
-from config import *
-from experiments.models.SIMnoGoal import TrajectoryGenerator
-from experiments.evaluate_GAN import evaluate
-from utils.utils import relative_to_abs
-
-torch.backends.cudnn.benchmark = False
-torch.backends.cudnn.enabled = True
-
-import warnings
-warnings.filterwarnings("ignore")
-
-FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
-logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)
-logger = logging.getLogger(__name__)
-
-
-
-def get_dtypes(args):
-    long_dtype = torch.LongTensor
-    float_dtype = torch.FloatTensor
-    if args.use_gpu == 1:
-        long_dtype = torch.cuda.LongTensor
-        float_dtype = torch.cuda.FloatTensor
-    return long_dtype, float_dtype
-
-
-class Preparation:
-    def __init__(self, data=None):
-        self.config = Config()
-        if data:
-            self.config.dataset_name = data
-            group_name = self.config.experiment_name
-            self.config.experiment_name =  self.config.experiment_name + '-' + data
-            path = self.config.DIR + self.config.experiment_name
-            if not os.path.exists(path):
-                os.makedirs(path)
-            self.config.model_path = path
-            self.config.output_dir = self.config.model_path
-            self.config.checkpoint_start_from = self.config.model_path + '/checkpoint_with_model.pt'
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True, group=group_name)
-        else:
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True)
-            # print('no_wandb')
-        seed = self.config.seed
-        torch.manual_seed(seed)
-        np.random.seed(seed)
-
-        os.environ['CUDA_VISIBLE_DEVICES'] = self.config.gpu_num
-        train_path = get_dset_path(self.config.dataset_name, 'train')
-        val_path = get_dset_path(self.config.dataset_name, 'test')
-        long_dtype, float_dtype = get_dtypes(self.config)
-        logger.info("Initializing train dataset")
-        self.train_dset, self.train_loader = data_loader(self.config, train_path, augment=self.config.augment)
-        logger.info("Initializing val dataset")
-        _, self.val_loader = data_loader(self.config, val_path)
-        self.iterations_per_epoch = len(self.train_loader)
-        logger.info(
-            'There are {} iterations per epoch'.format(self.iterations_per_epoch)
-        )
-
-        self.generator = TrajectoryGenerator(self.config)
-        self.generator.type(float_dtype).train()
-        logger.info('Here is the generator:')
-        logger.info(self.generator)
-
-        if self.config.adam:
-            print('Learning with ADAM!')
-            # betas_d = (0.5, 0.9999)
-            self.optimizer = optim.Adam(self.generator.parameters(), lr=self.config.g_learning_rate)
-        else:
-            print('Learning with RMSprop!')
-            self.optimizer = optim.SGD(self.generator.parameters(), lr=0.001, momentum=0.9)
-        restore_path = None
-        if self.config.checkpoint_start_from is not None:
-            restore_path = self.config.checkpoint_start_from
-        elif self.config.restore_from_checkpoint == True:
-            restore_path = os.path.join(self.config.output_dir,
-                                        '%s_with_model.pt' % self.config.checkpoint_name)
-
-        if os.path.isfile(restore_path):
-            logger.info('Restoring from checkpoint {}'.format(restore_path))
-            self.checkpoint = torch.load(restore_path)
-            self.generator.load_state_dict(self.checkpoint['state'])
-            self.optimizer.load_state_dict(self.checkpoint['optim_state'])
-            self.t = self.checkpoint['counters']['t']
-            self.epoch = self.checkpoint['counters']['epoch']
-            self.checkpoint['restore_ts'].append(self.t)
-        else:
-            # Starting from scratch, so initialize checkpoint data structure
-            self.t, self.epoch = 0, 0
-            self.checkpoint = {
-                'args': self.config.__dict__,
-                'losses': defaultdict(list),
-                'losses_ts': [],
-                'metrics_val': defaultdict(list),
-                'metrics_train': defaultdict(list),
-                'sample_ts': [],
-                'restore_ts': [],
-                'counters': {
-                    't': None,
-                    'epoch': None,
-                },
-                'state': None,
-                'optim_state': None,
-                'best_state': None,
-            }
-
-    def check_accuracy(self, loader, generator):  # TODO Change this!
-        metrics = {}
-        generator.eval()  # will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
-
-        ade, fde, act, _ = evaluate(self.config, loader, generator)
-        metrics['act'] = act
-        metrics['ade'] = ade
-        metrics['fde'] = fde
-        mean = (act+ ade)/2
-        metrics['mean'] = mean
-        generator.train()
-        wandb.log({"ade":  metrics['ade'], "fde": metrics['fde'],
-                   "act_best_ade": metrics['act'], "Mean_ade_act": metrics['mean']})
-        return metrics
-
-    def print_stats(self):
-        dictlist = 'Epoch = {}, t = {} '.format(self.epoch, self.t) + '[D] '
-        dictlist += ' [G] '
-
-        for k, v in sorted(self.losses.items()):
-            self.checkpoint['losses'][k].append(v)
-            dictlist += ' ' + '{}: {:.6f}'.format(k, v)
-
-        logger.info(dictlist)
-
-        self.checkpoint['losses_ts'].append(self.t)
-
-    def save_model(self):
-        self.checkpoint['counters']['t'] = self.t
-        self.checkpoint['counters']['epoch'] = self.epoch
-        self.checkpoint['sample_ts'].append(self.t)
-
-        # Check stats on the validation set
-        logger.info('Checking stats on val ...')
-        metrics_val = self.check_accuracy(self.val_loader, self.generator)
-    #    self.scheduler.step(metrics_val['ade'])
-        for k, v in sorted(metrics_val.items()):
-            logger.info('  [val] {}: {:.3f}'.format(k, v))
-            self.checkpoint['metrics_val'][k].append(v)
-
-        min_mean = min(self.checkpoint['metrics_val']['mean'])
-        if metrics_val['mean'] == min_mean:
-            logger.info('New low for mean error')
-            self.checkpoint['best_t'] = self.t
-            self.checkpoint['best_state'] = copy.deepcopy(self.generator.state_dict())
-
-        # Save another checkpoint with model weights and
-        # optimizer state
-        self.checkpoint['state'] = self.generator.state_dict()
-        self.checkpoint['optim_state'] = self.optimizer.state_dict()
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_with_model.pt' % self.config.checkpoint_name
-        )
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        torch.save(self.checkpoint, checkpoint_path)
-        torch.save(self.checkpoint, os.path.join(wandb.run.dir, 'model.pt'))
-        logger.info('Done.')
-
-        # Save a checkpoint with no model weights by making a shallow
-        # copy of the checkpoint excluding some items
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_no_model.pt' % self.config.checkpoint_name)
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        key_blacklist = [
-            'state', 'best_state', 'optim_state'
-        ]
-        small_checkpoint = {}
-        for k, v in self.checkpoint.items():
-            if k not in key_blacklist:
-                small_checkpoint[k] = v
-        torch.save(small_checkpoint, checkpoint_path)
-        logger.info('Done.')
-
-    def model_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask,\
-        loss_mask, seq_start_end, nei_num_index, nei_num, = batch
-
-        losses = {}
-        MSE_loss = nn.MSELoss()
-        for param in self.generator.parameters(): param.grad = None
-
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-        pred_traj_fake_rel, nll, pred_traj_fake_rel_sampled = self.generator(model_input, obs_traj, pred_traj_gt,
-                                                                 seq_start_end, nei_num_index,
-                                                 nei_num, mode='train',)
-        pred_traj_fake_abs = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-        pred_traj_fake_abs_sampled = relative_to_abs(pred_traj_fake_rel_sampled, obs_traj[-1]) #ToDo: detach this?
-
-        corrected_pos_loss = MSE_loss(pred_traj_fake_abs, pred_traj_fake_abs_sampled)
-
-
-        loss_count = coll_smoothed_loss(pred_traj_fake_abs, seq_start_end, nei_num_index)
-
-
-        loss = nll  + 30. * loss_count + corrected_pos_loss
-        loss.backward()
-        losses['nll'] = nll.item()
-        torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=3.0, norm_type=2)
-        self.optimizer.step()
-        wandb.log({"nll": nll.item()})
-        return losses
-
-    def train(self):
-        self.t_step = 0
-        while self.epoch < self.config.num_epochs:
-            self.t_step = 0
-            logger.info('Starting epoch {}'.format(self.epoch))
-            for batch in self.train_loader:
-                batch = [tensor.cuda() for tensor in batch]
-                self.losses = self.model_step(batch)
-                self.t_step += 1
-            if self.epoch % self.config.check_after_num_epochs == 0:
-                self.save_model()
-            self.epoch += 1
-
-if __name__ == '__main__':
-    prep = Preparation()
-    prep.train()
\ No newline at end of file
diff --git a/experiments/train_VAE.py b/experiments/train_VAE.py
deleted file mode 100644
index 087b1e7..0000000
--- a/experiments/train_VAE.py
+++ /dev/null
@@ -1,254 +0,0 @@
-
-import logging
-import sys
-import numpy as np
-from collections import defaultdict
-import torch
-import torch.optim as optim
-import copy
-import wandb
-from data.loader import data_loader
-from utils.losses import coll_smoothed_loss, compute_mmd
-from utils.utils import get_dset_path
-from config import *
-from models.VAE import TrajectoryGenerator
-from evaluate_GAN import evaluate
-from utils.utils import relative_to_abs
-
-torch.backends.cudnn.benchmark = False
-torch.backends.cudnn.enabled = True
-
-import warnings
-warnings.filterwarnings("ignore")
-
-FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
-logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)
-logger = logging.getLogger(__name__)
-
-
-
-def get_dtypes(args):
-    long_dtype = torch.LongTensor
-    float_dtype = torch.FloatTensor
-    if args.use_gpu == 1:
-        long_dtype = torch.cuda.LongTensor
-        float_dtype = torch.cuda.FloatTensor
-    return long_dtype, float_dtype
-
-
-class Preparation:
-    def __init__(self, data=None):
-        self.config = Config()
-        if data:
-            self.config.dataset_name = data
-            group_name = self.config.experiment_name
-            self.config.experiment_name =  self.config.experiment_name + '-' + data
-            path = self.config.DIR + self.config.experiment_name
-            if not os.path.exists(path):
-                os.makedirs(path)
-            self.config.model_path = path
-            self.config.output_dir = self.config.model_path
-            self.config.checkpoint_start_from = self.config.model_path + '/checkpoint_with_model.pt'
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True, group=group_name)
-        else:
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True)
-            # print('no_wandb')
-        seed = self.config.seed
-        torch.manual_seed(seed)
-        np.random.seed(seed)
-
-        os.environ['CUDA_VISIBLE_DEVICES'] = self.config.gpu_num
-        train_path = get_dset_path(self.config.dataset_name, 'train')
-        val_path = get_dset_path(self.config.dataset_name, 'test')
-        long_dtype, float_dtype = get_dtypes(self.config)
-        logger.info("Initializing train dataset")
-        self.train_dset, self.train_loader = data_loader(self.config, train_path, augment=self.config.augment)
-        logger.info("Initializing val dataset")
-        _, self.val_loader = data_loader(self.config, val_path)
-        self.iterations_per_epoch = len(self.train_loader)
-        logger.info(
-            'There are {} iterations per epoch'.format(self.iterations_per_epoch)
-        )
-
-        self.generator = TrajectoryGenerator(self.config)
-        self.generator.type(float_dtype).train()
-        logger.info('Here is the generator:')
-        logger.info(self.generator)
-
-        # Log model
-        # wandb.watch(self.generator, log='all')
-
-
-        if self.config.adam:
-            print('Learning with ADAM!')
-            # betas_d = (0.5, 0.9999)
-            self.optimizer = optim.Adam(self.generator.parameters(), lr=self.config.g_learning_rate)
-        else:
-            print('Learning with RMSprop!')
-            self.optimizer = optim.SGD(self.generator.parameters(), lr=0.001, momentum=0.9)
-        restore_path = None
-        if self.config.checkpoint_start_from is not None:
-            restore_path = self.config.checkpoint_start_from
-        elif self.config.restore_from_checkpoint == True:
-            restore_path = os.path.join(self.config.output_dir,
-                                        '%s_with_model.pt' % self.config.checkpoint_name)
-
-        if os.path.isfile(restore_path):
-            logger.info('Restoring from checkpoint {}'.format(restore_path))
-            self.checkpoint = torch.load(restore_path)
-            self.generator.load_state_dict(self.checkpoint['state'])
-            self.optimizer.load_state_dict(self.checkpoint['optim_state'])
-            self.t = self.checkpoint['counters']['t']
-            self.epoch = self.checkpoint['counters']['epoch']
-            self.checkpoint['restore_ts'].append(self.t)
-        else:
-            # Starting from scratch, so initialize checkpoint data structure
-            self.t, self.epoch = 0, 0
-            self.checkpoint = {
-                'args': self.config.__dict__,
-                'losses': defaultdict(list),
-                'losses_ts': [],
-                'metrics_val': defaultdict(list),
-                'metrics_train': defaultdict(list),
-                'sample_ts': [],
-                'restore_ts': [],
-                'counters': {
-                    't': None,
-                    'epoch': None,
-                },
-                'state': None,
-                'optim_state': None,
-                'best_state': None,
-            }
-
-    def check_accuracy(self, loader, generator):  # TODO Change this!
-        metrics = {}
-        generator.eval()  # will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
-
-        ade, fde, act, _ = evaluate(self.config, loader, generator)
-        metrics['act'] = act
-        metrics['ade'] = ade
-        metrics['fde'] = fde
-        mean = (act+ ade)/2
-        metrics['mean'] = mean
-        generator.train()
-        wandb.log({"ade":  metrics['ade'], "fde": metrics['fde'],
-                   "act_best_ade": metrics['act'], "Mean_ade_act": metrics['mean']})
-        return metrics
-
-    def print_stats(self):
-        dictlist = 'Epoch = {}, t = {} '.format(self.epoch, self.t) + '[D] '
-        dictlist += ' [G] '
-
-        for k, v in sorted(self.losses.items()):
-            self.checkpoint['losses'][k].append(v)
-            dictlist += ' ' + '{}: {:.6f}'.format(k, v)
-
-        logger.info(dictlist)
-
-        self.checkpoint['losses_ts'].append(self.t)
-
-    def save_model(self):
-        self.checkpoint['counters']['t'] = self.t
-        self.checkpoint['counters']['epoch'] = self.epoch
-        self.checkpoint['sample_ts'].append(self.t)
-
-        # Check stats on the validation set
-        logger.info('Checking stats on val ...')
-        metrics_val = self.check_accuracy(self.val_loader, self.generator)
-    #    self.scheduler.step(metrics_val['ade'])
-        for k, v in sorted(metrics_val.items()):
-            logger.info('  [val] {}: {:.3f}'.format(k, v))
-            self.checkpoint['metrics_val'][k].append(v)
-
-        min_mean = min(self.checkpoint['metrics_val']['mean'])
-        if metrics_val['mean'] == min_mean:
-            logger.info('New low for mean error')
-            self.checkpoint['best_t'] = self.t
-            self.checkpoint['best_state'] = copy.deepcopy(self.generator.state_dict())
-
-        # Save another checkpoint with model weights and
-        # optimizer state
-        self.checkpoint['state'] = self.generator.state_dict()
-        self.checkpoint['optim_state'] = self.optimizer.state_dict()
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_with_model.pt' % self.config.checkpoint_name
-        )
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        torch.save(self.checkpoint, checkpoint_path)
-        torch.save(self.checkpoint, os.path.join(wandb.run.dir, 'model.pt'))
-        logger.info('Done.')
-
-        # Save a checkpoint with no model weights by making a shallow
-        # copy of the checkpoint excluding some items
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_no_model.pt' % self.config.checkpoint_name)
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        key_blacklist = [
-            'state', 'best_state', 'optim_state'
-        ]
-        small_checkpoint = {}
-        for k, v in self.checkpoint.items():
-            if k not in key_blacklist:
-                small_checkpoint[k] = v
-        torch.save(small_checkpoint, checkpoint_path)
-        logger.info('Done.')
-
-    def model_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask,\
-        loss_mask, seq_start_end, nei_num_index, nei_num, = batch
-
-        losses = {}
-
-        for param in self.generator.parameters(): param.grad = None
-
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-
-        pred_traj_fake_rel, nll, z_mu, z_log_var, \
-        z_mu_obs, z_log_var_obs = self.generator(model_input, obs_traj, pred_traj_gt,
-                                             seq_start_end, nei_num_index, nei_num,
-                                             mode='train')
-
-        pred_traj_fake_abs = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-
-        loss_count = coll_smoothed_loss(pred_traj_fake_abs, seq_start_end, nei_num_index)
-
-        z_std = torch.exp(z_log_var / 2)
-        z_std_obs = torch.exp(z_log_var_obs / 2)
-        eps = torch.randn_like(z_std)
-
-        z = z_mu + z_std * eps
-        z_obs = z_mu_obs + z_std_obs * eps
-        MMD_loss = compute_mmd(z_obs, z)
-
-        loss = nll + 30. * loss_count + self.MMD_multiplier*MMD_loss
-        loss.backward()
-        losses['nll'] = nll.item()
-        torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=3.0, norm_type=2)
-        self.optimizer.step()
-        wandb.log({"nll": nll.item()})
-        return losses
-
-    def train(self):
-        self.t_step = 0
-        while self.epoch < self.config.num_epochs:
-            self.t_step = 0
-            if self.epoch == self.config.alpha_epoch_start:
-                self.MMD_multiplier = self.config.initial_mmd
-            if self.config.initial_mmd <= self.config.MMD_multiplier < self.config.max_mmd:
-                self.MMD_multiplier *= math.exp(self.config.alpha_multiplier * self.epoch)
-            if self.MMD_multiplier > self.config.max_mmd:
-                self.MMD_multiplier = self.config.max_mmd
-
-            logger.info('Starting epoch {}'.format(self.epoch))
-            for batch in self.train_loader:
-                batch = [tensor.cuda() for tensor in batch]
-                self.losses = self.model_step(batch)
-                self.t_step += 1
-            if self.epoch % self.config.check_after_num_epochs == 0:
-                self.save_model()
-            self.epoch += 1
-
-if __name__ == '__main__':
-    prep = Preparation()
-    prep.train()
\ No newline at end of file
diff --git a/experiments/train_dataset_GANorVAE.py b/experiments/train_dataset_GANorVAE.py
deleted file mode 100644
index 2bec65c..0000000
--- a/experiments/train_dataset_GANorVAE.py
+++ /dev/null
@@ -1,15 +0,0 @@
-from train_GAN import Preparation
-from config import Config
-config = Config()
-
-if config.trajnet:
-    # datasets = ['students1','students3', 'zara1', 'zara3', 'hotel','lcas']
-    datasets = ['wildtrack', 'students1', 'students3', 'zara1', 'zara3', 'hotel', 'lcas']
-else:
-    datasets = ['eth', 'zara1', 'zara2', 'univ', 'hotel']  # eth & utc
-if __name__ == '__main__':
-    for data in datasets:
-        prep = Preparation(data)
-        prep.train()
-        del prep
-        print('START NEW DATASET ####################################################')
\ No newline at end of file
diff --git a/models/GoalFLow.py b/models/GoalFLow.py
deleted file mode 100644
index d3f5918..0000000
--- a/models/GoalFLow.py
+++ /dev/null
@@ -1,143 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from config import *
-config = Config()
-import models.flows as fnn
-from utils.losses import diversit_obj
-from models.utils import Pooling_net
-
-class GoalGenerator(nn.Module):
-    def __init__(self,config):
-        super(GoalGenerator, self).__init__()
-        self.config = config
-        obs_len= config.obs_len
-        pred_len= config.pred_len
-        traj_lstm_input_size= 2
-        traj_lstm_hidden_size=16
-        rela_embed_size = 16
-        self.inputLayer_encoder = nn.Linear(traj_lstm_input_size, rela_embed_size)
-        self.inputLayer_decoder = nn.Linear(traj_lstm_input_size + 16, rela_embed_size)
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-        self.pl_net = Pooling_net(h_dim=traj_lstm_hidden_size)
-        self.clamp_flow = config.clamp_flow
-
-        modules = []
-        for _ in range(4):
-            modules += [
-                # fnn.ActNorm(traj_lstm_input_size),
-                fnn.MADE(traj_lstm_input_size, traj_lstm_hidden_size,
-                         traj_lstm_hidden_size, act='relu',clamp=self.clamp_flow),
-                fnn.Reverse(2)
-            ]
-        self.maf = fnn.FlowSequential(*modules)
-        for module in self.maf.modules():
-            if isinstance(module, nn.Linear):
-                nn.init.orthogonal_(module.weight)
-                if hasattr(module, 'bias') and module.bias is not None:
-                    module.bias.data.fill_(0)
-
-        self.maf.to('cuda')
-        # self.train()
-
-        self.traj_lstm_hidden_size = traj_lstm_hidden_size
-        self.traj_lstm_input_size = traj_lstm_input_size
-        self.pred_lstm_hidden_size = self.traj_lstm_hidden_size
-        self.traj_lstm_model = nn.LSTMCell(rela_embed_size, 16)
-        self.pred_lstm_model =  nn.LSTMCell(rela_embed_size, 16)
-        self.pred_hidden2pos = nn.Linear(traj_lstm_hidden_size, 2)
-        self.dropout = nn.Dropout(p=0.1)
-
-        self.init_parameters()
-        self.noise_dim = (8,)
-        self.noise_type = 'gaussian'
-        self.noise_mix_type = 'global'
-
-    def init_parameters(self):
-        nn.init.constant_(self.inputLayer_encoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder.weight, std=0.2)
-
-        nn.init.constant_(self.inputLayer_decoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_decoder.weight, std=0.2)
-
-        nn.init.xavier_uniform_(self.traj_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.traj_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.traj_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.traj_lstm_model.bias_hh, 0.0)
-        n = self.traj_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.traj_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-        nn.init.xavier_uniform_(self.pred_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.pred_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.pred_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.pred_lstm_model.bias_hh, 0.0)
-        n = self.pred_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.pred_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-    def init_hidden_traj_lstm(self, batch):
-        return (
-            torch.randn(batch, 16).cuda(),
-            torch.randn(batch, 16).cuda(),
-        )
-
-
-    def forward(self, traj_rel, obs_traj_pos, pred_traj_gt_pos, seq_start_end,
-                nei_index, nei_num_index, mode="test", plot_sample=False, sampling_module=None):
-        batch = traj_rel.shape[1]
-
-        nll = 0.
-        traj_lstm_h_t, traj_lstm_c_t = self.init_hidden_traj_lstm(batch)
-        final_goal_gt = pred_traj_gt_pos[-1] - obs_traj_pos[-1]
-        for i, input_t in enumerate(traj_rel[: self.obs_len].chunk(
-                traj_rel[: self.obs_len].size(0), dim=0)):
-            input_embedded = self.dropout(F.relu(self.inputLayer_encoder(input_t.squeeze(0))))
-            lstm_state = self.traj_lstm_model(
-                input_embedded, (traj_lstm_h_t, traj_lstm_c_t)
-            )
-            traj_lstm_h_t, traj_lstm_c_t = lstm_state
-
-        pred_lstm_hidden = traj_lstm_h_t
-        output = traj_rel[self.obs_len]
-        curr_pos_abs = obs_traj_pos[-1]
-
-        corr = curr_pos_abs.repeat(batch, 1, 1)
-        corr_index = (corr.transpose(0,1)-corr)
-
-        # lstm_state_context, _, _ = self.pl_net(corr_index, nei_index[i], nei_num_index, pred_lstm_hidden, curr_pos_abs)
-        concat_output = pred_lstm_hidden #+ lstm_state_context
-        noise = torch.randn([concat_output.shape[0],2])
-
-
-        if mode == 'sampling':
-            noise_sampled = sampling_module(concat_output.detach()).permute(1,0,2)
-            output_pred_k_all = []
-            for noise_sampled_k in noise_sampled:
-                output_pred_k = self.maf.sample(concat_output.detach(), output.detach(), noise=noise_sampled_k)
-                nll = nll - self.maf.log_probs(output_pred_k, concat_output.detach(), output.detach())[1]  # + l2
-                output_pred_k_all.append(output_pred_k + obs_traj_pos[-1])
-            output_pred_k = torch.stack(output_pred_k_all)
-            div_loss = diversit_obj(output_pred_k, seq_start_end)
-            nll = nll.mean() - div_loss # - div_loss to maximise the distanze!
-        # output_pred = self.pred_hidden2pos(concat_output)  + obs_traj_pos[-1]
-        elif mode != 'test':
-            nll = nll - self.maf.log_probs(final_goal_gt, concat_output, output.detach())[1] #+ min_k_fde
-        output_pred = self.maf.sample(concat_output, output.detach(), noise=noise) + obs_traj_pos[-1]
-        if plot_sample:
-            plt_s = []
-            for l in range(20):
-                noise = torch.randn([concat_output.shape[0], 2])
-                plt_s.append(self.maf.sample(concat_output, output.detach(), noise=noise) + obs_traj_pos[-1])
-            samples = torch.stack(plt_s)
-
-        if mode == 'test':
-            if plot_sample:
-                return output_pred, samples
-            else:
-                return output_pred
-        elif mode == 'train':
-            return output_pred, nll.mean()
-        elif mode == 'sampling':
-            return output_pred_k, nll
diff --git a/models/GoalFLow_mlp.py b/models/GoalFLow_mlp.py
deleted file mode 100644
index 1d27d85..0000000
--- a/models/GoalFLow_mlp.py
+++ /dev/null
@@ -1,147 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from config import *
-config = Config()
-import models.flows as fnn
-from utils.losses import diversit_obj
-from models.utils import Pooling_net
-from torch.nn.utils import spectral_norm
-
-class GoalGenerator(nn.Module):
-    def __init__(self,config):
-        super(GoalGenerator, self).__init__()
-        self.config = config
-        obs_len= config.obs_len
-        pred_len= config.pred_len
-        traj_lstm_input_size= 2
-        traj_lstm_hidden_size=16
-        rela_embed_size = 16
-
-        self.obs_len = obs_len
-        self.pred_len = pred_len
-        self.pl_net = Pooling_net(h_dim=traj_lstm_hidden_size)
-        self.inputLayer_encoder = spectral_norm(nn.Linear(2 * self.obs_len, traj_lstm_hidden_size))
-
-
-        modules = []
-        for _ in range(4):
-            modules += [
-                fnn.MADE(traj_lstm_input_size, traj_lstm_hidden_size, traj_lstm_hidden_size, act='relu'),
-                # fnn.BatchNormFlow(traj_lstm_input_size),
-                fnn.Reverse(2)
-            ]
-        self.maf = fnn.FlowSequential(*modules)
-        for module in self.maf.modules():
-            if isinstance(module, nn.Linear):
-                nn.init.orthogonal_(module.weight)
-                if hasattr(module, 'bias') and module.bias is not None:
-                    module.bias.data.fill_(0)
-
-        self.maf.to('cuda')
-        # self.train()
-        self.pred_hidden2pos = spectral_norm(nn.Linear(traj_lstm_hidden_size, 2))
-        self.dropout = nn.Dropout(p=0.1)
-
-        # self.init_parameters()
-
-
-    def init_parameters(self):
-        nn.init.constant_(self.inputLayer_encoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_encoder.weight, std=0.2)
-
-        nn.init.constant_(self.inputLayer_decoder.bias, 0.0)
-        nn.init.normal_(self.inputLayer_decoder.weight, std=0.2)
-
-        nn.init.xavier_uniform_(self.traj_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.traj_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.traj_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.traj_lstm_model.bias_hh, 0.0)
-        n = self.traj_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.traj_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-        nn.init.xavier_uniform_(self.pred_lstm_model.weight_ih)
-        nn.init.orthogonal_(self.pred_lstm_model.weight_hh, gain=0.001)
-
-        nn.init.constant_(self.pred_lstm_model.bias_ih, 0.0)
-        nn.init.constant_(self.pred_lstm_model.bias_hh, 0.0)
-        n = self.pred_lstm_model.bias_ih.size(0)
-        nn.init.constant_(self.pred_lstm_model.bias_ih[n // 4:n // 2], 1.0)
-
-    def init_hidden_traj_lstm(self, batch):
-        return (
-            torch.randn(batch, 16).cuda(),
-            torch.randn(batch, 16).cuda(),
-        )
-    def forward(self, traj_rel, obs_traj_pos, pred_traj_gt_pos, seq_start_end,
-                nei_index, nei_num_index, mode="test", plot_sample=False, sampling_module=None):
-        batch = traj_rel.shape[1]
-
-        nll = 0.
-
-        final_goal_gt = pred_traj_gt_pos[-1] - obs_traj_pos[-1]
-        # for i, input_t in enumerate(traj_rel[: self.obs_len].chunk(
-        #         traj_rel[: self.obs_len].size(0), dim=0)):
-        #     input_embedded = self.dropout(F.relu(self.inputLayer_encoder(input_t.squeeze(0))))
-        #     lstm_state = self.traj_lstm_model(
-        #         input_embedded, (traj_lstm_h_t, traj_lstm_c_t)
-        #     )
-        #     traj_lstm_h_t, traj_lstm_c_t = lstm_state
-        traj_lstm_h_t = self.dropout(F.relu(self.inputLayer_encoder(traj_rel[: self.obs_len].permute(1,0,2).reshape(-1,2*self.obs_len))))
-        pred_lstm_hidden = traj_lstm_h_t
-        output = traj_rel[self.obs_len]
-        # curr_pos_abs = obs_traj_pos[-1]
-
-        # corr = curr_pos_abs.repeat(batch, 1, 1)
-        # corr_index = (corr.transpose(0,1)-corr)
-
-        # lstm_state_context, _, _ = self.pl_net(corr_index, nei_index[i], nei_num_index, pred_lstm_hidden, curr_pos_abs)
-        concat_output = pred_lstm_hidden #+ lstm_state_context
-        noise = torch.randn([concat_output.shape[0],2])
-        # output_pred =10.*torch.tanh(self.pred_hidden2pos(concat_output)) + obs_traj_pos[-1]
-        # noise = 0 + 0.5 * noise
-        output_pred = self.maf.sample(concat_output, output.detach(), noise=noise) + obs_traj_pos[-1]
-
-        if mode == 'sampling':
-            noise_sampled = sampling_module(concat_output.detach()).permute(1,0,2)
-            output_pred_k_all = []
-            for noise_sampled_k in noise_sampled:
-                output_pred_k = self.maf.sample(concat_output.detach(), output.detach(), noise=noise_sampled_k)
-                nll = nll - self.maf.log_probs(output_pred_k, concat_output.detach(), output.detach())[1]  # + l2
-                output_pred_k_all.append(output_pred_k + obs_traj_pos[-1])
-            output_pred_k = torch.stack(output_pred_k_all)
-            div_loss = diversit_obj(output_pred_k, seq_start_end)
-            nll = nll.mean() - div_loss # - div_loss to maximise the distance!
-        # output_pred = self.pred_hidden2pos(concat_output)  + obs_traj_pos[-1]
-        elif mode != 'test':
-            min_k_fde = []
-            # for k in range(20):
-            #     noise = torch.randn([concat_output.shape[0], 2])
-            #     tmp_pre = self.maf.sample(concat_output, output.detach(), noise=noise) #+ obs_traj_pos[-1]
-            #     kde_loss = (tmp_pre-final_goal_gt).pow(2).sum(dim=-1)
-            #     min_k_fde.append(kde_loss)
-            # min_k_fde = torch.stack(min_k_fde).permute(1,0)
-            # min_k_fde = torch.min(min_k_fde,dim=1)[0]
-
-            nll = nll - self.maf.log_probs(final_goal_gt, concat_output, output.detach())[1] #+ min_k_fde
-            # nll = min_k_fde
-
-        if plot_sample:
-            plt_s = []
-            for l in range(20):
-                noise = torch.randn([concat_output.shape[0], 2])
-                # noise = 0 + 0.5*noise
-                plt_s.append(self.maf.sample(concat_output, output.detach(), noise=noise) + obs_traj_pos[-1])
-            samples = torch.stack(plt_s)
-
-
-        if mode == 'test':
-            if plot_sample:
-                return output_pred, samples
-            else:
-                return output_pred
-        elif mode == 'train':
-            return output_pred, nll.mean()
-        elif mode == 'sampling':
-            return output_pred_k, nll
diff --git a/models/Robot.py b/models/Robot.py
deleted file mode 100644
index 3c323d0..0000000
--- a/models/Robot.py
+++ /dev/null
@@ -1,13 +0,0 @@
-import torch.nn as nn
-import torch.nn.functional as F
-from torch import tanh
-
-class Robot(nn.Module):
-    def __init__(self):
-        super(Robot, self).__init__()
-        self.mlp_robot1 = nn.Linear(16 + 6, 16)
-        self.mlp_robot2 = nn.Linear(16, 2)
-
-    def forward(self, x):
-        x = F.relu(self.mlp_robot1(x))
-        return tanh(self.mlp_robot2(x))*2.
diff --git a/models/Sampler.py b/models/Sampler.py
deleted file mode 100644
index 905ff84..0000000
--- a/models/Sampler.py
+++ /dev/null
@@ -1,30 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from config import *
-config = Config()
-
-class Sampler(nn.Module):
-    def __init__(self,config):
-        super(Sampler, self).__init__()
-        self.config = config
-        obs_len= config.obs_len
-        pred_len= config.pred_len
-        traj_lstm_input_size= 2
-        traj_lstm_hidden_size=16
-        rela_embed_size = 16
-
-        self.L1 = nn.Linear(traj_lstm_hidden_size, 32)
-        self.L2 = nn.Linear(32, 32)
-        self.L3 = nn.Linear(32, config.num_samples*2)
-        self.dropout = nn.Dropout(p=0.25)
-
-    def forward(self, concat_output):
-        # eps = torch.randn_like(concat_output).to(concat_output)
-        x = F.relu(self.L1(concat_output))
-        x = F.relu(self.L2(x))
-        samples = torch.tanh(self.L3(x))*2
-        samples = samples.view(concat_output.shape[0],config.num_samples,2)
-        return samples
-
diff --git a/models/flows.py b/models/flows.py
deleted file mode 100644
index 014a7ce..0000000
--- a/models/flows.py
+++ /dev/null
@@ -1,796 +0,0 @@
-import math
-import types
-
-import numpy as np
-import scipy as sp
-import scipy.linalg
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-
-def get_mask(in_features, out_features, in_flow_features, mask_type=None):
-    """
-    mask_type: input | None | output
-    
-    See Figure 1 for a better illustration:
-    https://arxiv.org/pdf/1502.03509.pdf
-    """
-    if mask_type == 'input':
-        in_degrees = torch.arange(in_features) % in_flow_features
-    else:
-        in_degrees = torch.arange(in_features) % (in_flow_features - 1)
-
-    if mask_type == 'output':
-        out_degrees = torch.arange(out_features) % in_flow_features - 1
-    else:
-        out_degrees = torch.arange(out_features) % (in_flow_features - 1)
-
-    return (out_degrees.unsqueeze(-1) >= in_degrees.unsqueeze(0)).float()
-
-
-class MaskedLinear(nn.Module):
-    def __init__(self,
-                 in_features,
-                 out_features,
-                 mask,
-                 cond_in_features=None,
-                 bias=True):
-        super(MaskedLinear, self).__init__()
-        self.linear = nn.Linear(in_features, out_features)
-        if cond_in_features is not None:
-            self.cond_linear = nn.Linear(
-                cond_in_features, out_features, bias=False)
-
-        self.register_buffer('mask', mask)
-
-    def forward(self, inputs, cond_inputs=None):
-        output = F.linear(inputs, self.linear.weight * self.mask,
-                          self.linear.bias)
-        if cond_inputs is not None:
-            output += self.cond_linear(cond_inputs)
-        return output
-
-
-nn.MaskedLinear = MaskedLinear
-
-
-class MADESplit(nn.Module):
-    """ An implementation of MADE
-    (https://arxiv.org/abs/1502.03509).
-    """
-
-    def __init__(self,
-                 num_inputs,
-                 num_hidden,
-                 num_cond_inputs=None,
-                 s_act='tanh',
-                 t_act='relu',
-                 pre_exp_tanh=False):
-        super(MADESplit, self).__init__()
-
-        self.pre_exp_tanh = pre_exp_tanh
-
-        activations = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}
-
-        input_mask = get_mask(num_inputs, num_hidden, num_inputs,
-                              mask_type='input')
-        hidden_mask = get_mask(num_hidden, num_hidden, num_inputs)
-        output_mask = get_mask(num_hidden, num_inputs, num_inputs,
-                               mask_type='output')
-
-        act_func = activations[s_act]
-        self.s_joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,
-                                      num_cond_inputs)
-
-        self.s_trunk = nn.Sequential(act_func(),
-                                   nn.MaskedLinear(num_hidden, num_hidden,
-                                                   hidden_mask), act_func(),
-                                   nn.MaskedLinear(num_hidden, num_inputs,
-                                                   output_mask))
-
-        act_func = activations[t_act]
-        self.t_joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,
-                                      num_cond_inputs)
-
-        self.t_trunk = nn.Sequential(act_func(),
-                                   nn.MaskedLinear(num_hidden, num_hidden,
-                                                   hidden_mask), act_func(),
-                                   nn.MaskedLinear(num_hidden, num_inputs,
-                                                   output_mask))
-        
-    def forward(self, inputs, cond_inputs=None, mode='direct'):
-        if mode == 'direct':
-            h = self.s_joiner(inputs, cond_inputs)
-            m = self.s_trunk(h)
-            
-            h = self.t_joiner(inputs, cond_inputs)
-            a = self.t_trunk(h)
-
-            if self.pre_exp_tanh:
-                a = torch.tanh(a)
-            
-            u = (inputs - m) * torch.exp(-a)
-            return u, -a.sum(-1, keepdim=True)
-
-        else:
-            x = torch.zeros_like(inputs)
-            for i_col in range(inputs.shape[1]):
-                h = self.s_joiner(x, cond_inputs)
-                m = self.s_trunk(h)
-
-                h = self.t_joiner(x, cond_inputs)
-                a = self.t_trunk(h)
-
-                if self.pre_exp_tanh:
-                    a = torch.tanh(a)
-
-                x[:, i_col] = inputs[:, i_col] * torch.exp(
-                    a[:, i_col]) + m[:, i_col]
-            return x, -a.sum(-1, keepdim=True)
-
-class MADE(nn.Module):
-    """ An implementation of MADE
-    (https://arxiv.org/abs/1502.03509).
-    """
-
-    def __init__(self,
-                 num_inputs,
-                 num_hidden,
-                 num_cond_inputs=None,
-                 act='relu',
-                 clamp=False,
-                 pre_exp_tanh=False):
-        super(MADE, self).__init__()
-
-        self.clamp = clamp
-        activations = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh, 'elu': nn.ELU, 'selu': nn.SELU}
-        act_func = activations[act]
-
-        input_mask = get_mask(
-            num_inputs, num_hidden, num_inputs, mask_type='input')
-        hidden_mask = get_mask(num_hidden, num_hidden, num_inputs)
-        output_mask = get_mask(
-            num_hidden, num_inputs * 2, num_inputs, mask_type='output')
-
-        self.joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,
-                                      num_cond_inputs)
-
-        self.trunk = nn.Sequential(act_func(),
-                                   nn.MaskedLinear(num_hidden, num_hidden,
-                                                   hidden_mask), act_func(),
-                                   nn.MaskedLinear(num_hidden, num_inputs * 2,
-                                                   output_mask))
-        # self.trunk = nn.MaskedLinear(num_hidden, num_inputs * 2, output_mask)
-
-    def forward(self, inputs, cond_inputs, last_vel=None, mode='direct'):
-        if mode == 'direct':
-            h = self.joiner(inputs, cond_inputs)
-            m, a = self.trunk(h).chunk(2, 1)
-            if self.clamp:
-                # a = a.clone()
-                # with torch.no_grad():
-                a = torch.clamp(a, min=-9, max=4)
-            u = (inputs - m) * torch.exp(-a)
-            return u, -a.sum(-1, keepdim=True)
-
-        else:
-            x = torch.zeros_like(inputs)
-            for i_col in range(inputs.shape[1]):
-                mask = torch.zeros_like(inputs, dtype=torch.bool)
-                h = self.joiner(x, cond_inputs)
-                m, a = self.trunk(h).chunk(2, 1)
-                if self.clamp:
-                    # a = a.clone()
-                    # with torch.no_grad():
-                    a = torch.clamp(a, min=-9, max=4)
-                mask[:, i_col]= True
-                x = x.masked_scatter(mask, inputs[:, i_col] * torch.exp(
-                    a[:, i_col]) + m[:, i_col])
-                if torch.isinf(x).sum() >= 1:
-                    print('inf in Flow')
-            logabsdet = torch.sum(a, dim=-1, keepdim=True)
-            return x, -logabsdet
-
-class NADE(nn.Module):
-    """ An implementation of MADE
-    (https://arxiv.org/abs/1502.03509).
-    """
-
-    def __init__(self,
-                 num_inputs,
-                 num_hidden,
-                 num_cond_inputs=None,
-                 act='relu',
-                 pre_exp_tanh=False):
-        super(NADE, self).__init__()
-
-        activations = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}
-        act_func = activations[act]
-
-        input_mask = get_mask(
-            num_inputs, num_hidden, num_inputs, mask_type='input')
-        hidden_mask = get_mask(num_hidden, num_hidden, num_inputs)
-        output_mask = get_mask(
-            num_hidden, num_inputs * 2, num_inputs, mask_type='output')
-
-        self.joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,
-                                      num_cond_inputs)
-
-        self.trunk = nn.Sequential(act_func(),
-                                   nn.MaskedLinear(num_hidden, num_hidden,
-                                                   hidden_mask), act_func(),
-                                   nn.MaskedLinear(num_hidden, num_inputs * 2,
-                                                   output_mask))
-
-    def forward(self, inputs, cond_inputs, last_vel, mode='direct'):
-        if mode == 'direct':
-            h = self.joiner(inputs, cond_inputs)
-            m, a = self.trunk(h).chunk(2, 1)
-            a =  torch.sigmoid(a)*0.2 + 1e-2
-            # a = F.softplus(a) + 1e-3
-            u = ((inputs - m ) / a)  #- last_vel
-            # u = (inputs - m) / a
-            logabsdet = torch.log(torch.abs(torch.prod(a, dim=-1))).unsqueeze(dim=1)  # determinant == product over xy-coordinates
-            # logabsdet = torch.sum(logabsdet, dim=-1, keepdim=True)
-            return u, -logabsdet
-
-        else:
-            x = torch.zeros_like(inputs)
-            for i_col in range(inputs.shape[1]):
-                mask = torch.zeros_like(inputs, dtype=torch.bool)
-                h = self.joiner(x, cond_inputs)
-                m, a = self.trunk(h).chunk(2, 1)
-                m = m + last_vel
-                # a = F.softplus(a) + 1e-3
-                a = torch.sigmoid(a)*0.2 + 1e-2
-                mask[:, i_col]= True
-                x = x.masked_scatter(mask, inputs[:, i_col] * a[:, i_col] + m[:, i_col])
-
-
-            return x, m, a
-
-
-class AF(nn.Module):
-    """ An implementation of MADE
-    (https://arxiv.org/abs/1502.03509).
-    """
-
-    def __init__(self,
-                 num_inputs,
-                 num_hidden,
-                 num_cond_inputs=None,
-                 act='relu',
-                 pre_exp_tanh=False):
-        super(AF, self).__init__()
-
-        activations = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}
-        act_func = activations[act]
-        self.joiner = nn.Linear(num_cond_inputs+num_hidden, num_hidden)
-        self.num_hidden = num_hidden
-        self._decoder = nn.GRUCell(1, num_hidden)
-
-        self._locscale = nn.Linear(num_hidden, 2)
-
-
-    def forward(self, inputs, cond_inputs, last_vel, mode='direct'):
-        h_state = torch.zeros(  # pylint: disable=no-member
-            size=(inputs.shape[0], self.num_hidden),
-            dtype=inputs.dtype,
-        ).to(inputs.device)
-        # c_state = torch.zeros_like(h_state)
-        # h = (h_state,c_state)
-        y_tm1 = torch.zeros(  # pylint: disable=no-member
-            size=(inputs.shape[0], 1),
-            dtype=inputs.dtype,
-        ).to(inputs.device)
-        z = h_state
-        if mode == 'direct':
-            # Output containers.
-            x = list()
-            scales = list()
-            # z = self.joiner(cond_inputs)
-            for t in range(inputs.shape[1]):
-                y_t = inputs[:, t]
-               # if y_t.size() == 1:
-                y_t = y_t.unsqueeze(1)
-
-
-                # Unrolls the GRU.
-                z = self._decoder(y_tm1, z)
-
-                # Predicts the location and scale of the MVN distribution.
-                pred_out = self.joiner(torch.cat([cond_inputs, z], dim =-1))
-                dloc_scale = self._locscale(pred_out)
-                dloc = dloc_scale[:, 0].unsqueeze(1)
-                scale = dloc_scale[:, 1].unsqueeze(1)
-                # scale = dloc_scale[:, 1].unsqueeze(1)
-                scale = F.softplus(scale) + 1e-2
-
-                # Base distribution corresponding sample.
-                # x_t = (y_t - (dloc)) * torch.exp(-scale)
-                x_t = (y_t - dloc) / scale
-
-                # Update containers.
-                x.append(x_t)
-                scales.append(scale)
-                y_tm1 = y_t.detach()
-
-
-            # Prepare tensors, reshape to [B, T, 2].
-            x = torch.cat(x, dim=1) # pylint: disable=no-member
-            scales = torch.cat(scales, dim=1)  # pylint: disable=no-member
-            logabsdet = torch.log(torch.abs(torch.prod(scales, dim=-1))).unsqueeze(dim=1)  # determinant == product over xy-coordinates
-            # logabsdet = torch.sum(logabsdet, dim=-1, keepdim=True)
-            # return  x, -scales.sum(-1, keepdim=True)
-            return x, -logabsdet
-
-        else:
-            # Output containers.
-            y = list()
-            scales = list()
-            mus = []
-
-            for t in range(inputs.shape[1]):
-                x_t = inputs[:, t]
-               # if y_t.size() == 1:
-                x_t = x_t.unsqueeze(1)
-                # Unrolls the GRU.
-
-                z = self._decoder(y_tm1, z)
-
-                # Predicts the location and scale of the MVN distribution.
-                pred_out = self.joiner(torch.cat([cond_inputs, z], dim =-1))
-                dloc_scale = self._locscale(pred_out)
-                dloc = dloc_scale[:, 0].unsqueeze(1) # + last_vel[:, t].unsqueeze(1)
-                scale = dloc_scale[:, 1].unsqueeze(1)
-
-                # scale = dloc_scale[:, 1].unsqueeze(1)
-                scale = F.softplus(scale) + 1e-2
-                # Base distribution corresponding sample.
-
-                y_t= dloc + scale * x_t
-                # y_t = (dloc) + scale * x_t
-                # Update containers.
-                y.append(y_t)
-                scales.append(scale)
-                mus.append(dloc)
-                y_tm1 = y_t.detach()
-
-                # Prepare tensors, reshape to [B, T, 2].
-            y = torch.cat(y, dim=1)  # pylint: disable=no-member
-            scales = torch.cat(scales, dim=1) # pylint: disable=no-member
-            mus = torch.cat(mus, dim=1)  # pylint: disable=no-m
-            logabsdet = torch.log(torch.abs(torch.prod(scales, dim=-1))).unsqueeze(dim=1)  # determinant == product over xy-coordinates
-            # logabsdet = torch.sum(logabsdet, dim=-1, keepdim=True)
-            # return  y, -scales.sum(-1, keepdim=True)
-            return y, -logabsdet
-            # return y, mus, scales
-
-
-
-class Sigmoid(nn.Module):
-    def __init__(self):
-        super(Sigmoid, self).__init__()
-
-    def forward(self, inputs, cond_inputs=None, mode='direct'):
-        if mode == 'direct':
-            s = torch.sigmoid
-            return s(inputs), torch.log(s(inputs) * (1 - s(inputs))).sum(
-                -1, keepdim=True)
-        else:
-            return torch.log(inputs /
-                             (1 - inputs)), -torch.log(inputs - inputs**2).sum(
-                                 -1, keepdim=True)
-
-
-class Logit(Sigmoid):
-    def __init__(self):
-        super(Logit, self).__init__()
-
-    def forward(self, inputs, cond_inputs=None, mode='direct'):
-        if mode == 'direct':
-            return super(Logit, self).forward(inputs, 'inverse')
-        else:
-            return super(Logit, self).forward(inputs, 'direct')
-
-
-class BatchNormFlow(nn.Module):
-    """ An implementation of a batch normalization layer from
-    Density estimation using Real NVP
-    (https://arxiv.org/abs/1605.08803).
-    """
-
-    def __init__(self, num_inputs, momentum=0.0, eps=1e-5):
-        super(BatchNormFlow, self).__init__()
-
-        self.log_gamma = nn.Parameter(torch.zeros(num_inputs))
-        self.beta = nn.Parameter(torch.zeros(num_inputs))
-        self.momentum = momentum
-        self.eps = eps
-
-        self.register_buffer('running_mean', torch.zeros(num_inputs))
-        self.register_buffer('running_var', torch.ones(num_inputs))
-
-    def forward(self, inputs, cond_inputs=None, last_vel=None, mode='direct'):
-        if mode == 'direct':
-            if self.training:
-                self.batch_mean = inputs.mean(0)
-                self.batch_var = (
-                    inputs - self.batch_mean).pow(2).mean(0) + self.eps
-
-                self.running_mean.mul_(self.momentum)
-                self.running_var.mul_(self.momentum)
-
-                self.running_mean.add_(self.batch_mean.data *
-                                       (1 - self.momentum))
-                self.running_var.add_(self.batch_var.data *
-                                      (1 - self.momentum))
-
-                mean = self.batch_mean
-                var = self.batch_var
-            else:
-                mean = self.running_mean
-                var = self.running_var
-
-            x_hat = (inputs - mean) / var.sqrt()
-            y = torch.exp(self.log_gamma) * x_hat + self.beta
-            return y, (self.log_gamma - 0.5 * torch.log(var)).sum(
-                -1, keepdim=True)
-        else:
-            if self.training:
-                mean = self.batch_mean
-                var = self.batch_var
-            else:
-                mean = self.running_mean
-                var = self.running_var
-
-            x_hat = (inputs - self.beta) / torch.exp(self.log_gamma)
-
-            y = x_hat * var.sqrt() + mean
-
-            return y, (-self.log_gamma + 0.5 * torch.log(var)).sum(
-                -1, keepdim=True)
-
-
-class ActNorm(nn.Module):
-    """ An implementation of a activation normalization layer
-    from Glow: Generative Flow with Invertible 1x1 Convolutions
-    (https://arxiv.org/abs/1807.03039).
-    """
-
-    def __init__(self, num_inputs):
-        super(ActNorm, self).__init__()
-        self.weight = nn.Parameter(torch.ones(num_inputs))
-        self.bias = nn.Parameter(torch.zeros(num_inputs))
-        self.initialized = False
-
-    def forward(self, inputs, cond_inputs=None,last_vel=None, mode='direct'):
-        if self.initialized == False:
-            self.weight.data.copy_(torch.log(1.0 / (inputs.std(0) + 1e-12)))
-            self.bias.data.copy_(inputs.mean(0))
-            self.initialized = True
-
-        if mode == 'direct':
-            return (
-                inputs - self.bias) * torch.exp(self.weight), self.weight.sum(
-                    -1, keepdim=True).unsqueeze(0).repeat(inputs.size(0), 1)
-        else:
-            return inputs * torch.exp(
-                -self.weight) + self.bias, -self.weight.sum(
-                    -1, keepdim=True).unsqueeze(0).repeat(inputs.size(0), 1)
-
-
-class InvertibleMM(nn.Module):
-    """ An implementation of a invertible matrix multiplication
-    layer from Glow: Generative Flow with Invertible 1x1 Convolutions
-    (https://arxiv.org/abs/1807.03039).
-    """
-
-    def __init__(self, num_inputs):
-        super(InvertibleMM, self).__init__()
-        self.W = nn.Parameter(torch.Tensor(num_inputs, num_inputs))
-        nn.init.orthogonal_(self.W)
-
-    def forward(self, inputs, cond_inputs=None, mode='direct'):
-        if mode == 'direct':
-            return inputs @ self.W, torch.slogdet(
-                self.W)[-1].unsqueeze(0).unsqueeze(0).repeat(
-                    inputs.size(0), 1)
-        else:
-            return inputs @ torch.inverse(self.W), -torch.slogdet(
-                self.W)[-1].unsqueeze(0).unsqueeze(0).repeat(
-                    inputs.size(0), 1)
-
-
-class LUInvertibleMM(nn.Module):
-    """ An implementation of a invertible matrix multiplication
-    layer from Glow: Generative Flow with Invertible 1x1 Convolutions
-    (https://arxiv.org/abs/1807.03039).
-    """
-
-    def __init__(self, num_inputs):
-        super(LUInvertibleMM, self).__init__()
-        self.W = torch.Tensor(num_inputs, num_inputs)
-        nn.init.orthogonal_(self.W)
-        self.L_mask = torch.tril(torch.ones(self.W.size()), -1)
-        self.U_mask = self.L_mask.t().clone()
-
-        P, L, U = sp.linalg.lu(self.W.numpy())
-        self.P = torch.from_numpy(P)
-        self.L = nn.Parameter(torch.from_numpy(L))
-        self.U = nn.Parameter(torch.from_numpy(U))
-
-        S = np.diag(U)
-        sign_S = np.sign(S)
-        log_S = np.log(abs(S))
-        self.sign_S = torch.from_numpy(sign_S)
-        self.log_S = nn.Parameter(torch.from_numpy(log_S))
-
-        self.I = torch.eye(self.L.size(0))
-
-    def forward(self, inputs, cond_inputs=None, mode='direct'):
-        if str(self.L_mask.device) != str(self.L.device):
-            self.L_mask = self.L_mask.to(self.L.device)
-            self.U_mask = self.U_mask.to(self.L.device)
-            self.I = self.I.to(self.L.device)
-            self.P = self.P.to(self.L.device)
-            self.sign_S = self.sign_S.to(self.L.device)
-
-        L = self.L * self.L_mask + self.I
-        U = self.U * self.U_mask + torch.diag(
-            self.sign_S * torch.exp(self.log_S))
-        W = self.P @ L @ U
-
-        if mode == 'direct':
-            return inputs @ W, self.log_S.sum().unsqueeze(0).unsqueeze(
-                0).repeat(inputs.size(0), 1)
-        else:
-            return inputs @ torch.inverse(
-                W), -self.log_S.sum().unsqueeze(0).unsqueeze(0).repeat(
-                    inputs.size(0), 1)
-
-
-class Shuffle(nn.Module):
-    """ An implementation of a shuffling layer from
-    Density estimation using Real NVP
-    (https://arxiv.org/abs/1605.08803).
-    """
-
-    def __init__(self, num_inputs):
-        super(Shuffle, self).__init__()
-        self.register_buffer("perm", torch.randperm(num_inputs))
-        self.register_buffer("inv_perm", torch.argsort(self.perm))
-
-    def forward(self, inputs, cond_inputs=None, mode='direct'):
-        if mode == 'direct':
-            return inputs[:, self.perm], torch.zeros(
-                inputs.size(0), 1, device=inputs.device)
-        else:
-            return inputs[:, self.inv_perm], torch.zeros(
-                inputs.size(0), 1, device=inputs.device)
-
-
-class Reverse(nn.Module):
-    """ An implementation of a reversing layer from
-    Density estimation using Real NVP
-    (https://arxiv.org/abs/1605.08803).
-    """
-
-    def __init__(self, num_inputs):
-        super(Reverse, self).__init__()
-        self.perm = np.array(np.arange(0, num_inputs)[::-1])
-        self.inv_perm = np.argsort(self.perm)
-
-    def forward(self, inputs, cond_inputs=None,last_vel=None, mode='direct'):
-        if mode == 'direct':
-            return inputs[:, self.perm], torch.zeros(
-                inputs.size(0), 1, device=inputs.device)
-        else:
-            return inputs[:, self.inv_perm], torch.zeros(
-                inputs.size(0), 1, device=inputs.device)
-
-
-class CouplingLayer(nn.Module):
-    """ An implementation of a coupling layer
-    from RealNVP (https://arxiv.org/abs/1605.08803).
-    """
-
-    def __init__(self,
-                 num_inputs,
-                 num_hidden,
-                 mask,
-                 num_cond_inputs=None,
-                 s_act='tanh',
-                 t_act='relu'):
-        super(CouplingLayer, self).__init__()
-
-        self.num_inputs = num_inputs
-        self.mask = mask
-
-        activations = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}
-        s_act_func = activations[s_act]
-        t_act_func = activations[t_act]
-
-        if num_cond_inputs is not None:
-            total_inputs = num_inputs + num_cond_inputs
-        else:
-            total_inputs = num_inputs
-            
-        self.scale_net = nn.Sequential(
-            nn.Linear(total_inputs, num_hidden), s_act_func(),
-            nn.Linear(num_hidden, num_hidden), s_act_func(),
-            nn.Linear(num_hidden, num_inputs))
-        self.translate_net = nn.Sequential(
-            nn.Linear(total_inputs, num_hidden), t_act_func(),
-            nn.Linear(num_hidden, num_hidden), t_act_func(),
-            nn.Linear(num_hidden, num_inputs))
-
-        def init(m):
-            if isinstance(m, nn.Linear):
-                m.bias.data.fill_(0)
-                nn.init.orthogonal_(m.weight.data)
-
-    def forward(self, inputs, cond_inputs=None, mode='direct'):
-        mask = self.mask
-        
-        masked_inputs = inputs * mask
-        if cond_inputs is not None:
-            masked_inputs = torch.cat([masked_inputs, cond_inputs], -1)
-        
-        if mode == 'direct':
-            log_s = self.scale_net(masked_inputs) * (1 - mask)
-            t = self.translate_net(masked_inputs) * (1 - mask)
-            s = torch.exp(log_s)
-            return inputs * s + t, log_s.sum(-1, keepdim=True)
-        else:
-            log_s = self.scale_net(masked_inputs) * (1 - mask)
-            t = self.translate_net(masked_inputs) * (1 - mask)
-            s = torch.exp(-log_s)
-            return (inputs - t) * s, -log_s.sum(-1, keepdim=True)
-
-import matplotlib.pyplot as plt
-
-class FlowSequential(nn.Sequential):
-    """ A sequential container for flows.
-    In addition to a forward pass it implements a backward pass and
-    computes log jacobians.
-    """
-
-    def forward(self, inputs, cond_inputs, last_vel, mode='direct', logdets=None):
-        """ Performs a forward or backward pass for flow modules.
-        Args:
-            inputs: a tuple of inputs and logdets
-            mode: to run direct computation or inverse
-        """
-        self.num_inputs = inputs.size(-1)
-
-        if logdets is None:
-            logdets = torch.zeros(inputs.size(0), 1, device=inputs.device)
-
-        assert mode in ['direct', 'inverse']
-        if mode == 'direct':
-            for module in self._modules.values():
-                inputs, logdet = module(inputs, cond_inputs, last_vel, mode)
-                logdets = logdets + logdet
-        else:
-            for module in reversed(self._modules.values()):
-                inputs, logdet = module(inputs, cond_inputs, last_vel, mode)
-                logdets = logdets + logdet
-
-        return inputs, logdets
-
-    def log_probs(self, inputs, cond_inputs, last_vel):
-        u, log_jacob = self(inputs, cond_inputs, last_vel)
-        # testi = (-0.5 * u.pow(2)).sum(-1, keepdim=True)
-        log_probs = (-0.5 * u.pow(2) - 0.5 * math.log(2 * math.pi)).sum(      # Here we assume N(0,1)
-            -1, keepdim=True)
-        return u, (log_probs + log_jacob).sum(-1, keepdim=True)
-
-    def sample(self, cond_inputs, last_vel, num_samples=None, noise=None):
-        if noise is None:
-            noise = torch.Tensor(num_samples, self.num_inputs).normal_()
-        device = next(self.parameters()).device
-        noise = noise.to(device)
-        if cond_inputs is not None:
-            cond_inputs = cond_inputs.to(device)
-        samples = self.forward(noise, cond_inputs, last_vel, mode='inverse')[0]
-        return samples
-
-    def plot_sampling(self, num_samples=8000, noise=None, cond_inputs=None):
-        results = []
-        names = ['Base']
-        device = next(self.parameters()).device
-        inputs = torch.Tensor(num_samples, 2).normal_().to(device) # base distr
-        results.append(inputs.cpu().numpy())
-
-        for module in reversed(self._modules.values()):
-            inputs, _ = module(inputs, cond_inputs, 'inverse')
-            names.append(module.__class__.__name__ )
-            results.append(inputs.detach())
-
-        f, arr = plt.subplots(1, len(results), figsize=(4 * (len(results)), 4))
-        X0 = results[0]
-        for i in range(len(results)):
-            if i > 0:
-                X1 = results[i].cpu().numpy()
-            else:
-                X1 = results[i]
-            idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] < 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='red')
-            idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] < 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='green')
-            idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] > 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='blue')
-            idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] > 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='black')
-            # arr[i].set_xlim([-2, 2])
-            # arr[i].set_ylim([-2, 2])
-            arr[i].set_title(names[i])
-        plt.show()
-
-
-        inverse_results = []
-        names = ['Base']
-        inputs_last = inputs.clone().detach().cpu().numpy()
-        inverse_results.append(inputs_last)
-        for j, module in enumerate(self._modules.values()):
-            inputs, _ = module(inputs, cond_inputs, 'direct')
-            names.append(module.__class__.__name__)
-            inverse_results.append(inputs.detach())
-
-        f, arr = plt.subplots(1, len(results), figsize=(4 * (len(inverse_results)), 4))
-        X0 = results[0]
-        for i in range(len(inverse_results)):
-
-            if i > 0:
-                X1 = inverse_results[i].cpu().numpy()
-            else:
-                X1 = inverse_results[i]
-
-            idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] < 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='red')
-            idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] < 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='green')
-            idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] > 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='blue')
-            idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] > 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='black')
-            # arr[i].set_xlim([-2, 2])
-            # arr[i].set_ylim([-2, 2])
-            arr[i].set_title(names[i])
-        plt.show()
-
-    def plot_target_to_base(self, target_dis, cond_inputs=None):
-
-        device = next(self.parameters()).device
-        inverse_results = []
-        names = ['Target']
-        inverse_results.append(target_dis)
-        inputs = torch.Tensor(target_dis).to(device)
-        for j, module in enumerate(self._modules.values()):
-            inputs, _ = module(inputs, cond_inputs, 'direct')
-            names.append(module.__class__.__name__)
-            inverse_results.append(inputs.detach())
-
-        f, arr = plt.subplots(1, len(inverse_results), figsize=(4 * (len(inverse_results)), 4))
-        X0 = inverse_results[-1].clone().cpu().numpy()
-        for i in range(len(inverse_results)):
-
-            if i > 0:
-                X1 = inverse_results[i].cpu().numpy()
-            else:
-                X1 = inverse_results[i]
-
-            idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] < 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='red')
-            idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] < 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='green')
-            idx = np.logical_and(X0[:, 0] < 0, X0[:, 1] > 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='blue')
-            idx = np.logical_and(X0[:, 0] > 0, X0[:, 1] > 0)
-            arr[i].scatter(X1[idx, 0], X1[idx, 1], s=10, color='black')
-            # arr[i].set_xlim([-2, 2])
-            # arr[i].set_ylim([-2, 2])
-            arr[i].set_title(names[i])
-        plt.show()
\ No newline at end of file
diff --git a/train_GoalFlow.py b/train_GoalFlow.py
deleted file mode 100644
index a1c93cf..0000000
--- a/train_GoalFlow.py
+++ /dev/null
@@ -1,223 +0,0 @@
-
-import logging
-import sys
-import numpy as np
-from collections import defaultdict
-import torch
-import torch.optim as optim
-import copy
-import wandb
-from data.loader import data_loader
-from utils.utils import get_dset_path
-from config import *
-from models.GoalFLow import GoalGenerator
-from eval_GoalFlow import evaluate
-
-torch.backends.cudnn.benchmark = False
-torch.backends.cudnn.enabled = True
-
-import warnings
-warnings.filterwarnings("ignore")
-
-FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
-logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)
-logger = logging.getLogger(__name__)
-
-
-
-def get_dtypes(args):
-    long_dtype = torch.LongTensor
-    float_dtype = torch.FloatTensor
-    if args.use_gpu == 1:
-        long_dtype = torch.cuda.LongTensor
-        float_dtype = torch.cuda.FloatTensor
-    return long_dtype, float_dtype
-
-
-class Preparation:
-    def __init__(self, data=None):
-        self.config = Config()
-        if data:
-            self.config.dataset_name = data
-            group_name = self.config.experiment_name
-            self.config.experiment_name =  self.config.experiment_name + '-' + data
-            path = self.config.DIR + self.config.experiment_name
-            if not os.path.exists(path):
-                os.makedirs(path)
-            self.config.model_path = path
-            self.config.output_dir = self.config.model_path
-            self.config.checkpoint_start_from = self.config.model_path + '/checkpoint_with_model.pt'
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True, group=group_name)
-        else:
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True)
-            # print('no_wandb')
-        seed = self.config.seed
-        torch.manual_seed(seed)
-        np.random.seed(seed)
-
-        os.environ['CUDA_VISIBLE_DEVICES'] = self.config.gpu_num
-        train_path = get_dset_path(self.config.dataset_name, 'train')
-        val_path = get_dset_path(self.config.dataset_name, 'test')
-        long_dtype, float_dtype = get_dtypes(self.config)
-        logger.info("Initializing train dataset")
-        self.train_dset, self.train_loader = data_loader(self.config, train_path, augment=self.config.augment)
-        logger.info("Initializing val dataset")
-        _, self.val_loader = data_loader(self.config, val_path)
-        self.iterations_per_epoch = len(self.train_loader)
-        logger.info(
-            'There are {} iterations per epoch'.format(self.iterations_per_epoch)
-        )
-
-        self.generator = GoalGenerator(self.config)
-        self.generator.type(float_dtype).train()
-        logger.info('Here is the generator:')
-        logger.info(self.generator)
-
-        # Log model
-        # wandb.watch(self.generator, log='all')
-        if self.config.adam:
-            print('Learning with ADAM!')
-            # betas_d = (0.5, 0.9999)
-            self.optimizer = optim.Adam(self.generator.parameters(), lr=self.config.g_learning_rate)
-        else:
-            print('Learning with RMSprop!')
-            self.optimizer = optim.SGD(self.generator.parameters(), lr=0.001, momentum=0.9)
-        restore_path = None
-        if self.config.restore_from_checkpoint == True:
-            restore_path = os.path.join(self.config.output_dir,
-                                        '%s_with_model.pt' % self.config.checkpoint_name)
-            if os.path.isfile(restore_path):
-                logger.info('Restoring from checkpoint {}'.format(restore_path))
-                self.checkpoint = torch.load(restore_path)
-                self.generator.load_state_dict(self.checkpoint['state'])
-                self.optimizer.load_state_dict(self.checkpoint['optim_state'])
-                self.t = self.checkpoint['counters']['t']
-                self.epoch = self.checkpoint['counters']['epoch']
-                self.checkpoint['restore_ts'].append(self.t)
-        else:
-            # Starting from scratch, so initialize checkpoint data structure
-            self.t, self.epoch = 0, 0
-            self.checkpoint = {
-                'args': self.config.__dict__,
-                'losses': defaultdict(list),
-                'losses_ts': [],
-                'metrics_val': defaultdict(list),
-                'metrics_train': defaultdict(list),
-                'sample_ts': [],
-                'restore_ts': [],
-                'counters': {
-                    't': None,
-                    'epoch': None,
-                },
-                'state': None,
-                'optim_state': None,
-                'best_state': None,
-            }
-
-    def check_accuracy(self, loader, generator):  # TODO Change this!
-        metrics = {}
-        generator.eval()  # will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
-
-        fde = evaluate(self.config, loader, generator)
-        metrics['fde'] = fde
-        generator.train()
-        wandb.log({"fde": metrics['fde']})
-        return metrics
-
-    def print_stats(self):
-        dictlist = 'Epoch = {}, t = {} '.format(self.epoch, self.t) + '[D] '
-        dictlist += ' [G] '
-
-        for k, v in sorted(self.losses.items()):
-            self.checkpoint['losses'][k].append(v)
-            dictlist += ' ' + '{}: {:.6f}'.format(k, v)
-
-        logger.info(dictlist)
-
-        self.checkpoint['losses_ts'].append(self.t)
-
-    def save_model(self):
-        self.checkpoint['counters']['t'] = self.t
-        self.checkpoint['counters']['epoch'] = self.epoch
-        self.checkpoint['sample_ts'].append(self.t)
-
-        # Check stats on the validation set
-        logger.info('Checking stats on val ...')
-        metrics_val = self.check_accuracy(self.val_loader, self.generator)
-    #    self.scheduler.step(metrics_val['ade'])
-        for k, v in sorted(metrics_val.items()):
-            logger.info('  [val] {}: {:.3f}'.format(k, v))
-            self.checkpoint['metrics_val'][k].append(v)
-
-        min_fde = min(self.checkpoint['metrics_val']['fde'])
-        if metrics_val['fde'] == min_fde:
-            logger.info('New low for fde error')
-            self.checkpoint['best_t'] = self.t
-            self.checkpoint['best_state'] = copy.deepcopy(self.generator.state_dict())
-
-        # Save another checkpoint with model weights and
-        # optimizer state
-        self.checkpoint['state'] = self.generator.state_dict()
-        self.checkpoint['optim_state'] = self.optimizer.state_dict()
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_with_model.pt' % self.config.checkpoint_name
-        )
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        torch.save(self.checkpoint, checkpoint_path)
-        torch.save(self.checkpoint, os.path.join(wandb.run.dir, 'model.pt'))
-        logger.info('Done.')
-
-        # Save a checkpoint with no model weights by making a shallow
-        # copy of the checkpoint excluding some items
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_no_model.pt' % self.config.checkpoint_name)
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        key_blacklist = [
-            'state', 'best_state', 'optim_state'
-        ]
-        small_checkpoint = {}
-        for k, v in self.checkpoint.items():
-            if k not in key_blacklist:
-                small_checkpoint[k] = v
-        torch.save(small_checkpoint, checkpoint_path)
-        logger.info('Done.')
-
-    def model_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask,\
-        loss_mask, seq_start_end, nei_num_index, nei_num, = batch
-        l2_loss_rel = []
-        losses = {}
-        for param in self.generator.parameters(): param.grad = None
-
-        # loss_mask = loss_mask[-self.config.pred_len :]
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-
-
-        pred_traj_fake_rel, nll = self.generator(model_input, obs_traj, pred_traj_gt,
-                                                                 seq_start_end, nei_num_index, nei_num, mode='train')
-
-        loss = nll
-        loss.backward()
-        losses['nll'] = nll.item()
-        if self.config.clip_grad:
-            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=2.0, norm_type=2)
-        self.optimizer.step()
-        wandb.log({"nll": nll.item()})
-        return losses
-
-    def train(self):
-        self.t_step = 0
-        while self.epoch < self.config.num_epochs:
-            self.t_step = 0
-            logger.info('Starting epoch {}'.format(self.epoch))
-            for batch in self.train_loader:
-                batch = [tensor.cuda() for tensor in batch]
-                self.losses = self.model_step(batch)
-                self.t_step += 1
-            if self.epoch % self.config.check_after_num_epochs == 0:
-                self.save_model()
-            self.epoch += 1
-
-if __name__ == '__main__':
-    prep = Preparation()
-    prep.train()
\ No newline at end of file
diff --git a/train_GoalFlow_divSampling.py b/train_GoalFlow_divSampling.py
deleted file mode 100644
index 68aed25..0000000
--- a/train_GoalFlow_divSampling.py
+++ /dev/null
@@ -1,242 +0,0 @@
-
-import logging
-import sys
-import numpy as np
-from collections import defaultdict
-import torch
-import torch.optim as optim
-import copy
-import wandb
-from data.loader import data_loader
-from utils.utils import get_dset_path
-from config import *
-from models.GoalFLow import GoalGenerator
-from models.Sampler import Sampler
-from eval_GoalFlow_divSampling import evaluate
-
-torch.backends.cudnn.benchmark = False
-torch.backends.cudnn.enabled = True
-
-import warnings
-warnings.filterwarnings("ignore")
-
-FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
-logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)
-logger = logging.getLogger(__name__)
-
-
-
-def get_dtypes(args):
-    long_dtype = torch.LongTensor
-    float_dtype = torch.FloatTensor
-    if args.use_gpu == 1:
-        long_dtype = torch.cuda.LongTensor
-        float_dtype = torch.cuda.FloatTensor
-    return long_dtype, float_dtype
-
-
-class Preparation:
-    def __init__(self, data=None):
-        self.config = Config()
-        if data:
-            self.config.dataset_name = data
-            group_name = self.config.experiment_name
-            self.config.experiment_name =  self.config.experiment_name + '-' + data
-            path = self.config.DIR + self.config.experiment_name
-            if not os.path.exists(path):
-                os.makedirs(path)
-            self.config.model_path = path
-            self.config.output_dir = self.config.model_path
-            self.config.checkpoint_start_from = self.config.model_path + '/checkpoint_with_model.pt'
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True, group=group_name)
-        else:
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True)
-            # print('no_wandb')
-        seed = self.config.seed
-        torch.manual_seed(seed)
-        np.random.seed(seed)
-
-        os.environ['CUDA_VISIBLE_DEVICES'] = self.config.gpu_num
-        train_path = get_dset_path(self.config.dataset_name, 'train')
-        val_path = get_dset_path(self.config.dataset_name, 'test')
-        long_dtype, float_dtype = get_dtypes(self.config)
-        logger.info("Initializing train dataset")
-        self.train_dset, self.train_loader = data_loader(self.config, train_path, augment=self.config.augment)
-        logger.info("Initializing val dataset")
-        _, self.val_loader = data_loader(self.config, val_path)
-        self.iterations_per_epoch = len(self.train_loader)
-        logger.info(
-            'There are {} iterations per epoch'.format(self.iterations_per_epoch)
-        )
-        checkpoint_glow_path = self.config.GFlow_checkpoint_start_from \
-                                  + self.config.dataset_name + '/checkpoint_with_model.pt'
-        checkpoint_glow = torch.load(checkpoint_glow_path)
-        self.generator = GoalGenerator(self.config)
-        self.generator.load_state_dict(checkpoint_glow["best_state"])
-        self.generator.type(float_dtype).train()
-        logger.info('Here is the Goal Network loaded from: ' + checkpoint_glow_path)
-        logger.info(self.generator)
-
-        self.sampler = Sampler(self.config)
-        self.sampler.type(float_dtype).train()
-        logger.info('Here is the Sampler:')
-        logger.info(self.sampler)
-
-
-        # Log model
-        # wandb.watch(self.generator, log='all')
-
-        restore_path = None
-
-        if self.config.restore_from_checkpoint == True:
-            restore_path = os.path.join(self.config.output_dir,
-                                        '%s_with_model.pt' % self.config.checkpoint_name)
-            if os.path.isfile(restore_path):
-                logger.info('Restoring from checkpoint {}'.format(restore_path))
-                self.checkpoint = torch.load(restore_path)
-                self.sampler.load_state_dict(self.checkpoint['best_state'])
-                # self.optimizer.load_state_dict(self.checkpoint['optim_state'])
-                # self.t = self.checkpoint['counters']['t']
-                # self.t, self.epoch = 0, 0
-                # self.epoch = self.checkpoint['counters']['epoch']
-                # self.checkpoint['restore_ts'].append(self.t)
-        else:
-            # Starting from scratch, so initialize checkpoint data structure
-            self.t, self.epoch = 0, 0
-            self.checkpoint = {
-                'args': self.config.__dict__,
-                'losses': defaultdict(list),
-                'losses_ts': [],
-                'metrics_val': defaultdict(list),
-                'metrics_train': defaultdict(list),
-                'sample_ts': [],
-                'restore_ts': [],
-                'counters': {
-                    't': None,
-                    'epoch': None,
-                },
-                'state': None,
-                'optim_state': None,
-                'best_state': None,
-            }
-        if self.config.adam:
-            print('Learning with SGD!')
-            # betas_d = (0.5, 0.9999)
-            self.optimizer = optim.SGD(self.sampler.parameters(), lr=self.config.g_learning_rate)
-            # self.optimizer = optim.Adam(self.sampler.parameters(), lr=self.config.g_learning_rate)
-        else:
-            print('Learning with RMSprop!')
-            self.optimizer = optim.SGD(self.sampler.parameters(), lr=0.001, momentum=0.9)
-
-    def check_accuracy(self, loader, generator, sampler):  # TODO Change this!
-        metrics = {}
-        generator.eval()  # will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
-        sampler.eval()
-        fde = evaluate(self.config, loader, generator, sampler)
-        metrics['fde'] = fde
-        generator.train()
-        wandb.log({"fde": metrics['fde']})
-        return metrics
-
-    def print_stats(self):
-        dictlist = 'Epoch = {}, t = {} '.format(self.epoch, self.t) + '[D] '
-        dictlist += ' [G] '
-
-        for k, v in sorted(self.losses.items()):
-            self.checkpoint['losses'][k].append(v)
-            dictlist += ' ' + '{}: {:.6f}'.format(k, v)
-
-        logger.info(dictlist)
-
-        self.checkpoint['losses_ts'].append(self.t)
-
-    def save_model(self):
-        self.checkpoint['counters']['t'] = self.t
-        self.checkpoint['counters']['epoch'] = self.epoch
-        self.checkpoint['sample_ts'].append(self.t)
-
-        # Check stats on the validation set
-        logger.info('Checking stats on val ...')
-        metrics_val = self.check_accuracy(self.val_loader, self.generator, self.sampler)
-    #    self.scheduler.step(metrics_val['ade'])
-        for k, v in sorted(metrics_val.items()):
-            logger.info('  [val] {}: {:.3f}'.format(k, v))
-            self.checkpoint['metrics_val'][k].append(v)
-
-        min_fde = min(self.checkpoint['metrics_val']['fde'])
-        if metrics_val['fde'] == min_fde:
-            logger.info('New low for fde error')
-            self.checkpoint['best_t'] = self.t
-            self.checkpoint['best_state'] = copy.deepcopy(self.generator.state_dict())
-            self.checkpoint['best_state_sampler'] = copy.deepcopy(self.sampler.state_dict())
-
-        # Save another checkpoint with model weights and
-        # optimizer state
-        self.checkpoint['state'] = self.generator.state_dict()
-        self.checkpoint['optim_state'] = self.optimizer.state_dict()
-        self.checkpoint['state_sampler'] = self.sampler.state_dict()
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_with_model.pt' % self.config.checkpoint_name
-        )
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        torch.save(self.checkpoint, checkpoint_path)
-        torch.save(self.checkpoint, os.path.join(wandb.run.dir, 'model.pt'))
-        logger.info('Done.')
-
-        # Save a checkpoint with no model weights by making a shallow
-        # copy of the checkpoint excluding some items
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_no_model.pt' % self.config.checkpoint_name)
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        key_blacklist = [
-            'state', 'best_state', 'optim_state'
-        ]
-        small_checkpoint = {}
-        for k, v in self.checkpoint.items():
-            if k not in key_blacklist:
-                small_checkpoint[k] = v
-        torch.save(small_checkpoint, checkpoint_path)
-        logger.info('Done.')
-
-    def model_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask,\
-        loss_mask, seq_start_end, nei_num_index, nei_num, = batch
-        l2_loss_rel = []
-        losses = {}
-        for param in self.generator.parameters(): param.grad = None
-
-        # loss_mask = loss_mask[-self.config.pred_len :]
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-
-
-        pred_traj_fake_rel, div_loss = self.generator(model_input, obs_traj, pred_traj_gt,
-                                                      seq_start_end, nei_num_index, nei_num,
-                                                      mode='sampling', sampling_module=self.sampler)
-
-        loss = div_loss
-        loss.backward()
-        losses['div_loss'] = div_loss.item()
-        if self.config.clip_grad:
-            torch.nn.utils.clip_grad_norm_(self.sampler.parameters(), max_norm=2.0, norm_type=2)
-        self.optimizer.step()
-        wandb.log({"div_loss": div_loss.item()})
-        return losses
-
-    def train(self):
-        self.t_step = 0
-        while self.epoch < self.config.num_epochs:
-            self.t_step = 0
-            logger.info('Starting epoch {}'.format(self.epoch))
-            for batch in self.train_loader:
-                batch = [tensor.cuda() for tensor in batch]
-                self.losses = self.model_step(batch)
-                self.t_step += 1
-            if self.epoch % self.config.check_after_num_epochs == 0:
-                self.save_model()
-            self.epoch += 1
-            # if self.epoch == 40:
-            #     self.config.check_after_num_epochs = 1
-
-if __name__ == '__main__':
-    prep = Preparation()
-    prep.train()
\ No newline at end of file
diff --git a/train_SI.py b/train_SI.py
deleted file mode 100644
index 519d73b..0000000
--- a/train_SI.py
+++ /dev/null
@@ -1,347 +0,0 @@
-
-import logging
-import sys
-import numpy as np
-from collections import defaultdict
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import copy
-import wandb
-from matplotlib import pyplot as plt
-from matplotlib.lines import Line2D
-
-from data.loader import data_loader
-from utils.losses import KL_gaussians
-from utils.utils import get_dset_path,relative_to_abs
-from config import *
-from models.Sim2Goal import TrajectoryGenerator
-from models.GoalFLow import GoalGenerator
-from models.Robot import Robot
-
-from models.Sampler import Sampler
-from eval_SI import evaluate_social
-
-torch.backends.cudnn.benchmark = False
-torch.backends.cudnn.enabled = True
-
-import warnings
-warnings.filterwarnings("ignore")
-
-FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
-logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)
-logger = logging.getLogger(__name__)
-
-
-
-def get_dtypes(args):
-    long_dtype = torch.LongTensor
-    float_dtype = torch.FloatTensor
-    if args.use_gpu == 1:
-        long_dtype = torch.cuda.LongTensor
-        float_dtype = torch.cuda.FloatTensor
-    return long_dtype, float_dtype
-
-
-class Preparation:
-    def __init__(self, data=None):
-        self.config = Config()
-        if data:
-            self.config.dataset_name = data
-            group_name = self.config.experiment_name
-            self.config.experiment_name =  self.config.experiment_name + '-' + data
-            path = self.config.DIR + self.config.experiment_name
-            if not os.path.exists(path):
-                os.makedirs(path)
-            self.config.model_path = path
-            self.config.output_dir = self.config.model_path
-            self.config.checkpoint_start_from = self.config.model_path + '/checkpoint_with_model.pt'
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True, group=group_name)
-        else:
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True)
-            print('no_wandb')
-        seed = self.config.seed
-        torch.manual_seed(seed)
-        np.random.seed(seed)
-
-        os.environ['CUDA_VISIBLE_DEVICES'] = self.config.gpu_num
-        train_path = get_dset_path(self.config.dataset_name, 'train')
-        val_path = get_dset_path(self.config.dataset_name, 'val')
-        # test_path = get_dset_path(self.config.dataset_name, 'test')
-        long_dtype, float_dtype = get_dtypes(self.config)
-        logger.info("Initializing train dataset")
-        self.train_dset, self.train_loader = data_loader(self.config, train_path, augment=self.config.augment)
-        logger.info("Initializing val dataset")
-        _, self.val_loader = data_loader(self.config, val_path)
-
-        self.iterations_per_epoch = len(self.train_loader)
-        logger.info(
-            'There are {} iterations per epoch'.format(self.iterations_per_epoch)
-        )
-
-        self.robot = Robot()
-        self.robot.type(float_dtype).train()
-        logger.info('Here is the generator:')
-        logger.info(self.robot)
-        dataset_name = 'lcas'
-        checkpoint_student_path = self.config.student_checkpoint_start_from \
-                                 + dataset_name + '/checkpoint_with_model.pt'
-        # checkpoint_student_path = self.config.student_checkpoint_start_from + '/checkpoint_with_model.pt'
-        student_sampler = torch.load(checkpoint_student_path)
-        self.Student = TrajectoryGenerator(self.config)
-        self.Student.load_state_dict(student_sampler["best_state"])
-        self.Student.cuda().eval()
-
-
-        checkpoint_sampler_path = self.config.sampler_checkpoint_start_from \
-                                  + dataset_name + '/checkpoint_with_model.pt'
-        checkpoint_sampler = torch.load(checkpoint_sampler_path)
-        self.sample_generator = GoalGenerator(self.config)
-        self.sample_generator.load_state_dict(checkpoint_sampler["best_state"])
-        self.sample_generator.cuda().eval()
-
-        self.sampler = Sampler(self.config)
-        self.sampler.load_state_dict(checkpoint_sampler["best_state_sampler"])
-        self.sampler.cuda().eval()
-        logger.info('Sampler loaded from:' + str(checkpoint_sampler['args']))
-
-
-
-        # Log model
-        # wandb.watch(self.generator, log='all')
-
-
-        if self.config.adam:
-            print('Learning with ADAM!')
-            # betas_d = (0.5, 0.9999)
-            self.optimizer = optim.Adam(self.robot.parameters(), lr=self.config.g_learning_rate)
-        else:
-            print('Learning with SGD!')
-            self.optimizer = optim.SGD(self.robot.parameters(), lr=self.config.g_learning_rate, momentum=0.9)
-        restore_path = ''
-        if self.config.checkpoint_start_from:
-            restore_path = self.config.checkpoint_start_from
-        elif self.config.restore_from_checkpoint == True:
-            restore_path = os.path.join(self.config.output_dir,
-                                        '%s_with_model.pt' % self.config.checkpoint_name)
-
-        if os.path.isfile(restore_path):
-            logger.info('Restoring from checkpoint {}'.format(restore_path))
-            self.checkpoint = torch.load(restore_path)
-            self.robot.load_state_dict(self.checkpoint['state'])
-            self.optimizer.load_state_dict(self.checkpoint['optim_state'])
-            self.t = self.checkpoint['counters']['t']
-            self.epoch = self.checkpoint['counters']['epoch']
-            self.checkpoint['restore_ts'].append(self.t)
-        else:
-            # Starting from scratch, so initialize checkpoint data structure
-            self.t, self.epoch = 0, 0
-            self.checkpoint = {
-                'args': self.config.__dict__,
-                'losses': defaultdict(list),
-                'losses_ts': [],
-                'metrics_val': defaultdict(list),
-                'metrics_train': defaultdict(list),
-                'sample_ts': [],
-                'restore_ts': [],
-                'counters': {
-                    't': None,
-                    'epoch': None,
-                },
-                'state': None,
-                'optim_state': None,
-                'best_state': None,
-            }
-
-    def check_accuracy(self, loader, Student, Robot):  # TODO Change this!
-        metrics = {}
-        Student.eval()  # will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
-
-        ADE_min_mean, ADE_max_mean, ADE_avg_scene, MPE_vel_min_mean, \
-        MPE_vel_max_mean, MPE_vel_avg_scene, Mean_mutual_info, Mean_FDE, ACT, MRE = evaluate_social(self.config, loader, Student, Robot)
-        metrics['ADE_min_mean'] = ADE_min_mean
-        metrics['ADE_max_mean'] = ADE_max_mean
-        metrics['ADE_avg_scene'] = ADE_avg_scene
-        metrics['MPE_vel_min_mean'] = MPE_vel_min_mean
-        metrics['MPE_vel_max_mean'] = MPE_vel_max_mean
-        metrics['MPE_vel_avg_scene'] = MPE_vel_avg_scene
-        metrics['Mean_mutual_info'] = Mean_mutual_info
-        metrics['Mean_FDE'] = Mean_FDE
-        metrics['ACT'] = ACT
-        metrics['MRE'] = MRE
-        # metrics['MPE_vel_min_mean'] = MPE_vel_min_mean
-        # mean = (act+ ade)/2
-        # metrics['mean'] = mean
-        # Student.train()
-        wandb.log({"ADE_min_mean":  metrics['ADE_min_mean'], "ADE_max_mean": metrics['ADE_max_mean'],
-                   "ADE_avg_scene": metrics['ADE_avg_scene'], "MPE_vel_min_mean": metrics['MPE_vel_min_mean'],
-                   "MPE_vel_max_mean": metrics['MPE_vel_max_mean'], "MPE_vel_avg_scene": metrics['MPE_vel_avg_scene'],
-                   "Mean_mutual_info": metrics['Mean_mutual_info'], "Mean_FDE": metrics['Mean_FDE']
-                      , "Mean_ACT": metrics['ACT'], "MRE": metrics['MRE']})
-        return metrics
-
-    def print_stats(self):
-        dictlist = 'Epoch = {}, t = {} '.format(self.epoch, self.t) + '[D] '
-        dictlist += ' [G] '
-
-        for k, v in sorted(self.losses.items()):
-            self.checkpoint['losses'][k].append(v)
-            dictlist += ' ' + '{}: {:.6f}'.format(k, v)
-
-        logger.info(dictlist)
-
-        self.checkpoint['losses_ts'].append(self.t)
-
-    def save_model(self):
-        self.checkpoint['counters']['t'] = self.t
-        self.checkpoint['counters']['epoch'] = self.epoch
-        self.checkpoint['sample_ts'].append(self.t)
-
-        # Check stats on the validation set
-        logger.info('Checking stats on val ...')
-        metrics_val = self.check_accuracy(self.val_loader, self.Student, self.robot)
-        # metrics_va_gt = self.check_accuracy_gt(self.val_loader, self.Student, self.robot)
-        # metrics_val.update(metrics_va_gt)
-    #    self.scheduler.step(metrics_val['ade'])
-        for k, v in sorted(metrics_val.items()):
-            logger.info('  [val] {}: {:.3f}'.format(k, v))
-            self.checkpoint['metrics_val'][k].append(v)
-
-        min_mean = min(self.checkpoint['metrics_val']['Mean_mutual_info'])
-        if metrics_val['Mean_mutual_info'] == min_mean:
-            logger.info('New low for mean error')
-            self.checkpoint['best_t'] = self.t
-            self.checkpoint['best_state'] = copy.deepcopy(self.robot.state_dict())
-
-        # Save another checkpoint with model weights and
-        # optimizer state
-        self.checkpoint['state'] = self.robot.state_dict()
-        self.checkpoint['optim_state'] = self.optimizer.state_dict()
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_with_model.pt' % self.config.checkpoint_name
-        )
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        torch.save(self.checkpoint, checkpoint_path)
-        torch.save(self.checkpoint, os.path.join(wandb.run.dir, 'model.pt'))
-        logger.info('Done.')
-
-        # Save a checkpoint with no model weights by making a shallow
-        # copy of the checkpoint excluding some items
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_no_model.pt' % self.config.checkpoint_name)
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        key_blacklist = [
-            'state', 'best_state', 'optim_state'
-        ]
-        small_checkpoint = {}
-        for k, v in self.checkpoint.items():
-            if k not in key_blacklist:
-                small_checkpoint[k] = v
-        torch.save(small_checkpoint, checkpoint_path)
-        logger.info('Done.')
-    # for debugging
-    def plot_grad_flow(self, named_parameters):
-        '''Plots the gradients flowing through different layers in the net during training.
-        Can be used for checking for possible gradient vanishing / exploding problems.
-
-        Usage: Plug this function in Trainer class after loss.backwards() as
-        "plot_grad_flow(self.model.named_parameters())" to visualize the gradient flow'''
-        ave_grads = []
-        max_grads = []
-        layers = []
-        for n, p in named_parameters:
-            if (p.requires_grad) and ("bias" not in n):
-                layers.append(n)
-                ave_grads.append(p.grad.abs().mean())
-                max_grads.append(p.grad.abs().max())
-        plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color="c")
-        plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color="b")
-        plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color="k")
-        plt.xticks(range(0, len(ave_grads), 1), layers, rotation="vertical")
-        plt.xlim(left=0, right=len(ave_grads))
-        plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions
-        plt.xlabel("Layers")
-        plt.ylabel("average gradient")
-        plt.title("Gradient flow")
-        plt.grid(True)
-        plt.legend([Line2D([0], [0], color="c", lw=4),
-                    Line2D([0], [0], color="b", lw=4),
-                    Line2D([0], [0], color="k", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])
-        plt.show()
-
-    def model_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask,\
-        loss_mask, seq_start_end, nei_num_index, nei_num, = batch
-        robots_ids = seq_start_end[:,0]
-        losses = {}
-        MSE_loss = nn.MSELoss()
-        for param in self.robot.parameters(): param.grad = None
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-
-        sgoals = pred_traj_gt[-1]
-        # current_pos = obs_traj[-1]
-        # dist_mat_pred = torch.cdist(sgoals, current_pos, p=2.0, compute_mode='donot_use_mm_for_euclid_dist')
-        # for _, (start, end) in enumerate(seq_start_end):
-        #     start = start.item()
-        #     end = end.item()
-        #     dist_mat_pred_scene = dist_mat_pred[start:end, start]
-        #     dist_mat_pred_scene[0] = np.inf
-        #     min_index = torch.argmin(dist_mat_pred_scene)
-        #     sgoals[start] = sgoals[min_index + start]
-        # sgoals[robots_ids] = sgoals[seq_start_end[:, 1] - 1]
-        z = torch.randn_like(pred_traj_gt)
-        pred_traj_fake_rel_c, mu_c, logscale_c, nll_robot,  pred_traj_fake_rel_sampled_c = self.Student(model_input, obs_traj, pred_traj_gt,
-                                                                 seq_start_end, nei_num_index,nei_num, mode='robot_train',
-                                                                 sample_goal=sgoals,robot_net=self.robot,
-                                                                 robotID=robots_ids, noise=z, detached=False)
-        pred_traj_fake_abs = relative_to_abs(pred_traj_fake_rel_c, obs_traj[-1])
-        pred_traj_fake_abs_sampled = relative_to_abs(pred_traj_fake_rel_sampled_c, obs_traj[-1])
-        mu_c_offset = torch.cat([obs_traj[-1].unsqueeze(dim=0), pred_traj_fake_abs], dim=0)
-        mu_c = mu_c + mu_c_offset[0:-1]
-
-        end_pos = pred_traj_fake_abs_sampled[-1]
-        goal_loss = MSE_loss(end_pos,sgoals)
-        ids_no_robot = torch.ones([seq_start_end[-1, 1]], dtype=torch.bool).cuda()
-        ids_no_robot[robots_ids] = False
-
-        nei_num_index_tmp = nei_num_index[:, ids_no_robot]
-        nei_num_index_tmp = nei_num_index_tmp[:, :, ids_no_robot]
-        model_input_tmp = model_input[:, ids_no_robot]
-        obs_traj_tmp = obs_traj[:, ids_no_robot]
-        pred_traj_gt_tmp = pred_traj_gt[:, ids_no_robot]
-        sgoals_tmp = sgoals[ids_no_robot]
-        unconditioned_vel, mu_unc, logscale_unc = self.Student(model_input_tmp, obs_traj_tmp, pred_traj_gt_tmp,
-                                                          seq_start_end, nei_num_index_tmp, nei_num,
-                                                          mode='test', sample_goal=sgoals_tmp,
-                                                          noise=z[:, ids_no_robot], detached=False)
-
-        unconditioned_pos = relative_to_abs(unconditioned_vel, obs_traj_tmp[-1])
-        mu_offset = torch.cat([obs_traj_tmp[-1].unsqueeze(dim=0), unconditioned_pos], dim=0)
-        mu_unc = mu_unc + mu_offset[0:-1]
-
-        sum_timestep_KL = KL_gaussians(mu_c[:, ids_no_robot], logscale_c[ :, ids_no_robot],
-                                       mu_unc, logscale_unc, sum_dim=0).sum()
-
-        loss = self.config.g_w * goal_loss + self.config.si_w * sum_timestep_KL + self.config.nll_w * nll_robot
-
-        loss.backward()
-        # torch.nn.utils.clip_grad_norm_(self.robot.parameters(), max_norm=3.0, norm_type=2)
-        self.optimizer.step()
-        return losses
-
-    def train(self):
-        self.t_step = 0
-        while self.epoch < self.config.num_epochs:
-            self.t_step = 0
-            logger.info('Starting epoch {}'.format(self.epoch))
-            for j, batch in enumerate(self.train_loader):
-                batch = [tensor.cuda() for tensor in batch]
-                self.losses = self.model_step(batch)
-                self.t_step += 1
-            if self.epoch % self.config.check_after_num_epochs == 0:
-                self.save_model()
-            self.epoch += 1
-
-if __name__ == '__main__':
-    prep = Preparation()
-    prep.train()
\ No newline at end of file
diff --git a/train_Sim2Goal.py b/train_Sim2Goal.py
deleted file mode 100644
index dac42ea..0000000
--- a/train_Sim2Goal.py
+++ /dev/null
@@ -1,261 +0,0 @@
-
-import logging
-import sys
-import numpy as np
-from collections import defaultdict
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import copy
-import wandb
-from data.loader import data_loader
-from utils.losses import coll_smoothed_loss
-from utils.utils import get_dset_path
-from config import *
-
-from models.Sim2Goal import TrajectoryGenerator
-from models.GoalFLow import GoalGenerator
-
-from models.Sampler import Sampler
-from eval_Sim2Goal import evaluate
-from utils.utils import relative_to_abs
-
-torch.backends.cudnn.benchmark = False
-torch.backends.cudnn.enabled = True
-
-import warnings
-warnings.filterwarnings("ignore")
-
-FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
-logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)
-logger = logging.getLogger(__name__)
-
-
-
-def get_dtypes(args):
-    long_dtype = torch.LongTensor
-    float_dtype = torch.FloatTensor
-    if args.use_gpu == 1:
-        long_dtype = torch.cuda.LongTensor
-        float_dtype = torch.cuda.FloatTensor
-    return long_dtype, float_dtype
-
-
-class Preparation:
-    def __init__(self, data=None):
-        self.config = Config()
-        if data:
-            self.config.dataset_name = data
-            group_name = self.config.experiment_name
-            self.config.experiment_name =  self.config.experiment_name + '-' + data
-            path = self.config.DIR + self.config.experiment_name
-            if not os.path.exists(path):
-                os.makedirs(path)
-            self.config.model_path = path
-            self.config.output_dir = self.config.model_path
-            self.config.checkpoint_start_from = self.config.model_path + '/checkpoint_with_model.pt'
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True, group=group_name)
-        else:
-            wandb.init(project="NF-Traj", name=self.config.experiment_name, reinit=True)
-            # print('no_wandb')
-        seed = self.config.seed
-        torch.manual_seed(seed)
-        np.random.seed(seed)
-
-        os.environ['CUDA_VISIBLE_DEVICES'] = self.config.gpu_num
-        train_path = get_dset_path(self.config.dataset_name, 'train')
-        val_path = get_dset_path(self.config.dataset_name, 'test')
-        long_dtype, float_dtype = get_dtypes(self.config)
-        logger.info("Initializing train dataset")
-        self.train_dset, self.train_loader = data_loader(self.config, train_path, augment=self.config.augment)
-        logger.info("Initializing val dataset")
-        _, self.val_loader = data_loader(self.config, val_path)
-        self.iterations_per_epoch = len(self.train_loader)
-        logger.info(
-            'There are {} iterations per epoch'.format(self.iterations_per_epoch)
-        )
-
-        self.generator = TrajectoryGenerator(self.config)
-        self.generator.type(float_dtype).train()
-        logger.info('Here is the generator:')
-        logger.info(self.generator)
-
-        #checkpoint_sampler_path = self.config.DIR + 'GFlow_sampler_' \
-        #                          + self.config.dataset_name + '/checkpoint_with_model.pt'
-
-     #   191_Trajnet_lcas_Goal_sampler2 = self.config.DIR +
-        checkpoint_sampler_path = self.config.sampler_checkpoint_start_from + self.config.dataset_name +\
-                                  '/checkpoint_with_model.pt'
-        checkpoint_sampler = torch.load(checkpoint_sampler_path)
-        self.sample_generator = GoalGenerator(self.config)
-        self.sample_generator.load_state_dict(checkpoint_sampler["best_state"])
-        self.sample_generator.cuda().eval()
-
-        self.sampler = Sampler(self.config)
-        self.sampler.load_state_dict(checkpoint_sampler["best_state_sampler"])
-        self.sampler.cuda().eval()
-        logger.info('Sampler loaded from:' + str(checkpoint_sampler['args']))
-
-        if self.config.adam:
-            print('Learning with ADAM!')
-            # betas_d = (0.5, 0.9999)
-            self.optimizer = optim.Adam(self.generator.parameters(), lr=self.config.g_learning_rate)
-        else:
-            print('Learning with SGD!')
-            self.optimizer = optim.SGD(self.generator.parameters(), lr=self.config.g_learning_rate, momentum=0.9)
-        restore_path = None
-        if self.config.checkpoint_start_from is not None:
-            restore_path = self.config.checkpoint_start_from
-        elif self.config.restore_from_checkpoint == True:
-            restore_path = os.path.join(self.config.output_dir,
-                                        '%s_with_model.pt' % self.config.checkpoint_name)
-
-        if os.path.isfile(restore_path):
-            logger.info('Restoring from checkpoint {}'.format(restore_path))
-            self.checkpoint = torch.load(restore_path)
-            self.generator.load_state_dict(self.checkpoint['state'])
-            self.optimizer.load_state_dict(self.checkpoint['optim_state'])
-            self.t = self.checkpoint['counters']['t']
-            self.epoch = self.checkpoint['counters']['epoch']
-            self.checkpoint['restore_ts'].append(self.t)
-        else:
-            # Starting from scratch, so initialize checkpoint data structure
-            self.t, self.epoch = 0, 0
-            self.checkpoint = {
-                'args': self.config.__dict__,
-                'losses': defaultdict(list),
-                'losses_ts': [],
-                'metrics_val': defaultdict(list),
-                'metrics_train': defaultdict(list),
-                'sample_ts': [],
-                'restore_ts': [],
-                'counters': {
-                    't': None,
-                    'epoch': None,
-                },
-                'state': None,
-                'optim_state': None,
-                'best_state': None,
-            }
-
-    def check_accuracy(self, loader, generator):  # TODO Change this!
-        metrics = {}
-        generator.eval()  # will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
-
-        ade, fde, act, _ = evaluate(self.config, loader, generator, self.sample_generator, self.sampler)
-        metrics['act'] = act
-        metrics['ade'] = ade
-        metrics['fde'] = fde
-        mean = (act + fde)/2
-        metrics['mean'] = mean
-        generator.train()
-        wandb.log({"ade":  metrics['ade'], "fde": metrics['fde'],
-                   "act_best_ade": metrics['act'], "Mean_ade_act": metrics['mean']})
-        return metrics
-
-    def print_stats(self):
-        dictlist = 'Epoch = {}, t = {} '.format(self.epoch, self.t) + '[D] '
-        dictlist += ' [G] '
-
-        for k, v in sorted(self.losses.items()):
-            self.checkpoint['losses'][k].append(v)
-            dictlist += ' ' + '{}: {:.6f}'.format(k, v)
-
-        logger.info(dictlist)
-
-        self.checkpoint['losses_ts'].append(self.t)
-
-    def save_model(self):
-        self.checkpoint['counters']['t'] = self.t
-        self.checkpoint['counters']['epoch'] = self.epoch
-        self.checkpoint['sample_ts'].append(self.t)
-
-        # Check stats on the validation set
-        logger.info('Checking stats on val ...')
-        metrics_val = self.check_accuracy(self.val_loader, self.generator)
-    #    self.scheduler.step(metrics_val['ade'])
-        for k, v in sorted(metrics_val.items()):
-            logger.info('  [val] {}: {:.3f}'.format(k, v))
-            self.checkpoint['metrics_val'][k].append(v)
-
-        min_mean = min(self.checkpoint['metrics_val']['mean'])
-        if metrics_val['mean'] == min_mean:
-            logger.info('New low for mean error')
-            self.checkpoint['best_t'] = self.t
-            self.checkpoint['best_state'] = copy.deepcopy(self.generator.state_dict())
-
-        # Save another checkpoint with model weights and
-        # optimizer state
-        self.checkpoint['state'] = self.generator.state_dict()
-        self.checkpoint['optim_state'] = self.optimizer.state_dict()
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_with_model.pt' % self.config.checkpoint_name
-        )
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        torch.save(self.checkpoint, checkpoint_path)
-        torch.save(self.checkpoint, os.path.join(wandb.run.dir, 'model.pt'))
-        logger.info('Done.')
-
-        # Save a checkpoint with no model weights by making a shallow
-        # copy of the checkpoint excluding some items
-        checkpoint_path = os.path.join(
-            self.config.output_dir, '%s_no_model.pt' % self.config.checkpoint_name)
-        logger.info('Saving checkpoint to {}'.format(checkpoint_path))
-        key_blacklist = [
-            'state', 'best_state', 'optim_state'
-        ]
-        small_checkpoint = {}
-        for k, v in self.checkpoint.items():
-            if k not in key_blacklist:
-                small_checkpoint[k] = v
-        torch.save(small_checkpoint, checkpoint_path)
-        logger.info('Done.')
-
-    def model_step(self, batch):
-        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, val_mask,\
-        loss_mask, seq_start_end, nei_num_index, nei_num, = batch
-
-        losses = {}
-        MSE_loss = nn.MSELoss()
-        for param in self.generator.parameters(): param.grad = None
-
-        model_input = torch.cat((obs_traj_rel, pred_traj_gt_rel), dim=0)
-        goal = pred_traj_gt[-1]
-        pred_traj_fake_rel, nll, pred_traj_fake_rel_sampled = self.generator(model_input, obs_traj, pred_traj_gt,
-                                                                 seq_start_end, nei_num_index,
-                                                 nei_num, mode='train',sample_goal=goal)
-        pred_traj_fake_abs = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])
-        pred_traj_fake_abs_sampled = relative_to_abs(pred_traj_fake_rel_sampled, obs_traj[-1]) #ToDo: detach this?
-
-        corrected_pos_loss = MSE_loss(pred_traj_fake_abs, pred_traj_fake_abs_sampled)
-
-
-        loss_count = coll_smoothed_loss(pred_traj_fake_abs, seq_start_end, nei_num_index)
-        goal = pred_traj_gt[-1]
-        end_pos = pred_traj_fake_abs[-1]
-        goal_loss = MSE_loss(end_pos,goal)
-
-        loss = nll + 2.*goal_loss + 30. * loss_count + corrected_pos_loss
-        loss.backward()
-        losses['nll'] = nll.item()
-        torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=3.0, norm_type=2)
-        self.optimizer.step()
-        wandb.log({"nll": nll.item()})
-        return losses
-
-    def train(self):
-        self.t_step = 0
-        while self.epoch < self.config.num_epochs:
-            self.t_step = 0
-            logger.info('Starting epoch {}'.format(self.epoch))
-            for batch in self.train_loader:
-                batch = [tensor.cuda() for tensor in batch]
-                self.losses = self.model_step(batch)
-                self.t_step += 1
-            if self.epoch % self.config.check_after_num_epochs == 0:
-                self.save_model()
-            self.epoch += 1
-
-if __name__ == '__main__':
-    prep = Preparation()
-    prep.train()
\ No newline at end of file
diff --git a/train_dataset.py b/train_dataset.py
deleted file mode 100644
index 3ea9faf..0000000
--- a/train_dataset.py
+++ /dev/null
@@ -1,21 +0,0 @@
-# Choose what to train, do not forget to set the right config!
-# from train_GoalFlow import Preparation
-# from train_GoalFlow_divSampling import Preparation
-from train_Sim2Goal import Preparation
-# from experiments.train_GAN import Preparation
-# from experiments.train_VAE import Preparation
-# from experiments.train_SimNoGoal import Preparation
-
-from config import Config
-config = Config()
-
-if config.trajnet:
-    datasets = ['wildtrack', 'students1', 'students3', 'zara1', 'zara3', 'hotel', 'lcas']
-else:
-    datasets = ['hotel', 'zara1', 'zara2', 'univ', 'eth']  # eth & utc
-if __name__ == '__main__':
-    for data in datasets:
-        prep = Preparation(data)
-        prep.train()
-        del prep
-        print('START NEW DATASET ####################################################')
\ No newline at end of file
